{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Logistic_regression.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AndreaBertoglio/MLDM/blob/master/Logistic_regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZI9-kt4ALqSO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ae8d5641-2f55-4c6a-f9dd-61872901cfb6"
      },
      "source": [
        "import pip\n",
        "import sys\n",
        "#if not 'sklearn' in sys.modules.keys():\n",
        "#    pip.main(['install', 'sklearn'])\n",
        "#if not 'kaggle' in sys.modules.keys():\n",
        "#    pip.main(['install', 'kaggle'])\n",
        "import random\n",
        "\n",
        "print(\"Random number with seed 2020\")\n",
        "# first call\n",
        "random.seed(2020)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Random number with seed 2020\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "8524922d-be55-46fd-863f-004261fcfff0",
        "_uuid": "58cfd95aa5563209575b12977280983ffeea6492",
        "scrolled": false,
        "id": "zyxXPaP5LqSk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import graphviz\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Read the data from GitHub\n",
        "train = pd.read_csv('https://raw.githubusercontent.com/serivan/mldmlab/master/Datasets/Kaggle2020/train.csv')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDx-QVwILqSz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Le classi sono 1=Good 0=Disappointing\n",
        "train[\"Quality\"] = np.where(train[\"Quality\"].str.contains(\"Good\"), 1, 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJIUVDySLqTY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# pull data into target (y) and predictors (X)\n",
        "#La y è la classe, cioè la Quality\n",
        "train_y = train.Quality\n",
        "#seleziono colonne di interesse, non metto l'ID perchè non mi interessa\n",
        "predictor_cols = ['fixed.acidity','volatile.acidity','citric.acid','residual.sugar','chlorides','free.sulfur.dioxide','total.sulfur.dioxide','density','pH','sulphates','alcohol']\n",
        "\n",
        "# La x sono gli attributi\n",
        "train_X = train[predictor_cols]\n",
        "\n",
        "# Sostituisce i missing values con la media e lo applica alle x\n",
        "imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
        "imp = imp.fit(train_X)\n",
        "\n",
        "\n",
        "# Impute our data, then train\n",
        "train_X_imp = imp.transform(train_X)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgAodc3qLqU2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Uso l'80% dei dati per train e il restante 20% per test\n",
        "xTrain, xTest, yTrain, yTest = train_test_split(train_X_imp, train_y, train_size = 0.8, random_state = 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D0Bw-JZ0LqTo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generateSubmission(myModel, submissionFile, description):\n",
        "    # Read the test data\n",
        "    test = pd.read_csv('https://raw.githubusercontent.com/serivan/mldmlab/master/Datasets/Kaggle2020/test.csv')\n",
        "\n",
        "\n",
        "    # Treat the test data in the same way as training data. In this case, pull same columns.\n",
        "    test_X = test[predictor_cols]\n",
        "\n",
        "    # Impute each test item, then predict\n",
        "    test_X_imp = imp.transform(test_X)\n",
        "    \n",
        "    # Use the model to make predictions\n",
        "    predicted_q = myModel.predict(test_X_imp)\n",
        "    # We will look at the predicted Qualities to ensure we have something sensible.\n",
        "    print(predicted_q)\n",
        "    \n",
        "    #submission file\n",
        "    my_submission = pd.DataFrame({'Id': test.Id, 'Quality': predicted_q})\n",
        "    # you could use any filename. We choose submission here\n",
        "    my_submission.to_csv(submissionFile, index=False)\n",
        "    \n",
        "    #Submit authomatically; kaggle API authentication needed\n",
        "    #!kaggle competitions submit -c mldm-classification-competition-2020 -f {submissionFile} -m '{description}'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOjDBnxsLqXD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import StratifiedKFold"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Oi1-YS1LqXM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3c22ead4-01ac-482f-a751-cc636457ad66"
      },
      "source": [
        "log_reg = LogisticRegression()\n",
        "\n",
        "#Range di parametri che vengono esplorati per trovare la combinazione migliore\n",
        "parameter_grid = {'penalty': ['none', 'l2'],\n",
        "                  'dual':[False],\n",
        "                  'tol': [1e-4,1e-6,1e-2,1],\n",
        "                  'C': [0.01,0.1,0.5,2,50],\n",
        "                  #'class_weight' : [{'alcohol' : [2,3]}], \n",
        "                  'solver' : ['saga'],\n",
        "                  'max_iter': [30,50,100,200]\n",
        "                  }\n",
        "\n",
        "cross_validation = StratifiedKFold(n_splits=10)\n",
        "cross_validation.get_n_splits(train_X_imp, train_y)\n",
        "\n",
        "#Create the scoring dictionary\n",
        "SCORING = {'accuracy': 'accuracy',\n",
        "'balanced_accuracy': 'balanced_accuracy',\n",
        "'precision': 'precision_macro',\n",
        "'recall': 'recall_macro',\n",
        "'f1': 'f1_macro'}\n",
        "\n",
        "#grid_search = GridSearchCV(dtc, param_grid=parameter_grid, cv=cross_validation)\n",
        "grid_search = GridSearchCV(log_reg, param_grid=parameter_grid, cv=cross_validation, scoring=SCORING,return_train_score=True, refit='f1')\n",
        "\n",
        "grid_search.fit(train_X_imp, train_y)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=StratifiedKFold(n_splits=10, random_state=None, shuffle=False),\n",
              "             error_score=nan,\n",
              "             estimator=LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
              "                                          fit_intercept=True,\n",
              "                                          intercept_scaling=1, l1_ratio=None,\n",
              "                                          max_iter=100, multi_class='auto',\n",
              "                                          n_jobs=None, penalty='l2',\n",
              "                                          random_state=None, solver='lbfgs',\n",
              "                                          tol=0.0001, verbose=0,\n",
              "                                          warm_start=False),\n",
              "             iid='deprec...\n",
              "             param_grid={'C': [0.01, 0.1, 0.5, 2, 50], 'dual': [False],\n",
              "                         'max_iter': [30, 50, 100, 200],\n",
              "                         'penalty': ['none', 'l2'], 'solver': ['saga'],\n",
              "                         'tol': [0.0001, 1e-06, 0.01, 1]},\n",
              "             pre_dispatch='2*n_jobs', refit='f1', return_train_score=True,\n",
              "             scoring={'accuracy': 'accuracy',\n",
              "                      'balanced_accuracy': 'balanced_accuracy',\n",
              "                      'f1': 'f1_macro', 'precision': 'precision_macro',\n",
              "                      'recall': 'recall_macro'},\n",
              "             verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1fi-vRcLqXW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5f44ad5f-b0e7-41af-d849-a84a66fcf0f6"
      },
      "source": [
        "grid_search.cv_results_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'mean_fit_time': array([0.03371334, 0.03336709, 0.03374279, 0.00373018, 0.03263218,\n",
              "        0.03382306, 0.03499923, 0.00355995, 0.05300667, 0.05303652,\n",
              "        0.05363491, 0.00406272, 0.0524971 , 0.05292106, 0.05229468,\n",
              "        0.00351701, 0.10289798, 0.10326591, 0.0617779 , 0.00379496,\n",
              "        0.10207679, 0.1035094 , 0.06012487, 0.00356233, 0.16397307,\n",
              "        0.16380112, 0.06094382, 0.00388141, 0.16384208, 0.16320212,\n",
              "        0.06061761, 0.00347598, 0.03372447, 0.03301525, 0.03233104,\n",
              "        0.0038852 , 0.03299274, 0.03319688, 0.03270473, 0.00336428,\n",
              "        0.0523756 , 0.0539166 , 0.05351813, 0.00389726, 0.05392451,\n",
              "        0.0527267 , 0.05236979, 0.00345597, 0.101425  , 0.10291495,\n",
              "        0.06118402, 0.00384386, 0.10192311, 0.10222893, 0.06151118,\n",
              "        0.00341492, 0.1652411 , 0.16950111, 0.06079588, 0.0042486 ,\n",
              "        0.16427131, 0.16364727, 0.05964935, 0.00358136, 0.03283069,\n",
              "        0.03355167, 0.03337252, 0.00385849, 0.03364131, 0.03246799,\n",
              "        0.03390985, 0.00400422, 0.05314453, 0.05310307, 0.05266938,\n",
              "        0.00402272, 0.05274189, 0.05350721, 0.0524303 , 0.00344276,\n",
              "        0.1014957 , 0.10380404, 0.06239164, 0.00370967, 0.10249887,\n",
              "        0.10172508, 0.06091182, 0.00327356, 0.16456335, 0.1639251 ,\n",
              "        0.06118968, 0.00375807, 0.16363103, 0.16314392, 0.0604805 ,\n",
              "        0.00337539, 0.03366358, 0.03401251, 0.03242476, 0.00386007,\n",
              "        0.03231022, 0.03238583, 0.03256371, 0.00345194, 0.05289059,\n",
              "        0.05195229, 0.05329576, 0.00387223, 0.05285375, 0.05218582,\n",
              "        0.05257421, 0.00357189, 0.10097873, 0.10129476, 0.06132319,\n",
              "        0.00391638, 0.10192885, 0.10180826, 0.06010547, 0.00330477,\n",
              "        0.16277425, 0.16566136, 0.0612747 , 0.00392592, 0.16364064,\n",
              "        0.16344364, 0.06091609, 0.00340796, 0.03232093, 0.03307269,\n",
              "        0.03258035, 0.0037755 , 0.03235664, 0.03263063, 0.03281331,\n",
              "        0.00344863, 0.05385737, 0.05274162, 0.05286546, 0.00375943,\n",
              "        0.05346224, 0.05188951, 0.05286775, 0.00335515, 0.10184581,\n",
              "        0.10318131, 0.06031146, 0.00396531, 0.10324662, 0.10184076,\n",
              "        0.05991108, 0.00340419, 0.16338892, 0.16505487, 0.06046898,\n",
              "        0.00376279, 0.16425595, 0.16419778, 0.0598269 , 0.00343702]),\n",
              " 'mean_score_time': array([0.0055234 , 0.00550156, 0.00563843, 0.00531945, 0.00553975,\n",
              "        0.00580359, 0.00566642, 0.00515883, 0.00600166, 0.0057786 ,\n",
              "        0.00578184, 0.00521982, 0.00562274, 0.00578623, 0.0058331 ,\n",
              "        0.00627646, 0.00533657, 0.00506654, 0.00559134, 0.00553284,\n",
              "        0.00525742, 0.00536883, 0.0055414 , 0.00643816, 0.0043745 ,\n",
              "        0.00442801, 0.00564473, 0.00521212, 0.00428672, 0.00466993,\n",
              "        0.00572565, 0.00526452, 0.00562153, 0.00573511, 0.00634346,\n",
              "        0.00546827, 0.00575936, 0.0059793 , 0.00547872, 0.00515018,\n",
              "        0.00677965, 0.00575283, 0.00603828, 0.00530193, 0.0058799 ,\n",
              "        0.00570531, 0.00563841, 0.00525358, 0.0055727 , 0.00540113,\n",
              "        0.00626357, 0.0053654 , 0.00571735, 0.00565238, 0.00573549,\n",
              "        0.00515673, 0.00424747, 0.00523415, 0.00610425, 0.00532217,\n",
              "        0.0043762 , 0.00457029, 0.00574269, 0.00538943, 0.00591555,\n",
              "        0.00623763, 0.00609014, 0.00524454, 0.00573769, 0.00567715,\n",
              "        0.00589738, 0.00590184, 0.00632935, 0.00665166, 0.00571461,\n",
              "        0.00536611, 0.00570164, 0.00579352, 0.00575883, 0.00537057,\n",
              "        0.00552893, 0.00515931, 0.00561688, 0.00514019, 0.0054656 ,\n",
              "        0.00551488, 0.00559354, 0.00515308, 0.00471289, 0.0042428 ,\n",
              "        0.00569654, 0.00548728, 0.00426149, 0.00441093, 0.00672543,\n",
              "        0.00516965, 0.00642309, 0.00570922, 0.00557282, 0.00533676,\n",
              "        0.00586886, 0.00557113, 0.00620403, 0.00538094, 0.0061496 ,\n",
              "        0.0060456 , 0.00590451, 0.00527542, 0.00567031, 0.00576944,\n",
              "        0.00568407, 0.00534883, 0.0057112 , 0.00552354, 0.00567939,\n",
              "        0.0053745 , 0.00536642, 0.00544662, 0.00566199, 0.0051266 ,\n",
              "        0.00429754, 0.00429451, 0.00576584, 0.00556636, 0.00429473,\n",
              "        0.00436151, 0.00556579, 0.00518086, 0.00556085, 0.00583184,\n",
              "        0.00571675, 0.00628679, 0.00565176, 0.00558047, 0.00558944,\n",
              "        0.00520914, 0.00561628, 0.00563986, 0.00582771, 0.00526855,\n",
              "        0.00585706, 0.00563197, 0.00581701, 0.00525868, 0.00550001,\n",
              "        0.00520647, 0.00563686, 0.00545259, 0.00516326, 0.00550761,\n",
              "        0.00603468, 0.00523446, 0.00441635, 0.00455658, 0.00585892,\n",
              "        0.00524397, 0.00449595, 0.00429828, 0.00564818, 0.00519202]),\n",
              " 'mean_test_accuracy': array([0.66437276, 0.66437276, 0.66437276, 0.64632036, 0.66465929,\n",
              "        0.66494582, 0.66437276, 0.65462487, 0.66236291, 0.66264944,\n",
              "        0.66264944, 0.65262408, 0.66264944, 0.6629368 , 0.66264944,\n",
              "        0.64832444, 0.66092942, 0.66092942, 0.65978247, 0.64975464,\n",
              "        0.66092942, 0.66092942, 0.660069  , 0.65090406, 0.66350822,\n",
              "        0.66350822, 0.66006982, 0.64631624, 0.66408211, 0.66408211,\n",
              "        0.660069  , 0.65261914, 0.66437276, 0.66437276, 0.66465929,\n",
              "        0.65721026, 0.66494582, 0.66465929, 0.66408622, 0.64889421,\n",
              "        0.66264944, 0.66264944, 0.66264944, 0.65348039, 0.66264944,\n",
              "        0.66264944, 0.66264944, 0.65033099, 0.66092942, 0.66092942,\n",
              "        0.65978247, 0.65778168, 0.66064289, 0.66092942, 0.66006982,\n",
              "        0.64947057, 0.66379475, 0.66379475, 0.65978247, 0.65835474,\n",
              "        0.66408128, 0.66408128, 0.65949593, 0.65835556, 0.66465929,\n",
              "        0.66465929, 0.66465929, 0.65061753, 0.66465929, 0.66437276,\n",
              "        0.66465929, 0.65319797, 0.66264944, 0.66264944, 0.66236291,\n",
              "        0.65347956, 0.66264944, 0.66264944, 0.66264944, 0.65204937,\n",
              "        0.66064289, 0.66092942, 0.65978329, 0.65548859, 0.66092942,\n",
              "        0.66092942, 0.65978247, 0.6549147 , 0.66350822, 0.66350822,\n",
              "        0.65978247, 0.64889504, 0.66350822, 0.66350822, 0.65978247,\n",
              "        0.65033182, 0.66494582, 0.66437276, 0.66408622, 0.65118812,\n",
              "        0.66437276, 0.66465929, 0.66465929, 0.65118977, 0.66236291,\n",
              "        0.66264944, 0.66236209, 0.6494681 , 0.66264944, 0.66264944,\n",
              "        0.66264944, 0.65462981, 0.66092942, 0.66064289, 0.6592094 ,\n",
              "        0.65233343, 0.66092942, 0.66092942, 0.65978329, 0.65462981,\n",
              "        0.66350822, 0.66350822, 0.65949593, 0.65434328, 0.66350822,\n",
              "        0.66350822, 0.65949593, 0.65519794, 0.66437276, 0.66465929,\n",
              "        0.66437276, 0.65032935, 0.66465929, 0.66408622, 0.66437276,\n",
              "        0.65233755, 0.66264944, 0.66264944, 0.66264944, 0.65090488,\n",
              "        0.66264944, 0.66264944, 0.66264944, 0.65319715, 0.66092942,\n",
              "        0.66092942, 0.65949676, 0.65749514, 0.66092942, 0.66092942,\n",
              "        0.65978329, 0.65376774, 0.66350822, 0.66350822, 0.65949676,\n",
              "        0.65348368, 0.66350822, 0.66350822, 0.66006982, 0.65262491]),\n",
              " 'mean_test_balanced_accuracy': array([0.52663279, 0.52663279, 0.52663279, 0.50228634, 0.52685113,\n",
              "        0.52706947, 0.52663279, 0.50721942, 0.53234047, 0.53275369,\n",
              "        0.53275369, 0.50533018, 0.53275369, 0.53297299, 0.53275369,\n",
              "        0.49889082, 0.54396467, 0.54396467, 0.53251852, 0.50353748,\n",
              "        0.5437673 , 0.5437673 , 0.5325439 , 0.50502803, 0.55438082,\n",
              "        0.55438082, 0.53313007, 0.50242113, 0.55442812, 0.55442812,\n",
              "        0.5325439 , 0.50549462, 0.52663279, 0.52663279, 0.52685113,\n",
              "        0.51017057, 0.52706947, 0.52685113, 0.52641445, 0.50560906,\n",
              "        0.53275369, 0.53275369, 0.53275369, 0.51131435, 0.53275369,\n",
              "        0.53275369, 0.53275369, 0.50493306, 0.54396467, 0.54396467,\n",
              "        0.53271341, 0.5073046 , 0.543548  , 0.54396467, 0.53313007,\n",
              "        0.50857768, 0.55460012, 0.55460012, 0.53251852, 0.51029541,\n",
              "        0.55481941, 0.55481941, 0.5321053 , 0.50773631, 0.52685113,\n",
              "        0.52685113, 0.52685113, 0.50616675, 0.52685113, 0.52663279,\n",
              "        0.52685209, 0.50533384, 0.53275369, 0.53275369, 0.5325344 ,\n",
              "        0.50618141, 0.53275369, 0.53275369, 0.53275369, 0.50629706,\n",
              "        0.543548  , 0.54396467, 0.53271685, 0.50888272, 0.54396467,\n",
              "        0.54396467, 0.53251852, 0.50861433, 0.55438082, 0.55438082,\n",
              "        0.53271341, 0.50307332, 0.55438082, 0.55438082, 0.53251852,\n",
              "        0.50474183, 0.52706947, 0.52663279, 0.52641445, 0.50482502,\n",
              "        0.52663279, 0.52685113, 0.52685113, 0.5068257 , 0.5325344 ,\n",
              "        0.53275369, 0.5325344 , 0.50328219, 0.53275369, 0.53275369,\n",
              "        0.53275369, 0.50644066, 0.54396467, 0.543548  , 0.53188696,\n",
              "        0.50625758, 0.54396467, 0.54396467, 0.53271685, 0.50821871,\n",
              "        0.55438082, 0.55438082, 0.53230018, 0.5054583 , 0.55438082,\n",
              "        0.55438082, 0.5321053 , 0.50728911, 0.52663279, 0.52685113,\n",
              "        0.52663279, 0.50592412, 0.52685113, 0.52641445, 0.52682768,\n",
              "        0.50430627, 0.53275369, 0.53275369, 0.53275369, 0.50518348,\n",
              "        0.53275369, 0.53275369, 0.53275369, 0.50459718, 0.54396467,\n",
              "        0.54396467, 0.53230363, 0.50825708, 0.54396467, 0.54396467,\n",
              "        0.53271685, 0.50937081, 0.55438082, 0.55438082, 0.53230363,\n",
              "        0.50578763, 0.55438082, 0.55438082, 0.53313352, 0.50630745]),\n",
              " 'mean_test_f1': array([0.46688436, 0.46688436, 0.46688436, 0.42257886, 0.46703656,\n",
              "        0.46718338, 0.46688436, 0.42248225, 0.48622071, 0.48690415,\n",
              "        0.48690415, 0.41975738, 0.48690415, 0.48706372, 0.48690415,\n",
              "        0.4069675 , 0.51623986, 0.51623986, 0.49030919, 0.41988772,\n",
              "        0.51575374, 0.51575374, 0.49002685, 0.42233567, 0.53579887,\n",
              "        0.53579887, 0.49149956, 0.42372723, 0.53539824, 0.53539824,\n",
              "        0.49002685, 0.42104217, 0.46688436, 0.46688436, 0.46703118,\n",
              "        0.42671377, 0.46718338, 0.46703656, 0.46673776, 0.42879074,\n",
              "        0.48690415, 0.48690415, 0.48690415, 0.43732107, 0.48690415,\n",
              "        0.48690415, 0.48690415, 0.42328407, 0.51623986, 0.51623986,\n",
              "        0.49079316, 0.4157986 , 0.5155647 , 0.51623986, 0.49149956,\n",
              "        0.43738457, 0.53600561, 0.53600561, 0.49030919, 0.42530678,\n",
              "        0.53620644, 0.53620644, 0.48967057, 0.41582684, 0.46703656,\n",
              "        0.46703656, 0.46703656, 0.4264334 , 0.46703656, 0.46688436,\n",
              "        0.46706352, 0.41838543, 0.48690415, 0.48690415, 0.48672329,\n",
              "        0.42117221, 0.48690415, 0.48690415, 0.48690415, 0.42303609,\n",
              "        0.5155647 , 0.51623986, 0.49083489, 0.42632748, 0.51623986,\n",
              "        0.51623986, 0.49033524, 0.42664681, 0.53579887, 0.53579887,\n",
              "        0.49079316, 0.42109971, 0.53579887, 0.53579887, 0.49030919,\n",
              "        0.42318045, 0.46718338, 0.46688436, 0.46673776, 0.42154325,\n",
              "        0.46688436, 0.46703656, 0.46703118, 0.42649502, 0.48672329,\n",
              "        0.48690415, 0.48674486, 0.42027704, 0.48690415, 0.48690415,\n",
              "        0.48690415, 0.4198331 , 0.51623986, 0.5155647 , 0.4894883 ,\n",
              "        0.42426777, 0.51623986, 0.51623986, 0.49086095, 0.42590436,\n",
              "        0.53579887, 0.53579887, 0.49012691, 0.4167229 , 0.53579887,\n",
              "        0.53579887, 0.48967057, 0.42186657, 0.46688436, 0.46703118,\n",
              "        0.46688436, 0.42735593, 0.46703656, 0.46673776, 0.46743973,\n",
              "        0.41759155, 0.48690415, 0.48690415, 0.48690415, 0.42300069,\n",
              "        0.48690415, 0.48690415, 0.48690415, 0.41622952, 0.51623986,\n",
              "        0.51623986, 0.49019628, 0.41994868, 0.51623986, 0.51623986,\n",
              "        0.49083489, 0.43076367, 0.53579887, 0.53579887, 0.49019628,\n",
              "        0.41931897, 0.53579887, 0.53579887, 0.4915329 , 0.42387772]),\n",
              " 'mean_test_precision': array([0.64132523, 0.64132523, 0.64132523, 0.55034565, 0.64393879,\n",
              "        0.64623227, 0.64132523, 0.65291625, 0.61480341, 0.61589182,\n",
              "        0.61589182, 0.60160911, 0.61589182, 0.61693188, 0.61589182,\n",
              "        0.48012237, 0.60397594, 0.60397594, 0.60293215, 0.55803873,\n",
              "        0.60356857, 0.60356857, 0.6037154 , 0.55649991, 0.61134022,\n",
              "        0.61134022, 0.60422185, 0.52704379, 0.61241   , 0.61241   ,\n",
              "        0.6037154 , 0.54798303, 0.64132523, 0.64132523, 0.64361872,\n",
              "        0.66307382, 0.64623227, 0.64393879, 0.63935196, 0.58957993,\n",
              "        0.61589182, 0.61589182, 0.61589182, 0.61379064, 0.61589182,\n",
              "        0.61589182, 0.61589182, 0.53877552, 0.60397594, 0.60397594,\n",
              "        0.6027285 , 0.64178525, 0.60300905, 0.60397594, 0.60422185,\n",
              "        0.58437452, 0.61188353, 0.61188353, 0.60293215, 0.63176141,\n",
              "        0.61239168, 0.61239168, 0.60215271, 0.59576281, 0.64393879,\n",
              "        0.64393879, 0.64393879, 0.57751623, 0.64393879, 0.64132523,\n",
              "        0.64244456, 0.59680029, 0.61589182, 0.61589182, 0.6144831 ,\n",
              "        0.61227022, 0.61589182, 0.61589182, 0.61589182, 0.59205519,\n",
              "        0.60300905, 0.60397594, 0.60321791, 0.64350431, 0.60397594,\n",
              "        0.60397594, 0.60315665, 0.57089302, 0.61134022, 0.61134022,\n",
              "        0.6027285 , 0.55595746, 0.61134022, 0.61134022, 0.60293215,\n",
              "        0.56948028, 0.64623227, 0.64132523, 0.63935196, 0.60255128,\n",
              "        0.64132523, 0.64393879, 0.64361872, 0.52179747, 0.6144831 ,\n",
              "        0.61589182, 0.61493379, 0.57877507, 0.61589182, 0.61589182,\n",
              "        0.61589182, 0.63212653, 0.60397594, 0.60300905, 0.60118223,\n",
              "        0.60469761, 0.60397594, 0.60397594, 0.60344242, 0.63181782,\n",
              "        0.61134022, 0.61134022, 0.60196167, 0.54191192, 0.61134022,\n",
              "        0.61134022, 0.60215271, 0.62084032, 0.64132523, 0.64361872,\n",
              "        0.64132523, 0.52066504, 0.64393879, 0.63935196, 0.64021256,\n",
              "        0.6180243 , 0.61589182, 0.61589182, 0.61589182, 0.56653017,\n",
              "        0.61589182, 0.61589182, 0.61589182, 0.57461016, 0.60397594,\n",
              "        0.60397594, 0.60243847, 0.65588364, 0.60397594, 0.60397594,\n",
              "        0.60321791, 0.55926765, 0.61134022, 0.61134022, 0.60243847,\n",
              "        0.58380594, 0.61134022, 0.61134022, 0.60413695, 0.57892297]),\n",
              " 'mean_test_recall': array([0.52663279, 0.52663279, 0.52663279, 0.50228634, 0.52685113,\n",
              "        0.52706947, 0.52663279, 0.50721942, 0.53234047, 0.53275369,\n",
              "        0.53275369, 0.50533018, 0.53275369, 0.53297299, 0.53275369,\n",
              "        0.49889082, 0.54396467, 0.54396467, 0.53251852, 0.50353748,\n",
              "        0.5437673 , 0.5437673 , 0.5325439 , 0.50502803, 0.55438082,\n",
              "        0.55438082, 0.53313007, 0.50242113, 0.55442812, 0.55442812,\n",
              "        0.5325439 , 0.50549462, 0.52663279, 0.52663279, 0.52685113,\n",
              "        0.51017057, 0.52706947, 0.52685113, 0.52641445, 0.50560906,\n",
              "        0.53275369, 0.53275369, 0.53275369, 0.51131435, 0.53275369,\n",
              "        0.53275369, 0.53275369, 0.50493306, 0.54396467, 0.54396467,\n",
              "        0.53271341, 0.5073046 , 0.543548  , 0.54396467, 0.53313007,\n",
              "        0.50857768, 0.55460012, 0.55460012, 0.53251852, 0.51029541,\n",
              "        0.55481941, 0.55481941, 0.5321053 , 0.50773631, 0.52685113,\n",
              "        0.52685113, 0.52685113, 0.50616675, 0.52685113, 0.52663279,\n",
              "        0.52685209, 0.50533384, 0.53275369, 0.53275369, 0.5325344 ,\n",
              "        0.50618141, 0.53275369, 0.53275369, 0.53275369, 0.50629706,\n",
              "        0.543548  , 0.54396467, 0.53271685, 0.50888272, 0.54396467,\n",
              "        0.54396467, 0.53251852, 0.50861433, 0.55438082, 0.55438082,\n",
              "        0.53271341, 0.50307332, 0.55438082, 0.55438082, 0.53251852,\n",
              "        0.50474183, 0.52706947, 0.52663279, 0.52641445, 0.50482502,\n",
              "        0.52663279, 0.52685113, 0.52685113, 0.5068257 , 0.5325344 ,\n",
              "        0.53275369, 0.5325344 , 0.50328219, 0.53275369, 0.53275369,\n",
              "        0.53275369, 0.50644066, 0.54396467, 0.543548  , 0.53188696,\n",
              "        0.50625758, 0.54396467, 0.54396467, 0.53271685, 0.50821871,\n",
              "        0.55438082, 0.55438082, 0.53230018, 0.5054583 , 0.55438082,\n",
              "        0.55438082, 0.5321053 , 0.50728911, 0.52663279, 0.52685113,\n",
              "        0.52663279, 0.50592412, 0.52685113, 0.52641445, 0.52682768,\n",
              "        0.50430627, 0.53275369, 0.53275369, 0.53275369, 0.50518348,\n",
              "        0.53275369, 0.53275369, 0.53275369, 0.50459718, 0.54396467,\n",
              "        0.54396467, 0.53230363, 0.50825708, 0.54396467, 0.54396467,\n",
              "        0.53271685, 0.50937081, 0.55438082, 0.55438082, 0.53230363,\n",
              "        0.50578763, 0.55438082, 0.55438082, 0.53313352, 0.50630745]),\n",
              " 'mean_train_accuracy': array([0.66723977, 0.66733531, 0.66733531, 0.64931025, 0.66714426,\n",
              "        0.66749455, 0.6673035 , 0.65513843, 0.66478767, 0.66494691,\n",
              "        0.6647877 , 0.65561604, 0.6649469 , 0.66485138, 0.66475584,\n",
              "        0.65558417, 0.66045652, 0.66023359, 0.66249479, 0.65313206,\n",
              "        0.66064763, 0.66061578, 0.66303616, 0.65325932, 0.66491498,\n",
              "        0.66491498, 0.66224002, 0.65144418, 0.66434173, 0.66427804,\n",
              "        0.66306801, 0.65485176, 0.66743086, 0.66711238, 0.66720795,\n",
              "        0.6548516 , 0.66730347, 0.66717608, 0.66723977, 0.65052113,\n",
              "        0.66478768, 0.66475585, 0.66485139, 0.65179502, 0.66481952,\n",
              "        0.6647877 , 0.6647877 , 0.65099826, 0.66023359, 0.66029728,\n",
              "        0.66214448, 0.65743127, 0.66045652, 0.66032913, 0.6624311 ,\n",
              "        0.64829112, 0.66485129, 0.66488314, 0.66249474, 0.65765416,\n",
              "        0.66488314, 0.66488314, 0.66252664, 0.65762231, 0.66723977,\n",
              "        0.66720796, 0.66720793, 0.65268669, 0.6671124 , 0.66743086,\n",
              "        0.66720793, 0.65606178, 0.66475584, 0.66462846, 0.66485139,\n",
              "        0.65233586, 0.66453292, 0.66481954, 0.66475585, 0.65083958,\n",
              "        0.66016989, 0.66026543, 0.66214448, 0.65469278, 0.66026543,\n",
              "        0.66023359, 0.66255846, 0.6543421 , 0.66491498, 0.66488314,\n",
              "        0.66246295, 0.6547562 , 0.66488314, 0.66488314, 0.66271769,\n",
              "        0.65201725, 0.66720793, 0.66720793, 0.66727162, 0.65421475,\n",
              "        0.66739901, 0.66727162, 0.66736716, 0.65214465, 0.66481952,\n",
              "        0.66485137, 0.66491506, 0.65399188, 0.66481954, 0.66472398,\n",
              "        0.66469213, 0.65602997, 0.66026543, 0.66032913, 0.66243105,\n",
              "        0.6531002 , 0.66036097, 0.66026543, 0.66233556, 0.65392813,\n",
              "        0.66491498, 0.66491498, 0.66227186, 0.65647583, 0.66488314,\n",
              "        0.66491498, 0.66246292, 0.65618942, 0.66723977, 0.66720794,\n",
              "        0.66730347, 0.65290902, 0.66739903, 0.66720795, 0.66698502,\n",
              "        0.65615741, 0.66478768, 0.66485136, 0.664724  , 0.65281347,\n",
              "        0.66475582, 0.66491505, 0.66475582, 0.65501086, 0.66023359,\n",
              "        0.66032913, 0.6624311 , 0.65727211, 0.66032913, 0.66026543,\n",
              "        0.6623037 , 0.652623  , 0.66491498, 0.66491498, 0.66233555,\n",
              "        0.65208106, 0.66491498, 0.66491498, 0.66230371, 0.6543739 ]),\n",
              " 'mean_train_balanced_accuracy': array([0.52949529, 0.52959007, 0.52958999, 0.50641277, 0.52922639,\n",
              "        0.52964635, 0.5295222 , 0.50779666, 0.53546348, 0.5355851 ,\n",
              "        0.53542001, 0.50709066, 0.53547613, 0.53546867, 0.53524319,\n",
              "        0.50661103, 0.54335071, 0.54318047, 0.5359342 , 0.50776346,\n",
              "        0.54319169, 0.54314561, 0.53628233, 0.50779187, 0.55614386,\n",
              "        0.55614386, 0.53576136, 0.50857011, 0.55511801, 0.55506935,\n",
              "        0.53630666, 0.50851418, 0.52968481, 0.52935441, 0.52944931,\n",
              "        0.5080357 , 0.52950037, 0.52933764, 0.52949522, 0.50775478,\n",
              "        0.53546353, 0.5354393 , 0.53557768, 0.50845873, 0.53548788,\n",
              "        0.53537642, 0.5354418 , 0.5065956 , 0.54313688, 0.54325092,\n",
              "        0.5356884 , 0.50765092, 0.54330715, 0.54323164, 0.53588556,\n",
              "        0.50738069, 0.55609521, 0.55609773, 0.53599951, 0.50875601,\n",
              "        0.55609773, 0.55609773, 0.53598027, 0.50677253, 0.52947344,\n",
              "        0.52940556, 0.52942745, 0.50757431, 0.5293763 , 0.5297066 ,\n",
              "        0.52940552, 0.50854628, 0.53543924, 0.53529835, 0.53549048,\n",
              "        0.5057405 , 0.5351601 , 0.53546616, 0.53539569, 0.50422545,\n",
              "        0.54308826, 0.5432048 , 0.5356666 , 0.50793414, 0.54316123,\n",
              "        0.54315867, 0.53600469, 0.50810161, 0.55614386, 0.55611953,\n",
              "        0.53595346, 0.50945995, 0.55609773, 0.55611954, 0.53608274,\n",
              "        0.50697741, 0.52944928, 0.52944916, 0.52951957, 0.50828551,\n",
              "        0.52961693, 0.52954146, 0.52963623, 0.50678751, 0.53548788,\n",
              "        0.53551218, 0.53553907, 0.50883819, 0.53544438, 0.53537129,\n",
              "        0.53534704, 0.50854283, 0.54318299, 0.54320987, 0.53590721,\n",
              "        0.50824241, 0.54327774, 0.5432048 , 0.53585612, 0.508221  ,\n",
              "        0.55614386, 0.55614386, 0.53576389, 0.50814021, 0.55611954,\n",
              "        0.55614386, 0.53593168, 0.50800904, 0.52945166, 0.52940568,\n",
              "        0.52958762, 0.5094008 , 0.52961703, 0.52944927, 0.52921361,\n",
              "        0.50811671, 0.53544172, 0.53553395, 0.53530593, 0.50773904,\n",
              "        0.53537387, 0.53558266, 0.53541742, 0.50660687, 0.54315867,\n",
              "        0.54323167, 0.53584196, 0.50718011, 0.54325344, 0.5432048 ,\n",
              "        0.5358535 , 0.50782938, 0.55614386, 0.55614386, 0.53585611,\n",
              "        0.504501  , 0.55614386, 0.55614386, 0.53576646, 0.50864773]),\n",
              " 'mean_train_f1': array([0.47081622, 0.47093477, 0.47093399, 0.42984194, 0.47017965,\n",
              "        0.47082236, 0.47079465, 0.42312571, 0.49145139, 0.4915491 ,\n",
              "        0.49134167, 0.41970697, 0.49126071, 0.4913811 , 0.49091807,\n",
              "        0.4179248 , 0.51568052, 0.51553119, 0.49577261, 0.42699526,\n",
              "        0.5151329 , 0.51506045, 0.49594768, 0.42711464, 0.538216  ,\n",
              "        0.538216  , 0.49566445, 0.43385423, 0.53666172, 0.53661651,\n",
              "        0.49596742, 0.42636089, 0.47105143, 0.47060958, 0.47073908,\n",
              "        0.42464567, 0.47071713, 0.47044675, 0.47081557, 0.4328222 ,\n",
              "        0.49145292, 0.49143681, 0.49166914, 0.4319324 , 0.49147217,\n",
              "        0.49122532, 0.49139833, 0.42712678, 0.51543562, 0.51562245,\n",
              "        0.49561007, 0.41793809, 0.51558318, 0.51554765, 0.49573006,\n",
              "        0.4358252 , 0.53817072, 0.53814993, 0.49593812, 0.42147816,\n",
              "        0.53814993, 0.53814993, 0.49584717, 0.41403258, 0.47074636,\n",
              "        0.47060828, 0.47066431, 0.42724969, 0.47068554, 0.47111817,\n",
              "        0.47059323, 0.42352694, 0.49143586, 0.49123999, 0.49143673,\n",
              "        0.42199953, 0.49101373, 0.49141827, 0.49132198, 0.41926007,\n",
              "        0.51539231, 0.51555226, 0.49554999, 0.42463243, 0.51545606,\n",
              "        0.5154834 , 0.49586769, 0.42660332, 0.538216  , 0.53819344,\n",
              "        0.49586726, 0.43009641, 0.53814993, 0.53819328, 0.49585874,\n",
              "        0.42706756, 0.47073252, 0.47073141, 0.47083103, 0.42655338,\n",
              "        0.47090323, 0.47090092, 0.47101898, 0.42506388, 0.49147371,\n",
              "        0.49149091, 0.49147168, 0.42937349, 0.49136288, 0.49129967,\n",
              "        0.49128092, 0.42413414, 0.51550457, 0.51549904, 0.4957859 ,\n",
              "        0.42890029, 0.51561685, 0.51555226, 0.49577825, 0.42712076,\n",
              "        0.538216  , 0.538216  , 0.49562994, 0.42142118, 0.53819328,\n",
              "        0.538216  , 0.49580988, 0.42222242, 0.47068027, 0.4706018 ,\n",
              "        0.47098428, 0.43334011, 0.47090988, 0.47073701, 0.47041619,\n",
              "        0.42238324, 0.49139366, 0.49154697, 0.4911272 , 0.42738666,\n",
              "        0.49126279, 0.49158739, 0.4913738 , 0.41913823, 0.5154834 ,\n",
              "        0.51554484, 0.49561797, 0.41655119, 0.51559493, 0.51555226,\n",
              "        0.4958176 , 0.42855984, 0.538216  , 0.538216  , 0.49577871,\n",
              "        0.41745029, 0.538216  , 0.538216  , 0.4955957 , 0.42802149]),\n",
              " 'mean_train_precision': array([0.6537096 , 0.65439489, 0.65440371, 0.55976057, 0.65404415,\n",
              "        0.65594458, 0.65433943, 0.64321155, 0.62176713, 0.62245576,\n",
              "        0.62186823, 0.64150477, 0.62260991, 0.62211751, 0.62188149,\n",
              "        0.54660551, 0.6017864 , 0.6012084 , 0.6112089 , 0.62214177,\n",
              "        0.6023608 , 0.60226991, 0.61313142, 0.60961934, 0.61141734,\n",
              "        0.61141734, 0.61029434, 0.57468993, 0.61019304, 0.61005247,\n",
              "        0.61326154, 0.61522458, 0.65502001, 0.65297814, 0.65365275,\n",
              "        0.64194183, 0.6543932 , 0.65373049, 0.65376206, 0.58427444,\n",
              "        0.62178578, 0.62165333, 0.62197955, 0.60234044, 0.62191929,\n",
              "        0.62193921, 0.62183761, 0.55819735, 0.60122007, 0.60138011,\n",
              "        0.6099804 , 0.6469534 , 0.60179262, 0.60146229, 0.61095602,\n",
              "        0.57865125, 0.61127556, 0.6113451 , 0.61110367, 0.65949481,\n",
              "        0.6113451 , 0.6113451 , 0.61129254, 0.70750848, 0.65383581,\n",
              "        0.65387483, 0.65370874, 0.62541397, 0.65304284, 0.65491948,\n",
              "        0.65392763, 0.64668321, 0.62165209, 0.62112546, 0.62212164,\n",
              "        0.6325243 , 0.62083967, 0.62197659, 0.62173113, 0.5520581 ,\n",
              "        0.60104481, 0.6012944 , 0.60996205, 0.62344636, 0.60129222,\n",
              "        0.60121518, 0.61138448, 0.59869212, 0.61141734, 0.61134623,\n",
              "        0.6110592 , 0.60371179, 0.6113451 , 0.61134667, 0.61194117,\n",
              "        0.60737368, 0.65362762, 0.65351433, 0.65397503, 0.62981007,\n",
              "        0.65506626, 0.65383869, 0.65450032, 0.54901071, 0.62193106,\n",
              "        0.62206039, 0.62236428, 0.61535324, 0.62201752, 0.62156619,\n",
              "        0.62142274, 0.63717286, 0.6013052 , 0.60145723, 0.61096858,\n",
              "        0.60848366, 0.60153481, 0.6012944 , 0.61061371, 0.61705937,\n",
              "        0.61141734, 0.61141734, 0.61043856, 0.65228517, 0.61134667,\n",
              "        0.61141734, 0.61104515, 0.62296796, 0.65390614, 0.65389507,\n",
              "        0.65401114, 0.55229529, 0.65500201, 0.65367183, 0.65220405,\n",
              "        0.63946183, 0.62181024, 0.62200808, 0.62162705, 0.6237952 ,\n",
              "        0.62172843, 0.62228061, 0.62161015, 0.62749817, 0.60121518,\n",
              "        0.60145969, 0.61099515, 0.67374681, 0.60145669, 0.6012944 ,\n",
              "        0.61051606, 0.61402923, 0.61141734, 0.61141734, 0.610616  ,\n",
              "        0.59642328, 0.61141734, 0.61141734, 0.61057698, 0.60466512]),\n",
              " 'mean_train_recall': array([0.52949529, 0.52959007, 0.52958999, 0.50641277, 0.52922639,\n",
              "        0.52964635, 0.5295222 , 0.50779666, 0.53546348, 0.5355851 ,\n",
              "        0.53542001, 0.50709066, 0.53547613, 0.53546867, 0.53524319,\n",
              "        0.50661103, 0.54335071, 0.54318047, 0.5359342 , 0.50776346,\n",
              "        0.54319169, 0.54314561, 0.53628233, 0.50779187, 0.55614386,\n",
              "        0.55614386, 0.53576136, 0.50857011, 0.55511801, 0.55506935,\n",
              "        0.53630666, 0.50851418, 0.52968481, 0.52935441, 0.52944931,\n",
              "        0.5080357 , 0.52950037, 0.52933764, 0.52949522, 0.50775478,\n",
              "        0.53546353, 0.5354393 , 0.53557768, 0.50845873, 0.53548788,\n",
              "        0.53537642, 0.5354418 , 0.5065956 , 0.54313688, 0.54325092,\n",
              "        0.5356884 , 0.50765092, 0.54330715, 0.54323164, 0.53588556,\n",
              "        0.50738069, 0.55609521, 0.55609773, 0.53599951, 0.50875601,\n",
              "        0.55609773, 0.55609773, 0.53598027, 0.50677253, 0.52947344,\n",
              "        0.52940556, 0.52942745, 0.50757431, 0.5293763 , 0.5297066 ,\n",
              "        0.52940552, 0.50854628, 0.53543924, 0.53529835, 0.53549048,\n",
              "        0.5057405 , 0.5351601 , 0.53546616, 0.53539569, 0.50422545,\n",
              "        0.54308826, 0.5432048 , 0.5356666 , 0.50793414, 0.54316123,\n",
              "        0.54315867, 0.53600469, 0.50810161, 0.55614386, 0.55611953,\n",
              "        0.53595346, 0.50945995, 0.55609773, 0.55611954, 0.53608274,\n",
              "        0.50697741, 0.52944928, 0.52944916, 0.52951957, 0.50828551,\n",
              "        0.52961693, 0.52954146, 0.52963623, 0.50678751, 0.53548788,\n",
              "        0.53551218, 0.53553907, 0.50883819, 0.53544438, 0.53537129,\n",
              "        0.53534704, 0.50854283, 0.54318299, 0.54320987, 0.53590721,\n",
              "        0.50824241, 0.54327774, 0.5432048 , 0.53585612, 0.508221  ,\n",
              "        0.55614386, 0.55614386, 0.53576389, 0.50814021, 0.55611954,\n",
              "        0.55614386, 0.53593168, 0.50800904, 0.52945166, 0.52940568,\n",
              "        0.52958762, 0.5094008 , 0.52961703, 0.52944927, 0.52921361,\n",
              "        0.50811671, 0.53544172, 0.53553395, 0.53530593, 0.50773904,\n",
              "        0.53537387, 0.53558266, 0.53541742, 0.50660687, 0.54315867,\n",
              "        0.54323167, 0.53584196, 0.50718011, 0.54325344, 0.5432048 ,\n",
              "        0.5358535 , 0.50782938, 0.55614386, 0.55614386, 0.53585611,\n",
              "        0.504501  , 0.55614386, 0.55614386, 0.53576646, 0.50864773]),\n",
              " 'param_C': masked_array(data=[0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
              "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
              "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
              "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
              "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
              "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
              "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
              "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
              "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
              "                    0.5, 0.5, 0.5, 0.5, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "                    2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "                    2, 2, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
              "                    50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
              "                    50, 50, 50, 50, 50, 50],\n",
              "              mask=[False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False],\n",
              "        fill_value='?',\n",
              "             dtype=object),\n",
              " 'param_dual': masked_array(data=[False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False],\n",
              "              mask=[False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False],\n",
              "        fill_value='?',\n",
              "             dtype=object),\n",
              " 'param_max_iter': masked_array(data=[30, 30, 30, 30, 30, 30, 30, 30, 50, 50, 50, 50, 50, 50,\n",
              "                    50, 50, 100, 100, 100, 100, 100, 100, 100, 100, 200,\n",
              "                    200, 200, 200, 200, 200, 200, 200, 30, 30, 30, 30, 30,\n",
              "                    30, 30, 30, 50, 50, 50, 50, 50, 50, 50, 50, 100, 100,\n",
              "                    100, 100, 100, 100, 100, 100, 200, 200, 200, 200, 200,\n",
              "                    200, 200, 200, 30, 30, 30, 30, 30, 30, 30, 30, 50, 50,\n",
              "                    50, 50, 50, 50, 50, 50, 100, 100, 100, 100, 100, 100,\n",
              "                    100, 100, 200, 200, 200, 200, 200, 200, 200, 200, 30,\n",
              "                    30, 30, 30, 30, 30, 30, 30, 50, 50, 50, 50, 50, 50, 50,\n",
              "                    50, 100, 100, 100, 100, 100, 100, 100, 100, 200, 200,\n",
              "                    200, 200, 200, 200, 200, 200, 30, 30, 30, 30, 30, 30,\n",
              "                    30, 30, 50, 50, 50, 50, 50, 50, 50, 50, 100, 100, 100,\n",
              "                    100, 100, 100, 100, 100, 200, 200, 200, 200, 200, 200,\n",
              "                    200, 200],\n",
              "              mask=[False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False],\n",
              "        fill_value='?',\n",
              "             dtype=object),\n",
              " 'param_penalty': masked_array(data=['none', 'none', 'none', 'none', 'l2', 'l2', 'l2', 'l2',\n",
              "                    'none', 'none', 'none', 'none', 'l2', 'l2', 'l2', 'l2',\n",
              "                    'none', 'none', 'none', 'none', 'l2', 'l2', 'l2', 'l2',\n",
              "                    'none', 'none', 'none', 'none', 'l2', 'l2', 'l2', 'l2',\n",
              "                    'none', 'none', 'none', 'none', 'l2', 'l2', 'l2', 'l2',\n",
              "                    'none', 'none', 'none', 'none', 'l2', 'l2', 'l2', 'l2',\n",
              "                    'none', 'none', 'none', 'none', 'l2', 'l2', 'l2', 'l2',\n",
              "                    'none', 'none', 'none', 'none', 'l2', 'l2', 'l2', 'l2',\n",
              "                    'none', 'none', 'none', 'none', 'l2', 'l2', 'l2', 'l2',\n",
              "                    'none', 'none', 'none', 'none', 'l2', 'l2', 'l2', 'l2',\n",
              "                    'none', 'none', 'none', 'none', 'l2', 'l2', 'l2', 'l2',\n",
              "                    'none', 'none', 'none', 'none', 'l2', 'l2', 'l2', 'l2',\n",
              "                    'none', 'none', 'none', 'none', 'l2', 'l2', 'l2', 'l2',\n",
              "                    'none', 'none', 'none', 'none', 'l2', 'l2', 'l2', 'l2',\n",
              "                    'none', 'none', 'none', 'none', 'l2', 'l2', 'l2', 'l2',\n",
              "                    'none', 'none', 'none', 'none', 'l2', 'l2', 'l2', 'l2',\n",
              "                    'none', 'none', 'none', 'none', 'l2', 'l2', 'l2', 'l2',\n",
              "                    'none', 'none', 'none', 'none', 'l2', 'l2', 'l2', 'l2',\n",
              "                    'none', 'none', 'none', 'none', 'l2', 'l2', 'l2', 'l2',\n",
              "                    'none', 'none', 'none', 'none', 'l2', 'l2', 'l2', 'l2'],\n",
              "              mask=[False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False],\n",
              "        fill_value='?',\n",
              "             dtype=object),\n",
              " 'param_solver': masked_array(data=['saga', 'saga', 'saga', 'saga', 'saga', 'saga', 'saga',\n",
              "                    'saga', 'saga', 'saga', 'saga', 'saga', 'saga', 'saga',\n",
              "                    'saga', 'saga', 'saga', 'saga', 'saga', 'saga', 'saga',\n",
              "                    'saga', 'saga', 'saga', 'saga', 'saga', 'saga', 'saga',\n",
              "                    'saga', 'saga', 'saga', 'saga', 'saga', 'saga', 'saga',\n",
              "                    'saga', 'saga', 'saga', 'saga', 'saga', 'saga', 'saga',\n",
              "                    'saga', 'saga', 'saga', 'saga', 'saga', 'saga', 'saga',\n",
              "                    'saga', 'saga', 'saga', 'saga', 'saga', 'saga', 'saga',\n",
              "                    'saga', 'saga', 'saga', 'saga', 'saga', 'saga', 'saga',\n",
              "                    'saga', 'saga', 'saga', 'saga', 'saga', 'saga', 'saga',\n",
              "                    'saga', 'saga', 'saga', 'saga', 'saga', 'saga', 'saga',\n",
              "                    'saga', 'saga', 'saga', 'saga', 'saga', 'saga', 'saga',\n",
              "                    'saga', 'saga', 'saga', 'saga', 'saga', 'saga', 'saga',\n",
              "                    'saga', 'saga', 'saga', 'saga', 'saga', 'saga', 'saga',\n",
              "                    'saga', 'saga', 'saga', 'saga', 'saga', 'saga', 'saga',\n",
              "                    'saga', 'saga', 'saga', 'saga', 'saga', 'saga', 'saga',\n",
              "                    'saga', 'saga', 'saga', 'saga', 'saga', 'saga', 'saga',\n",
              "                    'saga', 'saga', 'saga', 'saga', 'saga', 'saga', 'saga',\n",
              "                    'saga', 'saga', 'saga', 'saga', 'saga', 'saga', 'saga',\n",
              "                    'saga', 'saga', 'saga', 'saga', 'saga', 'saga', 'saga',\n",
              "                    'saga', 'saga', 'saga', 'saga', 'saga', 'saga', 'saga',\n",
              "                    'saga', 'saga', 'saga', 'saga', 'saga', 'saga', 'saga',\n",
              "                    'saga', 'saga', 'saga', 'saga', 'saga', 'saga'],\n",
              "              mask=[False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False],\n",
              "        fill_value='?',\n",
              "             dtype=object),\n",
              " 'param_tol': masked_array(data=[0.0001, 1e-06, 0.01, 1, 0.0001, 1e-06, 0.01, 1, 0.0001,\n",
              "                    1e-06, 0.01, 1, 0.0001, 1e-06, 0.01, 1, 0.0001, 1e-06,\n",
              "                    0.01, 1, 0.0001, 1e-06, 0.01, 1, 0.0001, 1e-06, 0.01,\n",
              "                    1, 0.0001, 1e-06, 0.01, 1, 0.0001, 1e-06, 0.01, 1,\n",
              "                    0.0001, 1e-06, 0.01, 1, 0.0001, 1e-06, 0.01, 1, 0.0001,\n",
              "                    1e-06, 0.01, 1, 0.0001, 1e-06, 0.01, 1, 0.0001, 1e-06,\n",
              "                    0.01, 1, 0.0001, 1e-06, 0.01, 1, 0.0001, 1e-06, 0.01,\n",
              "                    1, 0.0001, 1e-06, 0.01, 1, 0.0001, 1e-06, 0.01, 1,\n",
              "                    0.0001, 1e-06, 0.01, 1, 0.0001, 1e-06, 0.01, 1, 0.0001,\n",
              "                    1e-06, 0.01, 1, 0.0001, 1e-06, 0.01, 1, 0.0001, 1e-06,\n",
              "                    0.01, 1, 0.0001, 1e-06, 0.01, 1, 0.0001, 1e-06, 0.01,\n",
              "                    1, 0.0001, 1e-06, 0.01, 1, 0.0001, 1e-06, 0.01, 1,\n",
              "                    0.0001, 1e-06, 0.01, 1, 0.0001, 1e-06, 0.01, 1, 0.0001,\n",
              "                    1e-06, 0.01, 1, 0.0001, 1e-06, 0.01, 1, 0.0001, 1e-06,\n",
              "                    0.01, 1, 0.0001, 1e-06, 0.01, 1, 0.0001, 1e-06, 0.01,\n",
              "                    1, 0.0001, 1e-06, 0.01, 1, 0.0001, 1e-06, 0.01, 1,\n",
              "                    0.0001, 1e-06, 0.01, 1, 0.0001, 1e-06, 0.01, 1, 0.0001,\n",
              "                    1e-06, 0.01, 1, 0.0001, 1e-06, 0.01, 1],\n",
              "              mask=[False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False],\n",
              "        fill_value='?',\n",
              "             dtype=object),\n",
              " 'params': [{'C': 0.01,\n",
              "   'dual': False,\n",
              "   'max_iter': 30,\n",
              "   'penalty': 'none',\n",
              "   'solver': 'saga',\n",
              "   'tol': 0.0001},\n",
              "  {'C': 0.01,\n",
              "   'dual': False,\n",
              "   'max_iter': 30,\n",
              "   'penalty': 'none',\n",
              "   'solver': 'saga',\n",
              "   'tol': 1e-06},\n",
              "  {'C': 0.01,\n",
              "   'dual': False,\n",
              "   'max_iter': 30,\n",
              "   'penalty': 'none',\n",
              "   'solver': 'saga',\n",
              "   'tol': 0.01},\n",
              "  {'C': 0.01,\n",
              "   'dual': False,\n",
              "   'max_iter': 30,\n",
              "   'penalty': 'none',\n",
              "   'solver': 'saga',\n",
              "   'tol': 1},\n",
              "  {'C': 0.01,\n",
              "   'dual': False,\n",
              "   'max_iter': 30,\n",
              "   'penalty': 'l2',\n",
              "   'solver': 'saga',\n",
              "   'tol': 0.0001},\n",
              "  {'C': 0.01,\n",
              "   'dual': False,\n",
              "   'max_iter': 30,\n",
              "   'penalty': 'l2',\n",
              "   'solver': 'saga',\n",
              "   'tol': 1e-06},\n",
              "  {'C': 0.01,\n",
              "   'dual': False,\n",
              "   'max_iter': 30,\n",
              "   'penalty': 'l2',\n",
              "   'solver': 'saga',\n",
              "   'tol': 0.01},\n",
              "  {'C': 0.01,\n",
              "   'dual': False,\n",
              "   'max_iter': 30,\n",
              "   'penalty': 'l2',\n",
              "   'solver': 'saga',\n",
              "   'tol': 1},\n",
              "  {'C': 0.01,\n",
              "   'dual': False,\n",
              "   'max_iter': 50,\n",
              "   'penalty': 'none',\n",
              "   'solver': 'saga',\n",
              "   'tol': 0.0001},\n",
              "  {'C': 0.01,\n",
              "   'dual': False,\n",
              "   'max_iter': 50,\n",
              "   'penalty': 'none',\n",
              "   'solver': 'saga',\n",
              "   'tol': 1e-06},\n",
              "  {'C': 0.01,\n",
              "   'dual': False,\n",
              "   'max_iter': 50,\n",
              "   'penalty': 'none',\n",
              "   'solver': 'saga',\n",
              "   'tol': 0.01},\n",
              "  {'C': 0.01,\n",
              "   'dual': False,\n",
              "   'max_iter': 50,\n",
              "   'penalty': 'none',\n",
              "   'solver': 'saga',\n",
              "   'tol': 1},\n",
              "  {'C': 0.01,\n",
              "   'dual': False,\n",
              "   'max_iter': 50,\n",
              "   'penalty': 'l2',\n",
              "   'solver': 'saga',\n",
              "   'tol': 0.0001},\n",
              "  {'C': 0.01,\n",
              "   'dual': False,\n",
              "   'max_iter': 50,\n",
              "   'penalty': 'l2',\n",
              "   'solver': 'saga',\n",
              "   'tol': 1e-06},\n",
              "  {'C': 0.01,\n",
              "   'dual': False,\n",
              "   'max_iter': 50,\n",
              "   'penalty': 'l2',\n",
              "   'solver': 'saga',\n",
              "   'tol': 0.01},\n",
              "  {'C': 0.01,\n",
              "   'dual': False,\n",
              "   'max_iter': 50,\n",
              "   'penalty': 'l2',\n",
              "   'solver': 'saga',\n",
              "   'tol': 1},\n",
              "  {'C': 0.01,\n",
              "   'dual': False,\n",
              "   'max_iter': 100,\n",
              "   'penalty': 'none',\n",
              "   'solver': 'saga',\n",
              "   'tol': 0.0001},\n",
              "  {'C': 0.01,\n",
              "   'dual': False,\n",
              "   'max_iter': 100,\n",
              "   'penalty': 'none',\n",
              "   'solver': 'saga',\n",
              "   'tol': 1e-06},\n",
              "  {'C': 0.01,\n",
              "   'dual': False,\n",
              "   'max_iter': 100,\n",
              "   'penalty': 'none',\n",
              "   'solver': 'saga',\n",
              "   'tol': 0.01},\n",
              "  {'C': 0.01,\n",
              "   'dual': False,\n",
              "   'max_iter': 100,\n",
              "   'penalty': 'none',\n",
              "   'solver': 'saga',\n",
              "   'tol': 1},\n",
              "  {'C': 0.01,\n",
              "   'dual': False,\n",
              "   'max_iter': 100,\n",
              "   'penalty': 'l2',\n",
              "   'solver': 'saga',\n",
              "   'tol': 0.0001},\n",
              "  {'C': 0.01,\n",
              "   'dual': False,\n",
              "   'max_iter': 100,\n",
              "   'penalty': 'l2',\n",
              "   'solver': 'saga',\n",
              "   'tol': 1e-06},\n",
              "  {'C': 0.01,\n",
              "   'dual': False,\n",
              "   'max_iter': 100,\n",
              "   'penalty': 'l2',\n",
              "   'solver': 'saga',\n",
              "   'tol': 0.01},\n",
              "  {'C': 0.01,\n",
              "   'dual': False,\n",
              "   'max_iter': 100,\n",
              "   'penalty': 'l2',\n",
              "   'solver': 'saga',\n",
              "   'tol': 1},\n",
              "  {'C': 0.01,\n",
              "   'dual': False,\n",
              "   'max_iter': 200,\n",
              "   'penalty': 'none',\n",
              "   'solver': 'saga',\n",
              "   'tol': 0.0001},\n",
              "  {'C': 0.01,\n",
              "   'dual': False,\n",
              "   'max_iter': 200,\n",
              "   'penalty': 'none',\n",
              "   'solver': 'saga',\n",
              "   'tol': 1e-06},\n",
              "  {'C': 0.01,\n",
              "   'dual': False,\n",
              "   'max_iter': 200,\n",
              "   'penalty': 'none',\n",
              "   'solver': 'saga',\n",
              "   'tol': 0.01},\n",
              "  {'C': 0.01,\n",
              "   'dual': False,\n",
              "   'max_iter': 200,\n",
              "   'penalty': 'none',\n",
              "   'solver': 'saga',\n",
              "   'tol': 1},\n",
              "  {'C': 0.01,\n",
              "   'dual': False,\n",
              "   'max_iter': 200,\n",
              "   'penalty': 'l2',\n",
              "   'solver': 'saga',\n",
              "   'tol': 0.0001},\n",
              "  {'C': 0.01,\n",
              "   'dual': False,\n",
              "   'max_iter': 200,\n",
              "   'penalty': 'l2',\n",
              "   'solver': 'saga',\n",
              "   'tol': 1e-06},\n",
              "  {'C': 0.01,\n",
              "   'dual': False,\n",
              "   'max_iter': 200,\n",
              "   'penalty': 'l2',\n",
              "   'solver': 'saga',\n",
              "   'tol': 0.01},\n",
              "  {'C': 0.01,\n",
              "   'dual': False,\n",
              "   'max_iter': 200,\n",
              "   'penalty': 'l2',\n",
              "   'solver': 'saga',\n",
              "   'tol': 1},\n",
              "  {'C': 0.1,\n",
              "   'dual': False,\n",
              "   'max_iter': 30,\n",
              "   'penalty': 'none',\n",
              "   'solver': 'saga',\n",
              "   'tol': 0.0001},\n",
              "  {'C': 0.1,\n",
              "   'dual': False,\n",
              "   'max_iter': 30,\n",
              "   'penalty': 'none',\n",
              "   'solver': 'saga',\n",
              "   'tol': 1e-06},\n",
              "  {'C': 0.1,\n",
              "   'dual': False,\n",
              "   'max_iter': 30,\n",
              "   'penalty': 'none',\n",
              "   'solver': 'saga',\n",
              "   'tol': 0.01},\n",
              "  {'C': 0.1,\n",
              "   'dual': False,\n",
              "   'max_iter': 30,\n",
              "   'penalty': 'none',\n",
              "   'solver': 'saga',\n",
              "   'tol': 1},\n",
              "  {'C': 0.1,\n",
              "   'dual': False,\n",
              "   'max_iter': 30,\n",
              "   'penalty': 'l2',\n",
              "   'solver': 'saga',\n",
              "   'tol': 0.0001},\n",
              "  {'C': 0.1,\n",
              "   'dual': False,\n",
              "   'max_iter': 30,\n",
              "   'penalty': 'l2',\n",
              "   'solver': 'saga',\n",
              "   'tol': 1e-06},\n",
              "  {'C': 0.1,\n",
              "   'dual': False,\n",
              "   'max_iter': 30,\n",
              "   'penalty': 'l2',\n",
              "   'solver': 'saga',\n",
              "   'tol': 0.01},\n",
              "  {'C': 0.1,\n",
              "   'dual': False,\n",
              "   'max_iter': 30,\n",
              "   'penalty': 'l2',\n",
              "   'solver': 'saga',\n",
              "   'tol': 1},\n",
              "  {'C': 0.1,\n",
              "   'dual': False,\n",
              "   'max_iter': 50,\n",
              "   'penalty': 'none',\n",
              "   'solver': 'saga',\n",
              "   'tol': 0.0001},\n",
              "  {'C': 0.1,\n",
              "   'dual': False,\n",
              "   'max_iter': 50,\n",
              "   'penalty': 'none',\n",
              "   'solver': 'saga',\n",
              "   'tol': 1e-06},\n",
              "  {'C': 0.1,\n",
              "   'dual': False,\n",
              "   'max_iter': 50,\n",
              "   'penalty': 'none',\n",
              "   'solver': 'saga',\n",
              "   'tol': 0.01},\n",
              "  {'C': 0.1,\n",
              "   'dual': False,\n",
              "   'max_iter': 50,\n",
              "   'penalty': 'none',\n",
              "   'solver': 'saga',\n",
              "   'tol': 1},\n",
              "  {'C': 0.1,\n",
              "   'dual': False,\n",
              "   'max_iter': 50,\n",
              "   'penalty': 'l2',\n",
              "   'solver': 'saga',\n",
              "   'tol': 0.0001},\n",
              "  {'C': 0.1,\n",
              "   'dual': False,\n",
              "   'max_iter': 50,\n",
              "   'penalty': 'l2',\n",
              "   'solver': 'saga',\n",
              "   'tol': 1e-06},\n",
              "  {'C': 0.1,\n",
              "   'dual': False,\n",
              "   'max_iter': 50,\n",
              "   'penalty': 'l2',\n",
              "   'solver': 'saga',\n",
              "   'tol': 0.01},\n",
              "  {'C': 0.1,\n",
              "   'dual': False,\n",
              "   'max_iter': 50,\n",
              "   'penalty': 'l2',\n",
              "   'solver': 'saga',\n",
              "   'tol': 1},\n",
              "  {'C': 0.1,\n",
              "   'dual': False,\n",
              "   'max_iter': 100,\n",
              "   'penalty': 'none',\n",
              "   'solver': 'saga',\n",
              "   'tol': 0.0001},\n",
              "  {'C': 0.1,\n",
              "   'dual': False,\n",
              "   'max_iter': 100,\n",
              "   'penalty': 'none',\n",
              "   'solver': 'saga',\n",
              "   'tol': 1e-06},\n",
              "  {'C': 0.1,\n",
              "   'dual': False,\n",
              "   'max_iter': 100,\n",
              "   'penalty': 'none',\n",
              "   'solver': 'saga',\n",
              "   'tol': 0.01},\n",
              "  {'C': 0.1,\n",
              "   'dual': False,\n",
              "   'max_iter': 100,\n",
              "   'penalty': 'none',\n",
              "   'solver': 'saga',\n",
              "   'tol': 1},\n",
              "  {'C': 0.1,\n",
              "   'dual': False,\n",
              "   'max_iter': 100,\n",
              "   'penalty': 'l2',\n",
              "   'solver': 'saga',\n",
              "   'tol': 0.0001},\n",
              "  {'C': 0.1,\n",
              "   'dual': False,\n",
              "   'max_iter': 100,\n",
              "   'penalty': 'l2',\n",
              "   'solver': 'saga',\n",
              "   'tol': 1e-06},\n",
              "  {'C': 0.1,\n",
              "   'dual': False,\n",
              "   'max_iter': 100,\n",
              "   'penalty': 'l2',\n",
              "   'solver': 'saga',\n",
              "   'tol': 0.01},\n",
              "  {'C': 0.1,\n",
              "   'dual': False,\n",
              "   'max_iter': 100,\n",
              "   'penalty': 'l2',\n",
              "   'solver': 'saga',\n",
              "   'tol': 1},\n",
              "  {'C': 0.1,\n",
              "   'dual': False,\n",
              "   'max_iter': 200,\n",
              "   'penalty': 'none',\n",
              "   'solver': 'saga',\n",
              "   'tol': 0.0001},\n",
              "  {'C': 0.1,\n",
              "   'dual': False,\n",
              "   'max_iter': 200,\n",
              "   'penalty': 'none',\n",
              "   'solver': 'saga',\n",
              "   'tol': 1e-06},\n",
              "  {'C': 0.1,\n",
              "   'dual': False,\n",
              "   'max_iter': 200,\n",
              "   'penalty': 'none',\n",
              "   'solver': 'saga',\n",
              "   'tol': 0.01},\n",
              "  {'C': 0.1,\n",
              "   'dual': False,\n",
              "   'max_iter': 200,\n",
              "   'penalty': 'none',\n",
              "   'solver': 'saga',\n",
              "   'tol': 1},\n",
              "  {'C': 0.1,\n",
              "   'dual': False,\n",
              "   'max_iter': 200,\n",
              "   'penalty': 'l2',\n",
              "   'solver': 'saga',\n",
              "   'tol': 0.0001},\n",
              "  {'C': 0.1,\n",
              "   'dual': False,\n",
              "   'max_iter': 200,\n",
              "   'penalty': 'l2',\n",
              "   'solver': 'saga',\n",
              "   'tol': 1e-06},\n",
              "  {'C': 0.1,\n",
              "   'dual': False,\n",
              "   'max_iter': 200,\n",
              "   'penalty': 'l2',\n",
              "   'solver': 'saga',\n",
              "   'tol': 0.01},\n",
              "  {'C': 0.1,\n",
              "   'dual': False,\n",
              "   'max_iter': 200,\n",
              "   'penalty': 'l2',\n",
              "   'solver': 'saga',\n",
              "   'tol': 1},\n",
              "  {'C': 0.5,\n",
              "   'dual': False,\n",
              "   'max_iter': 30,\n",
              "   'penalty': 'none',\n",
              "   'solver': 'saga',\n",
              "   'tol': 0.0001},\n",
              "  {'C': 0.5,\n",
              "   'dual': False,\n",
              "   'max_iter': 30,\n",
              "   'penalty': 'none',\n",
              "   'solver': 'saga',\n",
              "   'tol': 1e-06},\n",
              "  {'C': 0.5,\n",
              "   'dual': False,\n",
              "   'max_iter': 30,\n",
              "   'penalty': 'none',\n",
              "   'solver': 'saga',\n",
              "   'tol': 0.01},\n",
              "  {'C': 0.5,\n",
              "   'dual': False,\n",
              "   'max_iter': 30,\n",
              "   'penalty': 'none',\n",
              "   'solver': 'saga',\n",
              "   'tol': 1},\n",
              "  {'C': 0.5,\n",
              "   'dual': False,\n",
              "   'max_iter': 30,\n",
              "   'penalty': 'l2',\n",
              "   'solver': 'saga',\n",
              "   'tol': 0.0001},\n",
              "  {'C': 0.5,\n",
              "   'dual': False,\n",
              "   'max_iter': 30,\n",
              "   'penalty': 'l2',\n",
              "   'solver': 'saga',\n",
              "   'tol': 1e-06},\n",
              "  {'C': 0.5,\n",
              "   'dual': False,\n",
              "   'max_iter': 30,\n",
              "   'penalty': 'l2',\n",
              "   'solver': 'saga',\n",
              "   'tol': 0.01},\n",
              "  {'C': 0.5,\n",
              "   'dual': False,\n",
              "   'max_iter': 30,\n",
              "   'penalty': 'l2',\n",
              "   'solver': 'saga',\n",
              "   'tol': 1},\n",
              "  {'C': 0.5,\n",
              "   'dual': False,\n",
              "   'max_iter': 50,\n",
              "   'penalty': 'none',\n",
              "   'solver': 'saga',\n",
              "   'tol': 0.0001},\n",
              "  {'C': 0.5,\n",
              "   'dual': False,\n",
              "   'max_iter': 50,\n",
              "   'penalty': 'none',\n",
              "   'solver': 'saga',\n",
              "   'tol': 1e-06},\n",
              "  {'C': 0.5,\n",
              "   'dual': False,\n",
              "   'max_iter': 50,\n",
              "   'penalty': 'none',\n",
              "   'solver': 'saga',\n",
              "   'tol': 0.01},\n",
              "  {'C': 0.5,\n",
              "   'dual': False,\n",
              "   'max_iter': 50,\n",
              "   'penalty': 'none',\n",
              "   'solver': 'saga',\n",
              "   'tol': 1},\n",
              "  {'C': 0.5,\n",
              "   'dual': False,\n",
              "   'max_iter': 50,\n",
              "   'penalty': 'l2',\n",
              "   'solver': 'saga',\n",
              "   'tol': 0.0001},\n",
              "  {'C': 0.5,\n",
              "   'dual': False,\n",
              "   'max_iter': 50,\n",
              "   'penalty': 'l2',\n",
              "   'solver': 'saga',\n",
              "   'tol': 1e-06},\n",
              "  {'C': 0.5,\n",
              "   'dual': False,\n",
              "   'max_iter': 50,\n",
              "   'penalty': 'l2',\n",
              "   'solver': 'saga',\n",
              "   'tol': 0.01},\n",
              "  {'C': 0.5,\n",
              "   'dual': False,\n",
              "   'max_iter': 50,\n",
              "   'penalty': 'l2',\n",
              "   'solver': 'saga',\n",
              "   'tol': 1},\n",
              "  {'C': 0.5,\n",
              "   'dual': False,\n",
              "   'max_iter': 100,\n",
              "   'penalty': 'none',\n",
              "   'solver': 'saga',\n",
              "   'tol': 0.0001},\n",
              "  {'C': 0.5,\n",
              "   'dual': False,\n",
              "   'max_iter': 100,\n",
              "   'penalty': 'none',\n",
              "   'solver': 'saga',\n",
              "   'tol': 1e-06},\n",
              "  {'C': 0.5,\n",
              "   'dual': False,\n",
              "   'max_iter': 100,\n",
              "   'penalty': 'none',\n",
              "   'solver': 'saga',\n",
              "   'tol': 0.01},\n",
              "  {'C': 0.5,\n",
              "   'dual': False,\n",
              "   'max_iter': 100,\n",
              "   'penalty': 'none',\n",
              "   'solver': 'saga',\n",
              "   'tol': 1},\n",
              "  {'C': 0.5,\n",
              "   'dual': False,\n",
              "   'max_iter': 100,\n",
              "   'penalty': 'l2',\n",
              "   'solver': 'saga',\n",
              "   'tol': 0.0001},\n",
              "  {'C': 0.5,\n",
              "   'dual': False,\n",
              "   'max_iter': 100,\n",
              "   'penalty': 'l2',\n",
              "   'solver': 'saga',\n",
              "   'tol': 1e-06},\n",
              "  {'C': 0.5,\n",
              "   'dual': False,\n",
              "   'max_iter': 100,\n",
              "   'penalty': 'l2',\n",
              "   'solver': 'saga',\n",
              "   'tol': 0.01},\n",
              "  {'C': 0.5,\n",
              "   'dual': False,\n",
              "   'max_iter': 100,\n",
              "   'penalty': 'l2',\n",
              "   'solver': 'saga',\n",
              "   'tol': 1},\n",
              "  {'C': 0.5,\n",
              "   'dual': False,\n",
              "   'max_iter': 200,\n",
              "   'penalty': 'none',\n",
              "   'solver': 'saga',\n",
              "   'tol': 0.0001},\n",
              "  {'C': 0.5,\n",
              "   'dual': False,\n",
              "   'max_iter': 200,\n",
              "   'penalty': 'none',\n",
              "   'solver': 'saga',\n",
              "   'tol': 1e-06},\n",
              "  {'C': 0.5,\n",
              "   'dual': False,\n",
              "   'max_iter': 200,\n",
              "   'penalty': 'none',\n",
              "   'solver': 'saga',\n",
              "   'tol': 0.01},\n",
              "  {'C': 0.5,\n",
              "   'dual': False,\n",
              "   'max_iter': 200,\n",
              "   'penalty': 'none',\n",
              "   'solver': 'saga',\n",
              "   'tol': 1},\n",
              "  {'C': 0.5,\n",
              "   'dual': False,\n",
              "   'max_iter': 200,\n",
              "   'penalty': 'l2',\n",
              "   'solver': 'saga',\n",
              "   'tol': 0.0001},\n",
              "  {'C': 0.5,\n",
              "   'dual': False,\n",
              "   'max_iter': 200,\n",
              "   'penalty': 'l2',\n",
              "   'solver': 'saga',\n",
              "   'tol': 1e-06},\n",
              "  {'C': 0.5,\n",
              "   'dual': False,\n",
              "   'max_iter': 200,\n",
              "   'penalty': 'l2',\n",
              "   'solver': 'saga',\n",
              "   'tol': 0.01},\n",
              "  {'C': 0.5,\n",
              "   'dual': False,\n",
              "   'max_iter': 200,\n",
              "   'penalty': 'l2',\n",
              "   'solver': 'saga',\n",
              "   'tol': 1},\n",
              "  {'C': 2,\n",
              "   'dual': False,\n",
              "   'max_iter': 30,\n",
              "   'penalty': 'none',\n",
              "   'solver': 'saga',\n",
              "   'tol': 0.0001},\n",
              "  {'C': 2,\n",
              "   'dual': False,\n",
              "   'max_iter': 30,\n",
              "   'penalty': 'none',\n",
              "   'solver': 'saga',\n",
              "   'tol': 1e-06},\n",
              "  {'C': 2,\n",
              "   'dual': False,\n",
              "   'max_iter': 30,\n",
              "   'penalty': 'none',\n",
              "   'solver': 'saga',\n",
              "   'tol': 0.01},\n",
              "  {'C': 2,\n",
              "   'dual': False,\n",
              "   'max_iter': 30,\n",
              "   'penalty': 'none',\n",
              "   'solver': 'saga',\n",
              "   'tol': 1},\n",
              "  {'C': 2,\n",
              "   'dual': False,\n",
              "   'max_iter': 30,\n",
              "   'penalty': 'l2',\n",
              "   'solver': 'saga',\n",
              "   'tol': 0.0001},\n",
              "  {'C': 2,\n",
              "   'dual': False,\n",
              "   'max_iter': 30,\n",
              "   'penalty': 'l2',\n",
              "   'solver': 'saga',\n",
              "   'tol': 1e-06},\n",
              "  {'C': 2,\n",
              "   'dual': False,\n",
              "   'max_iter': 30,\n",
              "   'penalty': 'l2',\n",
              "   'solver': 'saga',\n",
              "   'tol': 0.01},\n",
              "  {'C': 2,\n",
              "   'dual': False,\n",
              "   'max_iter': 30,\n",
              "   'penalty': 'l2',\n",
              "   'solver': 'saga',\n",
              "   'tol': 1},\n",
              "  {'C': 2,\n",
              "   'dual': False,\n",
              "   'max_iter': 50,\n",
              "   'penalty': 'none',\n",
              "   'solver': 'saga',\n",
              "   'tol': 0.0001},\n",
              "  {'C': 2,\n",
              "   'dual': False,\n",
              "   'max_iter': 50,\n",
              "   'penalty': 'none',\n",
              "   'solver': 'saga',\n",
              "   'tol': 1e-06},\n",
              "  {'C': 2,\n",
              "   'dual': False,\n",
              "   'max_iter': 50,\n",
              "   'penalty': 'none',\n",
              "   'solver': 'saga',\n",
              "   'tol': 0.01},\n",
              "  {'C': 2,\n",
              "   'dual': False,\n",
              "   'max_iter': 50,\n",
              "   'penalty': 'none',\n",
              "   'solver': 'saga',\n",
              "   'tol': 1},\n",
              "  {'C': 2,\n",
              "   'dual': False,\n",
              "   'max_iter': 50,\n",
              "   'penalty': 'l2',\n",
              "   'solver': 'saga',\n",
              "   'tol': 0.0001},\n",
              "  {'C': 2,\n",
              "   'dual': False,\n",
              "   'max_iter': 50,\n",
              "   'penalty': 'l2',\n",
              "   'solver': 'saga',\n",
              "   'tol': 1e-06},\n",
              "  {'C': 2,\n",
              "   'dual': False,\n",
              "   'max_iter': 50,\n",
              "   'penalty': 'l2',\n",
              "   'solver': 'saga',\n",
              "   'tol': 0.01},\n",
              "  {'C': 2,\n",
              "   'dual': False,\n",
              "   'max_iter': 50,\n",
              "   'penalty': 'l2',\n",
              "   'solver': 'saga',\n",
              "   'tol': 1},\n",
              "  {'C': 2,\n",
              "   'dual': False,\n",
              "   'max_iter': 100,\n",
              "   'penalty': 'none',\n",
              "   'solver': 'saga',\n",
              "   'tol': 0.0001},\n",
              "  {'C': 2,\n",
              "   'dual': False,\n",
              "   'max_iter': 100,\n",
              "   'penalty': 'none',\n",
              "   'solver': 'saga',\n",
              "   'tol': 1e-06},\n",
              "  {'C': 2,\n",
              "   'dual': False,\n",
              "   'max_iter': 100,\n",
              "   'penalty': 'none',\n",
              "   'solver': 'saga',\n",
              "   'tol': 0.01},\n",
              "  {'C': 2,\n",
              "   'dual': False,\n",
              "   'max_iter': 100,\n",
              "   'penalty': 'none',\n",
              "   'solver': 'saga',\n",
              "   'tol': 1},\n",
              "  {'C': 2,\n",
              "   'dual': False,\n",
              "   'max_iter': 100,\n",
              "   'penalty': 'l2',\n",
              "   'solver': 'saga',\n",
              "   'tol': 0.0001},\n",
              "  {'C': 2,\n",
              "   'dual': False,\n",
              "   'max_iter': 100,\n",
              "   'penalty': 'l2',\n",
              "   'solver': 'saga',\n",
              "   'tol': 1e-06},\n",
              "  {'C': 2,\n",
              "   'dual': False,\n",
              "   'max_iter': 100,\n",
              "   'penalty': 'l2',\n",
              "   'solver': 'saga',\n",
              "   'tol': 0.01},\n",
              "  {'C': 2,\n",
              "   'dual': False,\n",
              "   'max_iter': 100,\n",
              "   'penalty': 'l2',\n",
              "   'solver': 'saga',\n",
              "   'tol': 1},\n",
              "  {'C': 2,\n",
              "   'dual': False,\n",
              "   'max_iter': 200,\n",
              "   'penalty': 'none',\n",
              "   'solver': 'saga',\n",
              "   'tol': 0.0001},\n",
              "  {'C': 2,\n",
              "   'dual': False,\n",
              "   'max_iter': 200,\n",
              "   'penalty': 'none',\n",
              "   'solver': 'saga',\n",
              "   'tol': 1e-06},\n",
              "  {'C': 2,\n",
              "   'dual': False,\n",
              "   'max_iter': 200,\n",
              "   'penalty': 'none',\n",
              "   'solver': 'saga',\n",
              "   'tol': 0.01},\n",
              "  {'C': 2,\n",
              "   'dual': False,\n",
              "   'max_iter': 200,\n",
              "   'penalty': 'none',\n",
              "   'solver': 'saga',\n",
              "   'tol': 1},\n",
              "  {'C': 2,\n",
              "   'dual': False,\n",
              "   'max_iter': 200,\n",
              "   'penalty': 'l2',\n",
              "   'solver': 'saga',\n",
              "   'tol': 0.0001},\n",
              "  {'C': 2,\n",
              "   'dual': False,\n",
              "   'max_iter': 200,\n",
              "   'penalty': 'l2',\n",
              "   'solver': 'saga',\n",
              "   'tol': 1e-06},\n",
              "  {'C': 2,\n",
              "   'dual': False,\n",
              "   'max_iter': 200,\n",
              "   'penalty': 'l2',\n",
              "   'solver': 'saga',\n",
              "   'tol': 0.01},\n",
              "  {'C': 2,\n",
              "   'dual': False,\n",
              "   'max_iter': 200,\n",
              "   'penalty': 'l2',\n",
              "   'solver': 'saga',\n",
              "   'tol': 1},\n",
              "  {'C': 50,\n",
              "   'dual': False,\n",
              "   'max_iter': 30,\n",
              "   'penalty': 'none',\n",
              "   'solver': 'saga',\n",
              "   'tol': 0.0001},\n",
              "  {'C': 50,\n",
              "   'dual': False,\n",
              "   'max_iter': 30,\n",
              "   'penalty': 'none',\n",
              "   'solver': 'saga',\n",
              "   'tol': 1e-06},\n",
              "  {'C': 50,\n",
              "   'dual': False,\n",
              "   'max_iter': 30,\n",
              "   'penalty': 'none',\n",
              "   'solver': 'saga',\n",
              "   'tol': 0.01},\n",
              "  {'C': 50,\n",
              "   'dual': False,\n",
              "   'max_iter': 30,\n",
              "   'penalty': 'none',\n",
              "   'solver': 'saga',\n",
              "   'tol': 1},\n",
              "  {'C': 50,\n",
              "   'dual': False,\n",
              "   'max_iter': 30,\n",
              "   'penalty': 'l2',\n",
              "   'solver': 'saga',\n",
              "   'tol': 0.0001},\n",
              "  {'C': 50,\n",
              "   'dual': False,\n",
              "   'max_iter': 30,\n",
              "   'penalty': 'l2',\n",
              "   'solver': 'saga',\n",
              "   'tol': 1e-06},\n",
              "  {'C': 50,\n",
              "   'dual': False,\n",
              "   'max_iter': 30,\n",
              "   'penalty': 'l2',\n",
              "   'solver': 'saga',\n",
              "   'tol': 0.01},\n",
              "  {'C': 50,\n",
              "   'dual': False,\n",
              "   'max_iter': 30,\n",
              "   'penalty': 'l2',\n",
              "   'solver': 'saga',\n",
              "   'tol': 1},\n",
              "  {'C': 50,\n",
              "   'dual': False,\n",
              "   'max_iter': 50,\n",
              "   'penalty': 'none',\n",
              "   'solver': 'saga',\n",
              "   'tol': 0.0001},\n",
              "  {'C': 50,\n",
              "   'dual': False,\n",
              "   'max_iter': 50,\n",
              "   'penalty': 'none',\n",
              "   'solver': 'saga',\n",
              "   'tol': 1e-06},\n",
              "  {'C': 50,\n",
              "   'dual': False,\n",
              "   'max_iter': 50,\n",
              "   'penalty': 'none',\n",
              "   'solver': 'saga',\n",
              "   'tol': 0.01},\n",
              "  {'C': 50,\n",
              "   'dual': False,\n",
              "   'max_iter': 50,\n",
              "   'penalty': 'none',\n",
              "   'solver': 'saga',\n",
              "   'tol': 1},\n",
              "  {'C': 50,\n",
              "   'dual': False,\n",
              "   'max_iter': 50,\n",
              "   'penalty': 'l2',\n",
              "   'solver': 'saga',\n",
              "   'tol': 0.0001},\n",
              "  {'C': 50,\n",
              "   'dual': False,\n",
              "   'max_iter': 50,\n",
              "   'penalty': 'l2',\n",
              "   'solver': 'saga',\n",
              "   'tol': 1e-06},\n",
              "  {'C': 50,\n",
              "   'dual': False,\n",
              "   'max_iter': 50,\n",
              "   'penalty': 'l2',\n",
              "   'solver': 'saga',\n",
              "   'tol': 0.01},\n",
              "  {'C': 50,\n",
              "   'dual': False,\n",
              "   'max_iter': 50,\n",
              "   'penalty': 'l2',\n",
              "   'solver': 'saga',\n",
              "   'tol': 1},\n",
              "  {'C': 50,\n",
              "   'dual': False,\n",
              "   'max_iter': 100,\n",
              "   'penalty': 'none',\n",
              "   'solver': 'saga',\n",
              "   'tol': 0.0001},\n",
              "  {'C': 50,\n",
              "   'dual': False,\n",
              "   'max_iter': 100,\n",
              "   'penalty': 'none',\n",
              "   'solver': 'saga',\n",
              "   'tol': 1e-06},\n",
              "  {'C': 50,\n",
              "   'dual': False,\n",
              "   'max_iter': 100,\n",
              "   'penalty': 'none',\n",
              "   'solver': 'saga',\n",
              "   'tol': 0.01},\n",
              "  {'C': 50,\n",
              "   'dual': False,\n",
              "   'max_iter': 100,\n",
              "   'penalty': 'none',\n",
              "   'solver': 'saga',\n",
              "   'tol': 1},\n",
              "  {'C': 50,\n",
              "   'dual': False,\n",
              "   'max_iter': 100,\n",
              "   'penalty': 'l2',\n",
              "   'solver': 'saga',\n",
              "   'tol': 0.0001},\n",
              "  {'C': 50,\n",
              "   'dual': False,\n",
              "   'max_iter': 100,\n",
              "   'penalty': 'l2',\n",
              "   'solver': 'saga',\n",
              "   'tol': 1e-06},\n",
              "  {'C': 50,\n",
              "   'dual': False,\n",
              "   'max_iter': 100,\n",
              "   'penalty': 'l2',\n",
              "   'solver': 'saga',\n",
              "   'tol': 0.01},\n",
              "  {'C': 50,\n",
              "   'dual': False,\n",
              "   'max_iter': 100,\n",
              "   'penalty': 'l2',\n",
              "   'solver': 'saga',\n",
              "   'tol': 1},\n",
              "  {'C': 50,\n",
              "   'dual': False,\n",
              "   'max_iter': 200,\n",
              "   'penalty': 'none',\n",
              "   'solver': 'saga',\n",
              "   'tol': 0.0001},\n",
              "  {'C': 50,\n",
              "   'dual': False,\n",
              "   'max_iter': 200,\n",
              "   'penalty': 'none',\n",
              "   'solver': 'saga',\n",
              "   'tol': 1e-06},\n",
              "  {'C': 50,\n",
              "   'dual': False,\n",
              "   'max_iter': 200,\n",
              "   'penalty': 'none',\n",
              "   'solver': 'saga',\n",
              "   'tol': 0.01},\n",
              "  {'C': 50,\n",
              "   'dual': False,\n",
              "   'max_iter': 200,\n",
              "   'penalty': 'none',\n",
              "   'solver': 'saga',\n",
              "   'tol': 1},\n",
              "  {'C': 50,\n",
              "   'dual': False,\n",
              "   'max_iter': 200,\n",
              "   'penalty': 'l2',\n",
              "   'solver': 'saga',\n",
              "   'tol': 0.0001},\n",
              "  {'C': 50,\n",
              "   'dual': False,\n",
              "   'max_iter': 200,\n",
              "   'penalty': 'l2',\n",
              "   'solver': 'saga',\n",
              "   'tol': 1e-06},\n",
              "  {'C': 50,\n",
              "   'dual': False,\n",
              "   'max_iter': 200,\n",
              "   'penalty': 'l2',\n",
              "   'solver': 'saga',\n",
              "   'tol': 0.01},\n",
              "  {'C': 50,\n",
              "   'dual': False,\n",
              "   'max_iter': 200,\n",
              "   'penalty': 'l2',\n",
              "   'solver': 'saga',\n",
              "   'tol': 1}],\n",
              " 'rank_test_accuracy': array([ 16,  16,  16, 159,   4,   1,  16, 131,  77,  52,  52, 140,  52,\n",
              "         51,  52, 158,  81,  81, 109, 153,  81,  81, 104, 148,  37,  37,\n",
              "        101, 160,  31,  31, 104, 141,  16,  16,   4, 125,   1,   4,  28,\n",
              "        157,  52,  52,  52, 135,  52,  52,  52, 151,  81,  81, 109, 123,\n",
              "         98,  81, 101, 154,  35,  35, 109, 122,  33,  33, 117, 121,   4,\n",
              "          4,   4, 149,   4,  16,   4, 137,  52,  52,  77, 136,  52,  52,\n",
              "         52, 144,  98,  81, 107, 126,  81,  81, 109, 128,  37,  37, 109,\n",
              "        156,  37,  37, 109, 150,   1,  16,  28, 146,  16,   4,   4, 145,\n",
              "         77,  52,  80, 155,  52,  52,  52, 129,  81,  98, 120, 143,  81,\n",
              "         81, 106, 130,  37,  37, 117, 132,  37,  37, 117, 127,  16,   4,\n",
              "         16, 152,   4,  28,  16, 142,  52,  52,  52, 147,  52,  52,  52,\n",
              "        138,  81,  81, 115, 124,  81,  81, 107, 133,  37,  37, 115, 134,\n",
              "         37,  37, 101, 139], dtype=int32),\n",
              " 'rank_test_balanced_accuracy': array([107, 107, 107, 159,  95,  91, 107, 133,  84,  45,  45, 147,  45,\n",
              "         44,  45, 160,  21,  21,  80, 155,  36,  36,  75, 149,   7,   7,\n",
              "         42, 158,   5,   5,  75, 144, 107, 107,  95, 123,  91,  95, 118,\n",
              "        143,  45,  45,  45, 121,  45,  45,  45, 150,  21,  21,  73, 131,\n",
              "         38,  21,  42, 127,   3,   3,  80, 122,   1,   1,  88, 130,  95,\n",
              "         95,  95, 140,  95, 107,  94, 146,  45,  45,  77, 139,  45,  45,\n",
              "         45, 137,  38,  21,  70, 125,  21,  21,  80, 126,   7,   7,  73,\n",
              "        157,   7,   7,  80, 152,  91, 107, 118, 151, 107,  95,  95, 134,\n",
              "         77,  45,  77, 156,  45,  45,  45, 135,  21,  38,  90, 138,  21,\n",
              "         21,  70, 129,   7,   7,  87, 145,   7,   7,  88, 132, 107,  95,\n",
              "        107, 141,  95, 118, 106, 154,  45,  45,  45, 148,  45,  45,  45,\n",
              "        153,  21,  21,  85, 128,  21,  21,  70, 124,   7,   7,  85, 142,\n",
              "          7,   7,  41, 136], dtype=int32),\n",
              " 'rank_test_f1': array([107, 107, 107, 140,  96,  92, 107, 141,  90,  62,  62, 152,  62,\n",
              "         61,  62, 160,  21,  21,  50, 150,  36,  36,  56, 142,   5,   5,\n",
              "         42, 135,  19,  19,  56, 147, 107, 107, 104, 126,  92,  96, 118,\n",
              "        124,  62,  62,  62, 122,  62,  62,  62, 136,  21,  21,  47, 159,\n",
              "         38,  21,  42, 121,   3,   3,  50, 132,   1,   1,  58, 158,  96,\n",
              "         96,  96, 129,  96, 107,  95, 154,  62,  62,  88, 145,  62,  62,\n",
              "         62, 138,  38,  21,  45, 130,  21,  21,  49, 127,   5,   5,  47,\n",
              "        146,   5,   5,  50, 137,  92, 107, 118, 144, 107,  96, 104, 128,\n",
              "         88,  62,  87, 148,  62,  62,  62, 151,  21,  38,  60, 133,  21,\n",
              "         21,  44, 131,   5,   5,  55, 156,   5,   5,  58, 143, 107, 104,\n",
              "        107, 125,  96, 118,  91, 155,  62,  62,  62, 139,  62,  62,  62,\n",
              "        157,  21,  21,  53, 149,  21,  21,  45, 123,   5,   5,  53, 153,\n",
              "          5,   5,  41, 134], dtype=int32),\n",
              " 'rank_test_precision': array([ 21,  21,  21, 153,   7,   4,  21,   3,  68,  42,  42, 134,  42,\n",
              "         41,  42, 160,  97,  97, 123, 150, 114, 114, 112, 151,  79,  79,\n",
              "         94, 157,  72,  72, 112, 154,  21,  21,  15,   1,   4,   7,  33,\n",
              "        139,  42,  42,  42,  71,  42,  42,  42, 156,  97,  97, 126,  20,\n",
              "        120,  97,  94, 140,  77,  77, 123,  38,  74,  74, 131, 137,   7,\n",
              "          7,   7, 144,   7,  21,  19, 136,  42,  42,  69,  76,  42,  42,\n",
              "         42, 138, 120,  97, 117,  18,  97,  97, 119, 146,  79,  79, 126,\n",
              "        152,  79,  79, 123, 147,   4,  21,  33, 128,  21,   7,  15, 158,\n",
              "         69,  42,  67, 143,  42,  42,  42,  36,  97, 120, 135,  93,  97,\n",
              "         97, 116,  37,  79,  79, 133, 155,  79,  79, 131,  39,  21,  15,\n",
              "         21, 159,   7,  33,  32,  40,  42,  42,  42, 148,  42,  42,  42,\n",
              "        145,  97,  97, 129,   2,  97,  97, 117, 149,  79,  79, 129, 141,\n",
              "         79,  79,  96, 142], dtype=int32),\n",
              " 'rank_test_recall': array([107, 107, 107, 159,  95,  91, 107, 133,  84,  45,  45, 147,  45,\n",
              "         44,  45, 160,  21,  21,  80, 155,  36,  36,  75, 149,   7,   7,\n",
              "         42, 158,   5,   5,  75, 144, 107, 107,  95, 123,  91,  95, 118,\n",
              "        143,  45,  45,  45, 121,  45,  45,  45, 150,  21,  21,  73, 131,\n",
              "         38,  21,  42, 127,   3,   3,  80, 122,   1,   1,  88, 130,  95,\n",
              "         95,  95, 140,  95, 107,  94, 146,  45,  45,  77, 139,  45,  45,\n",
              "         45, 137,  38,  21,  70, 125,  21,  21,  80, 126,   7,   7,  73,\n",
              "        157,   7,   7,  80, 152,  91, 107, 118, 151, 107,  95,  95, 134,\n",
              "         77,  45,  77, 156,  45,  45,  45, 135,  21,  38,  90, 138,  21,\n",
              "         21,  70, 129,   7,   7,  87, 145,   7,   7,  88, 132, 107,  95,\n",
              "        107, 141,  95, 118, 106, 154,  45,  45,  45, 148,  45,  45,  45,\n",
              "        153,  21,  21,  85, 128,  21,  21,  70, 124,   7,   7,  85, 142,\n",
              "          7,   7,  41, 136], dtype=int32),\n",
              " 'split0_test_accuracy': array([0.66189112, 0.66189112, 0.66189112, 0.64469914, 0.66189112,\n",
              "        0.66189112, 0.66189112, 0.6504298 , 0.66475645, 0.66475645,\n",
              "        0.66475645, 0.6504298 , 0.66475645, 0.66475645, 0.66475645,\n",
              "        0.64469914, 0.65616046, 0.65616046, 0.65902579, 0.6504298 ,\n",
              "        0.65616046, 0.65616046, 0.65902579, 0.65616046, 0.64469914,\n",
              "        0.64469914, 0.65902579, 0.63610315, 0.64756447, 0.64756447,\n",
              "        0.65902579, 0.63037249, 0.66189112, 0.66189112, 0.66189112,\n",
              "        0.64469914, 0.66189112, 0.66189112, 0.66189112, 0.64756447,\n",
              "        0.66475645, 0.66475645, 0.66475645, 0.65616046, 0.66475645,\n",
              "        0.66475645, 0.66475645, 0.62750716, 0.65616046, 0.65616046,\n",
              "        0.65902579, 0.64756447, 0.65616046, 0.65616046, 0.65902579,\n",
              "        0.63323782, 0.64469914, 0.64469914, 0.65902579, 0.65616046,\n",
              "        0.64756447, 0.64756447, 0.65902579, 0.65329513, 0.66189112,\n",
              "        0.66189112, 0.66189112, 0.64469914, 0.66189112, 0.66189112,\n",
              "        0.66475645, 0.65329513, 0.66475645, 0.66475645, 0.66475645,\n",
              "        0.64756447, 0.66475645, 0.66475645, 0.66475645, 0.62750716,\n",
              "        0.65616046, 0.65616046, 0.65902579, 0.64469914, 0.65616046,\n",
              "        0.65616046, 0.65902579, 0.64756447, 0.64469914, 0.64469914,\n",
              "        0.65902579, 0.64469914, 0.64469914, 0.64469914, 0.65902579,\n",
              "        0.63610315, 0.66189112, 0.66189112, 0.66189112, 0.64756447,\n",
              "        0.66189112, 0.66189112, 0.66189112, 0.6504298 , 0.66475645,\n",
              "        0.66475645, 0.66475645, 0.64756447, 0.66475645, 0.66475645,\n",
              "        0.66475645, 0.64756447, 0.65616046, 0.65616046, 0.65902579,\n",
              "        0.62750716, 0.65616046, 0.65616046, 0.65902579, 0.65329513,\n",
              "        0.64469914, 0.64469914, 0.65902579, 0.64469914, 0.64469914,\n",
              "        0.64469914, 0.65902579, 0.64183381, 0.66189112, 0.66189112,\n",
              "        0.66189112, 0.62750716, 0.66189112, 0.66189112, 0.66189112,\n",
              "        0.64756447, 0.66475645, 0.66475645, 0.66475645, 0.64756447,\n",
              "        0.66475645, 0.66475645, 0.66475645, 0.64469914, 0.65616046,\n",
              "        0.65616046, 0.65902579, 0.64756447, 0.65616046, 0.65616046,\n",
              "        0.65902579, 0.64756447, 0.64469914, 0.64469914, 0.65902579,\n",
              "        0.63037249, 0.64469914, 0.64469914, 0.65902579, 0.62464183]),\n",
              " 'split0_test_balanced_accuracy': array([0.53566768, 0.53566768, 0.53566768, 0.4992388 , 0.53566768,\n",
              "        0.53566768, 0.53566768, 0.50556401, 0.54561766, 0.54561766,\n",
              "        0.54561766, 0.50556401, 0.54561766, 0.54561766, 0.54561766,\n",
              "        0.4992388 , 0.54873496, 0.54873496, 0.54123169, 0.50168552,\n",
              "        0.54873496, 0.54873496, 0.54123169, 0.50607148, 0.54190228,\n",
              "        0.54190228, 0.54123169, 0.50041685, 0.54409526, 0.54409526,\n",
              "        0.54123169, 0.49603088, 0.53566768, 0.53566768, 0.53566768,\n",
              "        0.49729955, 0.53566768, 0.53566768, 0.53566768, 0.50724953,\n",
              "        0.54561766, 0.54561766, 0.54561766, 0.50801073, 0.54561766,\n",
              "        0.54561766, 0.54561766, 0.4938379 , 0.54873496, 0.54873496,\n",
              "        0.54123169, 0.50143178, 0.54873496, 0.54873496, 0.54123169,\n",
              "        0.49434537, 0.54190228, 0.54190228, 0.54123169, 0.50607148,\n",
              "        0.54409526, 0.54409526, 0.54123169, 0.5038785 , 0.53566768,\n",
              "        0.53566768, 0.53566768, 0.4992388 , 0.53566768, 0.53566768,\n",
              "        0.53786066, 0.5038785 , 0.54561766, 0.54561766, 0.54561766,\n",
              "        0.50337103, 0.54561766, 0.54561766, 0.54561766, 0.4899594 ,\n",
              "        0.54873496, 0.54873496, 0.54123169, 0.4992388 , 0.54873496,\n",
              "        0.54873496, 0.54123169, 0.49949253, 0.54190228, 0.54190228,\n",
              "        0.54123169, 0.4992388 , 0.54190228, 0.54190228, 0.54123169,\n",
              "        0.50041685, 0.53566768, 0.53566768, 0.53566768, 0.50143178,\n",
              "        0.53566768, 0.53566768, 0.53566768, 0.50362476, 0.54561766,\n",
              "        0.54561766, 0.54561766, 0.50143178, 0.54561766, 0.54561766,\n",
              "        0.54561766, 0.49949253, 0.54873496, 0.54873496, 0.54123169,\n",
              "        0.49577715, 0.54873496, 0.54873496, 0.54123169, 0.50581775,\n",
              "        0.54190228, 0.54190228, 0.54123169, 0.49729955, 0.54190228,\n",
              "        0.54190228, 0.54123169, 0.49704582, 0.53566768, 0.53566768,\n",
              "        0.53566768, 0.4938379 , 0.53566768, 0.53566768, 0.53566768,\n",
              "        0.49949253, 0.54561766, 0.54561766, 0.54561766, 0.49949253,\n",
              "        0.54561766, 0.54561766, 0.54561766, 0.4992388 , 0.54873496,\n",
              "        0.54873496, 0.54123169, 0.49949253, 0.54873496, 0.54873496,\n",
              "        0.54123169, 0.50143178, 0.54190228, 0.54190228, 0.54123169,\n",
              "        0.49990938, 0.54190228, 0.54190228, 0.54123169, 0.48776642]),\n",
              " 'split0_test_f1': array([0.49407862, 0.49407862, 0.49407862, 0.41392199, 0.49407862,\n",
              "        0.49407862, 0.49407862, 0.42337486, 0.51484584, 0.51484584,\n",
              "        0.51484584, 0.42337486, 0.51484584, 0.51484584, 0.51484584,\n",
              "        0.41392199, 0.5296496 , 0.5296496 , 0.51103759, 0.40922966,\n",
              "        0.5296496 , 0.5296496 , 0.51103759, 0.41159942, 0.52535755,\n",
              "        0.52535755, 0.51103759, 0.4356418 , 0.52736581, 0.52736581,\n",
              "        0.51103759, 0.43273483, 0.49407862, 0.49407862, 0.49407862,\n",
              "        0.40685307, 0.49407862, 0.49407862, 0.49407862, 0.43520821,\n",
              "        0.51484584, 0.51484584, 0.51484584, 0.41891442, 0.51484584,\n",
              "        0.51484584, 0.51484584, 0.43128259, 0.5296496 , 0.5296496 ,\n",
              "        0.51103759, 0.41517146, 0.5296496 , 0.5296496 , 0.51103759,\n",
              "        0.42194617, 0.52535755, 0.52535755, 0.51103759, 0.41159942,\n",
              "        0.52736581, 0.52736581, 0.51103759, 0.41041536, 0.49407862,\n",
              "        0.49407862, 0.49407862, 0.41392199, 0.49407862, 0.49407862,\n",
              "        0.49587022, 0.41041536, 0.51484584, 0.51484584, 0.51484584,\n",
              "        0.42206874, 0.51484584, 0.51484584, 0.51484584, 0.41922683,\n",
              "        0.5296496 , 0.5296496 , 0.51103759, 0.41392199, 0.5296496 ,\n",
              "        0.5296496 , 0.51103759, 0.40804225, 0.52535755, 0.52535755,\n",
              "        0.51103759, 0.41392199, 0.52535755, 0.52535755, 0.51103759,\n",
              "        0.4356418 , 0.49407862, 0.49407862, 0.49407862, 0.41517146,\n",
              "        0.49407862, 0.49407862, 0.49407862, 0.41641996, 0.51484584,\n",
              "        0.51484584, 0.51484584, 0.41517146, 0.51484584, 0.51484584,\n",
              "        0.51484584, 0.40804225, 0.5296496 , 0.5296496 , 0.51103759,\n",
              "        0.4370409 , 0.5296496 , 0.5296496 , 0.51103759, 0.41766758,\n",
              "        0.52535755, 0.52535755, 0.51103759, 0.40685307, 0.52535755,\n",
              "        0.52535755, 0.51103759, 0.41267149, 0.49407862, 0.49407862,\n",
              "        0.49407862, 0.43128259, 0.49407862, 0.49407862, 0.49407862,\n",
              "        0.40804225, 0.51484584, 0.51484584, 0.51484584, 0.40804225,\n",
              "        0.51484584, 0.51484584, 0.51484584, 0.41392199, 0.5296496 ,\n",
              "        0.5296496 , 0.51103759, 0.40804225, 0.5296496 , 0.5296496 ,\n",
              "        0.51103759, 0.41517146, 0.52535755, 0.52535755, 0.51103759,\n",
              "        0.4441646 , 0.52535755, 0.52535755, 0.51103759, 0.41786674]),\n",
              " 'split0_test_precision': array([0.6131815 , 0.6131815 , 0.6131815 , 0.49313725, 0.6131815 ,\n",
              "        0.6131815 , 0.6131815 , 0.5501634 , 0.617507  , 0.617507  ,\n",
              "        0.617507  , 0.5501634 , 0.617507  , 0.617507  , 0.617507  ,\n",
              "        0.49313725, 0.59472312, 0.59472312, 0.60094959, 0.52703488,\n",
              "        0.59472312, 0.59472312, 0.60094959, 0.66136802, 0.57368689,\n",
              "        0.57368689, 0.60094959, 0.50174772, 0.57876845, 0.57876845,\n",
              "        0.60094959, 0.48477898, 0.6131815 , 0.6131815 , 0.6131815 ,\n",
              "        0.46888053, 0.6131815 , 0.6131815 , 0.6131815 , 0.54264392,\n",
              "        0.617507  , 0.617507  , 0.617507  , 0.62848837, 0.617507  ,\n",
              "        0.617507  , 0.617507  , 0.47732729, 0.59472312, 0.59472312,\n",
              "        0.60094959, 0.51447947, 0.59472312, 0.59472312, 0.60094959,\n",
              "        0.47236003, 0.57368689, 0.57368689, 0.60094959, 0.66136802,\n",
              "        0.57876845, 0.57876845, 0.60094959, 0.57753623, 0.6131815 ,\n",
              "        0.6131815 , 0.6131815 , 0.49313725, 0.6131815 , 0.6131815 ,\n",
              "        0.62437485, 0.57753623, 0.617507  , 0.617507  , 0.617507  ,\n",
              "        0.52743363, 0.617507  , 0.617507  , 0.617507  , 0.45582137,\n",
              "        0.59472312, 0.59472312, 0.60094959, 0.49313725, 0.59472312,\n",
              "        0.59472312, 0.60094959, 0.49319728, 0.57368689, 0.57368689,\n",
              "        0.60094959, 0.49313725, 0.57368689, 0.57368689, 0.60094959,\n",
              "        0.50174772, 0.6131815 , 0.6131815 , 0.6131815 , 0.51447947,\n",
              "        0.6131815 , 0.6131815 , 0.6131815 , 0.54177109, 0.617507  ,\n",
              "        0.617507  , 0.617507  , 0.51447947, 0.617507  , 0.617507  ,\n",
              "        0.617507  , 0.49319728, 0.59472312, 0.59472312, 0.60094959,\n",
              "        0.48561728, 0.59472312, 0.59472312, 0.60094959, 0.57798834,\n",
              "        0.57368689, 0.57368689, 0.60094959, 0.46888053, 0.57368689,\n",
              "        0.57368689, 0.60094959, 0.4759587 , 0.6131815 , 0.6131815 ,\n",
              "        0.6131815 , 0.47732729, 0.6131815 , 0.6131815 , 0.6131815 ,\n",
              "        0.49319728, 0.617507  , 0.617507  , 0.617507  , 0.49319728,\n",
              "        0.617507  , 0.617507  , 0.617507  , 0.49313725, 0.59472312,\n",
              "        0.59472312, 0.60094959, 0.49319728, 0.59472312, 0.59472312,\n",
              "        0.60094959, 0.51447947, 0.57368689, 0.57368689, 0.60094959,\n",
              "        0.49970231, 0.57368689, 0.57368689, 0.60094959, 0.44870821]),\n",
              " 'split0_test_recall': array([0.53566768, 0.53566768, 0.53566768, 0.4992388 , 0.53566768,\n",
              "        0.53566768, 0.53566768, 0.50556401, 0.54561766, 0.54561766,\n",
              "        0.54561766, 0.50556401, 0.54561766, 0.54561766, 0.54561766,\n",
              "        0.4992388 , 0.54873496, 0.54873496, 0.54123169, 0.50168552,\n",
              "        0.54873496, 0.54873496, 0.54123169, 0.50607148, 0.54190228,\n",
              "        0.54190228, 0.54123169, 0.50041685, 0.54409526, 0.54409526,\n",
              "        0.54123169, 0.49603088, 0.53566768, 0.53566768, 0.53566768,\n",
              "        0.49729955, 0.53566768, 0.53566768, 0.53566768, 0.50724953,\n",
              "        0.54561766, 0.54561766, 0.54561766, 0.50801073, 0.54561766,\n",
              "        0.54561766, 0.54561766, 0.4938379 , 0.54873496, 0.54873496,\n",
              "        0.54123169, 0.50143178, 0.54873496, 0.54873496, 0.54123169,\n",
              "        0.49434537, 0.54190228, 0.54190228, 0.54123169, 0.50607148,\n",
              "        0.54409526, 0.54409526, 0.54123169, 0.5038785 , 0.53566768,\n",
              "        0.53566768, 0.53566768, 0.4992388 , 0.53566768, 0.53566768,\n",
              "        0.53786066, 0.5038785 , 0.54561766, 0.54561766, 0.54561766,\n",
              "        0.50337103, 0.54561766, 0.54561766, 0.54561766, 0.4899594 ,\n",
              "        0.54873496, 0.54873496, 0.54123169, 0.4992388 , 0.54873496,\n",
              "        0.54873496, 0.54123169, 0.49949253, 0.54190228, 0.54190228,\n",
              "        0.54123169, 0.4992388 , 0.54190228, 0.54190228, 0.54123169,\n",
              "        0.50041685, 0.53566768, 0.53566768, 0.53566768, 0.50143178,\n",
              "        0.53566768, 0.53566768, 0.53566768, 0.50362476, 0.54561766,\n",
              "        0.54561766, 0.54561766, 0.50143178, 0.54561766, 0.54561766,\n",
              "        0.54561766, 0.49949253, 0.54873496, 0.54873496, 0.54123169,\n",
              "        0.49577715, 0.54873496, 0.54873496, 0.54123169, 0.50581775,\n",
              "        0.54190228, 0.54190228, 0.54123169, 0.49729955, 0.54190228,\n",
              "        0.54190228, 0.54123169, 0.49704582, 0.53566768, 0.53566768,\n",
              "        0.53566768, 0.4938379 , 0.53566768, 0.53566768, 0.53566768,\n",
              "        0.49949253, 0.54561766, 0.54561766, 0.54561766, 0.49949253,\n",
              "        0.54561766, 0.54561766, 0.54561766, 0.4992388 , 0.54873496,\n",
              "        0.54873496, 0.54123169, 0.49949253, 0.54873496, 0.54873496,\n",
              "        0.54123169, 0.50143178, 0.54190228, 0.54190228, 0.54123169,\n",
              "        0.49990938, 0.54190228, 0.54190228, 0.54123169, 0.48776642]),\n",
              " 'split0_train_accuracy': array([0.66528662, 0.66496815, 0.66464968, 0.65318471, 0.66496815,\n",
              "        0.66528662, 0.66528662, 0.65955414, 0.66592357, 0.66624204,\n",
              "        0.66624204, 0.65764331, 0.66656051, 0.66624204, 0.66687898,\n",
              "        0.66019108, 0.6611465 , 0.66050955, 0.65955414, 0.6589172 ,\n",
              "        0.66050955, 0.66082803, 0.66019108, 0.66082803, 0.66656051,\n",
              "        0.66656051, 0.65955414, 0.63917197, 0.66592357, 0.66592357,\n",
              "        0.66019108, 0.63949045, 0.6656051 , 0.66528662, 0.6656051 ,\n",
              "        0.65796178, 0.66464968, 0.66464968, 0.66464968, 0.65414013,\n",
              "        0.66624204, 0.66656051, 0.66624204, 0.65700637, 0.66624204,\n",
              "        0.66624204, 0.66624204, 0.63949045, 0.66050955, 0.66050955,\n",
              "        0.65923567, 0.6566879 , 0.6611465 , 0.6611465 , 0.65955414,\n",
              "        0.64585987, 0.66656051, 0.66656051, 0.65955414, 0.65923567,\n",
              "        0.66656051, 0.66656051, 0.65955414, 0.6589172 , 0.66496815,\n",
              "        0.66464968, 0.66496815, 0.65541401, 0.66496815, 0.6656051 ,\n",
              "        0.66496815, 0.65923567, 0.66624204, 0.66656051, 0.66624204,\n",
              "        0.64585987, 0.66624204, 0.66624204, 0.66624204, 0.6433121 ,\n",
              "        0.66050955, 0.66050955, 0.65955414, 0.6589172 , 0.6611465 ,\n",
              "        0.66050955, 0.66019108, 0.66082803, 0.66656051, 0.66656051,\n",
              "        0.65955414, 0.6589172 , 0.66656051, 0.66656051, 0.65955414,\n",
              "        0.63853503, 0.6656051 , 0.66496815, 0.6656051 , 0.65764331,\n",
              "        0.66496815, 0.66528662, 0.6656051 , 0.65764331, 0.66624204,\n",
              "        0.66656051, 0.66656051, 0.65796178, 0.66624204, 0.6656051 ,\n",
              "        0.66624204, 0.65955414, 0.66050955, 0.6611465 , 0.65955414,\n",
              "        0.63694268, 0.6611465 , 0.66050955, 0.65955414, 0.66019108,\n",
              "        0.66656051, 0.66656051, 0.65955414, 0.65828025, 0.66656051,\n",
              "        0.66656051, 0.65955414, 0.65254777, 0.66464968, 0.66496815,\n",
              "        0.6656051 , 0.6388535 , 0.6656051 , 0.66528662, 0.66528662,\n",
              "        0.66082803, 0.66656051, 0.66656051, 0.66624204, 0.65859873,\n",
              "        0.66624204, 0.66656051, 0.66624204, 0.65350318, 0.66050955,\n",
              "        0.66050955, 0.65955414, 0.65859873, 0.66050955, 0.66050955,\n",
              "        0.65955414, 0.65796178, 0.66656051, 0.66656051, 0.65955414,\n",
              "        0.63726115, 0.66656051, 0.66656051, 0.65955414, 0.6433121 ]),\n",
              " 'split0_train_balanced_accuracy': array([0.52765105, 0.52718979, 0.52672854, 0.50663435, 0.52718979,\n",
              "        0.52765105, 0.52765105, 0.51106203, 0.53707805, 0.53753931,\n",
              "        0.53753931, 0.51003902, 0.5377825 , 0.53753931, 0.53824375,\n",
              "        0.51133035, 0.54498758, 0.5445012 , 0.53417681, 0.50883114,\n",
              "        0.54406507, 0.54452633, 0.53466319, 0.51138061, 0.55937083,\n",
              "        0.55937083, 0.53417681, 0.50552877, 0.55823025, 0.55823025,\n",
              "        0.53466319, 0.50817067, 0.5281123 , 0.52765105, 0.5281123 ,\n",
              "        0.50919189, 0.52672854, 0.52672854, 0.52672854, 0.51368778,\n",
              "        0.53753931, 0.53800056, 0.53753931, 0.50388297, 0.53753931,\n",
              "        0.53753931, 0.53753931, 0.50969712, 0.5445012 , 0.5445012 ,\n",
              "        0.53393362, 0.50930945, 0.54498758, 0.54498758, 0.53417681,\n",
              "        0.5086732 , 0.55937083, 0.55937083, 0.53417681, 0.50667563,\n",
              "        0.55937083, 0.55937083, 0.53417681, 0.50883114, 0.52718979,\n",
              "        0.52672854, 0.52718979, 0.50899088, 0.52718979, 0.5281123 ,\n",
              "        0.52718979, 0.50907433, 0.53753931, 0.53800056, 0.53753931,\n",
              "        0.50234935, 0.53753931, 0.53753931, 0.53753931, 0.50825412,\n",
              "        0.5445012 , 0.5445012 , 0.53417681, 0.51035759, 0.54498758,\n",
              "        0.5445012 , 0.53466319, 0.5120348 , 0.55937083, 0.55937083,\n",
              "        0.53417681, 0.51122984, 0.55937083, 0.55937083, 0.53417681,\n",
              "        0.50460626, 0.5281123 , 0.52718979, 0.5281123 , 0.51091127,\n",
              "        0.52718979, 0.52765105, 0.5281123 , 0.51003902, 0.53753931,\n",
              "        0.53800056, 0.53800056, 0.5109364 , 0.53753931, 0.5366168 ,\n",
              "        0.53753931, 0.51018978, 0.5445012 , 0.54498758, 0.53417681,\n",
              "        0.50905997, 0.54498758, 0.5445012 , 0.53417681, 0.51067616,\n",
              "        0.55937083, 0.55937083, 0.53417681, 0.50965314, 0.55937083,\n",
              "        0.55937083, 0.53417681, 0.50680216, 0.52672854, 0.52718979,\n",
              "        0.5281123 , 0.50899267, 0.5281123 , 0.52765105, 0.52765105,\n",
              "        0.5120348 , 0.53800056, 0.53800056, 0.53753931, 0.50858795,\n",
              "        0.53753931, 0.53800056, 0.53753931, 0.5070956 , 0.5445012 ,\n",
              "        0.5445012 , 0.53417681, 0.50967827, 0.5445012 , 0.5445012 ,\n",
              "        0.53417681, 0.51006414, 0.55937083, 0.55937083, 0.53417681,\n",
              "        0.50843091, 0.55937083, 0.55937083, 0.53417681, 0.50912638]),\n",
              " 'split0_train_f1': array([0.46903128, 0.46819931, 0.46736614, 0.42463932, 0.46819931,\n",
              "        0.46903128, 0.46903128, 0.42597559, 0.49436928, 0.49513472,\n",
              "        0.49513472, 0.42667163, 0.49533331, 0.49513472, 0.4960982 ,\n",
              "        0.4254736 , 0.51875565, 0.51832479, 0.49538472, 0.41931283,\n",
              "        0.51737582, 0.51806614, 0.49578547, 0.42416879, 0.54335609,\n",
              "        0.54335609, 0.49538472, 0.44827627, 0.54165208, 0.54165208,\n",
              "        0.49578547, 0.45527916, 0.46986205, 0.46903128, 0.46986205,\n",
              "        0.42289526, 0.46736614, 0.46736614, 0.46736614, 0.44622646,\n",
              "        0.49513472, 0.49589913, 0.49513472, 0.40521841, 0.49513472,\n",
              "        0.49513472, 0.49513472, 0.45949793, 0.51832479, 0.51832479,\n",
              "        0.49518443, 0.42623612, 0.51875565, 0.51875565, 0.49538472,\n",
              "        0.44599407, 0.54335609, 0.54335609, 0.49538472, 0.41035451,\n",
              "        0.54335609, 0.54335609, 0.49538472, 0.41931283, 0.46819931,\n",
              "        0.46736614, 0.46819931, 0.42795663, 0.46819931, 0.46986205,\n",
              "        0.46819931, 0.41945053, 0.49513472, 0.49589913, 0.49513472,\n",
              "        0.42575435, 0.49513472, 0.49513472, 0.49513472, 0.44921806,\n",
              "        0.51832479, 0.51832479, 0.49538472, 0.42490164, 0.51875565,\n",
              "        0.51832479, 0.49578547, 0.4265505 , 0.54335609, 0.54335609,\n",
              "        0.49538472, 0.42803009, 0.54335609, 0.54335609, 0.49538472,\n",
              "        0.44666803, 0.46986205, 0.46819931, 0.46986205, 0.42975399,\n",
              "        0.46819931, 0.46903128, 0.46986205, 0.42667163, 0.49513472,\n",
              "        0.49589913, 0.49589913, 0.42913498, 0.49513472, 0.49360284,\n",
              "        0.49513472, 0.42280593, 0.51832479, 0.51875565, 0.49538472,\n",
              "        0.46157956, 0.51875565, 0.51832479, 0.49538472, 0.42308742,\n",
              "        0.54335609, 0.54335609, 0.49538472, 0.42382769, 0.54335609,\n",
              "        0.54335609, 0.49538472, 0.42663127, 0.46736614, 0.46819931,\n",
              "        0.46986205, 0.45854298, 0.46986205, 0.46903128, 0.46903128,\n",
              "        0.4265505 , 0.49589913, 0.49589913, 0.49513472, 0.41917512,\n",
              "        0.49513472, 0.49589913, 0.49513472, 0.42555013, 0.51832479,\n",
              "        0.51832479, 0.49538472, 0.42317829, 0.51832479, 0.51832479,\n",
              "        0.49538472, 0.42603833, 0.54335609, 0.54335609, 0.49538472,\n",
              "        0.45942253, 0.54335609, 0.54335609, 0.49538472, 0.45177326]),\n",
              " 'split0_train_precision': array([0.64006832, 0.63862886, 0.63717115, 0.56269212, 0.63862886,\n",
              "        0.64006832, 0.64006832, 0.65068669, 0.62496919, 0.62601215,\n",
              "        0.62601215, 0.61541854, 0.62734347, 0.62601215, 0.62837683,\n",
              "        0.66672499, 0.60319303, 0.60156682, 0.59997703, 0.6627432 ,\n",
              "        0.60158846, 0.60239266, 0.60209991, 0.69046189, 0.61467447,\n",
              "        0.61467447, 0.59997703, 0.52011496, 0.61337304, 0.61337304,\n",
              "        0.60209991, 0.52720753, 0.64148992, 0.64006832, 0.64148992,\n",
              "        0.62757822, 0.63717115, 0.63717115, 0.63717115, 0.57683047,\n",
              "        0.62601215, 0.62704708, 0.62601215, 0.71288529, 0.62601215,\n",
              "        0.62601215, 0.62601215, 0.53058487, 0.60156682, 0.60156682,\n",
              "        0.59892686, 0.60226533, 0.60319303, 0.60319303, 0.59997703,\n",
              "        0.53828481, 0.61467447, 0.61467447, 0.59997703, 0.79765525,\n",
              "        0.61467447, 0.61467447, 0.59997703, 0.6627432 , 0.63862886,\n",
              "        0.63717115, 0.63862886, 0.58605023, 0.63862886, 0.64148992,\n",
              "        0.63862886, 0.67157012, 0.62601215, 0.62704708, 0.62601215,\n",
              "        0.51543923, 0.62601215, 0.62601215, 0.62601215, 0.53250085,\n",
              "        0.60156682, 0.60156682, 0.59997703, 0.64109076, 0.60319303,\n",
              "        0.60156682, 0.60209991, 0.67709083, 0.61467447, 0.61467447,\n",
              "        0.59997703, 0.63325595, 0.61467447, 0.61467447, 0.59997703,\n",
              "        0.51690904, 0.64148992, 0.63862886, 0.64148992, 0.61160216,\n",
              "        0.63862886, 0.64006832, 0.64148992, 0.61541854, 0.62601215,\n",
              "        0.62704708, 0.62704708, 0.61668007, 0.62601215, 0.6239181 ,\n",
              "        0.62601215, 0.6630587 , 0.60156682, 0.60319303, 0.59997703,\n",
              "        0.52659418, 0.60319303, 0.60156682, 0.59997703, 0.67867253,\n",
              "        0.61467447, 0.61467447, 0.59997703, 0.63149483, 0.61467447,\n",
              "        0.61467447, 0.59997703, 0.55838067, 0.63717115, 0.63862886,\n",
              "        0.64148992, 0.52836303, 0.64148992, 0.64006832, 0.64006832,\n",
              "        0.67709083, 0.62704708, 0.62704708, 0.62601215, 0.65435484,\n",
              "        0.62601215, 0.62704708, 0.62601215, 0.5662128 , 0.60156682,\n",
              "        0.60156682, 0.59997703, 0.63961165, 0.60156682, 0.60156682,\n",
              "        0.59997703, 0.62137446, 0.61467447, 0.61467447, 0.59997703,\n",
              "        0.52554117, 0.61467447, 0.61467447, 0.59997703, 0.53459184]),\n",
              " 'split0_train_recall': array([0.52765105, 0.52718979, 0.52672854, 0.50663435, 0.52718979,\n",
              "        0.52765105, 0.52765105, 0.51106203, 0.53707805, 0.53753931,\n",
              "        0.53753931, 0.51003902, 0.5377825 , 0.53753931, 0.53824375,\n",
              "        0.51133035, 0.54498758, 0.5445012 , 0.53417681, 0.50883114,\n",
              "        0.54406507, 0.54452633, 0.53466319, 0.51138061, 0.55937083,\n",
              "        0.55937083, 0.53417681, 0.50552877, 0.55823025, 0.55823025,\n",
              "        0.53466319, 0.50817067, 0.5281123 , 0.52765105, 0.5281123 ,\n",
              "        0.50919189, 0.52672854, 0.52672854, 0.52672854, 0.51368778,\n",
              "        0.53753931, 0.53800056, 0.53753931, 0.50388297, 0.53753931,\n",
              "        0.53753931, 0.53753931, 0.50969712, 0.5445012 , 0.5445012 ,\n",
              "        0.53393362, 0.50930945, 0.54498758, 0.54498758, 0.53417681,\n",
              "        0.5086732 , 0.55937083, 0.55937083, 0.53417681, 0.50667563,\n",
              "        0.55937083, 0.55937083, 0.53417681, 0.50883114, 0.52718979,\n",
              "        0.52672854, 0.52718979, 0.50899088, 0.52718979, 0.5281123 ,\n",
              "        0.52718979, 0.50907433, 0.53753931, 0.53800056, 0.53753931,\n",
              "        0.50234935, 0.53753931, 0.53753931, 0.53753931, 0.50825412,\n",
              "        0.5445012 , 0.5445012 , 0.53417681, 0.51035759, 0.54498758,\n",
              "        0.5445012 , 0.53466319, 0.5120348 , 0.55937083, 0.55937083,\n",
              "        0.53417681, 0.51122984, 0.55937083, 0.55937083, 0.53417681,\n",
              "        0.50460626, 0.5281123 , 0.52718979, 0.5281123 , 0.51091127,\n",
              "        0.52718979, 0.52765105, 0.5281123 , 0.51003902, 0.53753931,\n",
              "        0.53800056, 0.53800056, 0.5109364 , 0.53753931, 0.5366168 ,\n",
              "        0.53753931, 0.51018978, 0.5445012 , 0.54498758, 0.53417681,\n",
              "        0.50905997, 0.54498758, 0.5445012 , 0.53417681, 0.51067616,\n",
              "        0.55937083, 0.55937083, 0.53417681, 0.50965314, 0.55937083,\n",
              "        0.55937083, 0.53417681, 0.50680216, 0.52672854, 0.52718979,\n",
              "        0.5281123 , 0.50899267, 0.5281123 , 0.52765105, 0.52765105,\n",
              "        0.5120348 , 0.53800056, 0.53800056, 0.53753931, 0.50858795,\n",
              "        0.53753931, 0.53800056, 0.53753931, 0.5070956 , 0.5445012 ,\n",
              "        0.5445012 , 0.53417681, 0.50967827, 0.5445012 , 0.5445012 ,\n",
              "        0.53417681, 0.51006414, 0.55937083, 0.55937083, 0.53417681,\n",
              "        0.50843091, 0.55937083, 0.55937083, 0.53417681, 0.50912638]),\n",
              " 'split1_test_accuracy': array([0.67908309, 0.67908309, 0.67908309, 0.62464183, 0.67908309,\n",
              "        0.67908309, 0.67908309, 0.65902579, 0.67621777, 0.67621777,\n",
              "        0.67621777, 0.65616046, 0.67621777, 0.67621777, 0.67621777,\n",
              "        0.61031519, 0.67048711, 0.67048711, 0.67335244, 0.65329513,\n",
              "        0.67048711, 0.67048711, 0.67335244, 0.66189112, 0.66189112,\n",
              "        0.66189112, 0.67335244, 0.62464183, 0.66189112, 0.66189112,\n",
              "        0.67335244, 0.66475645, 0.67908309, 0.67908309, 0.67908309,\n",
              "        0.66189112, 0.67908309, 0.67908309, 0.67908309, 0.66189112,\n",
              "        0.67621777, 0.67621777, 0.67621777, 0.66475645, 0.67621777,\n",
              "        0.67621777, 0.67621777, 0.65329513, 0.67048711, 0.67048711,\n",
              "        0.67335244, 0.65329513, 0.67048711, 0.67048711, 0.67335244,\n",
              "        0.66475645, 0.66189112, 0.66189112, 0.67335244, 0.65329513,\n",
              "        0.66189112, 0.66189112, 0.67335244, 0.65329513, 0.67908309,\n",
              "        0.67908309, 0.67908309, 0.62750716, 0.67908309, 0.67908309,\n",
              "        0.67908309, 0.62464183, 0.67621777, 0.67621777, 0.67621777,\n",
              "        0.65329513, 0.67621777, 0.67621777, 0.67621777, 0.66189112,\n",
              "        0.67048711, 0.67048711, 0.67335244, 0.66189112, 0.67048711,\n",
              "        0.67048711, 0.67335244, 0.66189112, 0.66189112, 0.66189112,\n",
              "        0.67335244, 0.66475645, 0.66189112, 0.66189112, 0.67335244,\n",
              "        0.65616046, 0.67908309, 0.67908309, 0.67908309, 0.65616046,\n",
              "        0.67908309, 0.67908309, 0.67908309, 0.65902579, 0.67621777,\n",
              "        0.67621777, 0.67621777, 0.66189112, 0.67621777, 0.67621777,\n",
              "        0.67621777, 0.66475645, 0.67048711, 0.67048711, 0.67335244,\n",
              "        0.66475645, 0.67048711, 0.67048711, 0.67335244, 0.66189112,\n",
              "        0.66189112, 0.66189112, 0.67335244, 0.65329513, 0.66189112,\n",
              "        0.66189112, 0.67335244, 0.66475645, 0.67908309, 0.67908309,\n",
              "        0.67908309, 0.65329513, 0.67908309, 0.67908309, 0.67908309,\n",
              "        0.61318052, 0.67621777, 0.67621777, 0.67621777, 0.66475645,\n",
              "        0.67621777, 0.67621777, 0.67621777, 0.65329513, 0.67048711,\n",
              "        0.67048711, 0.67335244, 0.65902579, 0.67048711, 0.67048711,\n",
              "        0.67335244, 0.66762178, 0.66189112, 0.66189112, 0.67335244,\n",
              "        0.66189112, 0.66189112, 0.66189112, 0.67335244, 0.66475645]),\n",
              " 'split1_test_balanced_accuracy': array([0.53912933, 0.53912933, 0.53912933, 0.49552342, 0.53912933,\n",
              "        0.53912933, 0.53912933, 0.51796071, 0.5427541 , 0.5427541 ,\n",
              "        0.5427541 , 0.50607148, 0.5427541 , 0.5427541 , 0.5427541 ,\n",
              "        0.47680151, 0.55388212, 0.55388212, 0.54443961, 0.5       ,\n",
              "        0.55388212, 0.55388212, 0.54443961, 0.51433594, 0.55118167,\n",
              "        0.55118167, 0.54443961, 0.49940191, 0.54924242, 0.54924242,\n",
              "        0.54443961, 0.51846817, 0.53912933, 0.53912933, 0.53912933,\n",
              "        0.51433594, 0.53912933, 0.53912933, 0.53912933, 0.51433594,\n",
              "        0.5427541 , 0.5427541 , 0.5427541 , 0.51846817, 0.5427541 ,\n",
              "        0.5427541 , 0.5427541 , 0.5       , 0.55388212, 0.55388212,\n",
              "        0.54443961, 0.5       , 0.55388212, 0.55388212, 0.54443961,\n",
              "        0.51846817, 0.55118167, 0.55118167, 0.54443961, 0.50193925,\n",
              "        0.55118167, 0.55118167, 0.54443961, 0.50193925, 0.53912933,\n",
              "        0.53912933, 0.53912933, 0.48802015, 0.53912933, 0.53912933,\n",
              "        0.53912933, 0.49940191, 0.5427541 , 0.5427541 , 0.5427541 ,\n",
              "        0.5       , 0.5427541 , 0.5427541 , 0.5427541 , 0.51433594,\n",
              "        0.55388212, 0.55388212, 0.54443961, 0.51433594, 0.55388212,\n",
              "        0.55388212, 0.54443961, 0.52015369, 0.55118167, 0.55118167,\n",
              "        0.54443961, 0.51846817, 0.55118167, 0.55118167, 0.54443961,\n",
              "        0.50607148, 0.53912933, 0.53912933, 0.53912933, 0.50607148,\n",
              "        0.53912933, 0.53912933, 0.53912933, 0.51020371, 0.5427541 ,\n",
              "        0.5427541 , 0.5427541 , 0.51433594, 0.5427541 , 0.5427541 ,\n",
              "        0.5427541 , 0.51846817, 0.55388212, 0.55388212, 0.54443961,\n",
              "        0.52622517, 0.55388212, 0.55388212, 0.54443961, 0.51433594,\n",
              "        0.55118167, 0.55118167, 0.54443961, 0.5       , 0.55118167,\n",
              "        0.55118167, 0.54443961, 0.51846817, 0.53912933, 0.53912933,\n",
              "        0.53912933, 0.5       , 0.53912933, 0.53912933, 0.53912933,\n",
              "        0.48093374, 0.5427541 , 0.5427541 , 0.5427541 , 0.52040742,\n",
              "        0.5427541 , 0.5427541 , 0.5427541 , 0.507757  , 0.55388212,\n",
              "        0.55388212, 0.54443961, 0.51020371, 0.55388212, 0.55388212,\n",
              "        0.54443961, 0.52260041, 0.55118167, 0.55118167, 0.54443961,\n",
              "        0.51433594, 0.55118167, 0.55118167, 0.54443961, 0.51846817]),\n",
              " 'split1_test_f1': array([0.47681765, 0.47681765, 0.47681765, 0.44109485, 0.47681765,\n",
              "        0.47681765, 0.47681765, 0.44732184, 0.49245183, 0.49245183,\n",
              "        0.49245183, 0.41159942, 0.49245183, 0.49245183, 0.49245183,\n",
              "        0.41105817, 0.5274733 , 0.5274733 , 0.50127858, 0.39514731,\n",
              "        0.5274733 , 0.5274733 , 0.50127858, 0.42859918, 0.52975701,\n",
              "        0.52975701, 0.50127858, 0.45171194, 0.52572784, 0.52572784,\n",
              "        0.50127858, 0.43691824, 0.47681765, 0.47681765, 0.47681765,\n",
              "        0.42859918, 0.47681765, 0.47681765, 0.47681765, 0.42859918,\n",
              "        0.49245183, 0.49245183, 0.49245183, 0.43691824, 0.49245183,\n",
              "        0.49245183, 0.49245183, 0.39514731, 0.5274733 , 0.5274733 ,\n",
              "        0.50127858, 0.39514731, 0.5274733 , 0.5274733 , 0.50127858,\n",
              "        0.43691824, 0.52975701, 0.52975701, 0.50127858, 0.40291269,\n",
              "        0.52975701, 0.52975701, 0.50127858, 0.40291269, 0.47681765,\n",
              "        0.47681765, 0.47681765, 0.41291408, 0.47681765, 0.47681765,\n",
              "        0.47681765, 0.45171194, 0.49245183, 0.49245183, 0.49245183,\n",
              "        0.39514731, 0.49245183, 0.49245183, 0.49245183, 0.42859918,\n",
              "        0.5274733 , 0.5274733 , 0.50127858, 0.42859918, 0.5274733 ,\n",
              "        0.5274733 , 0.50127858, 0.44879002, 0.52975701, 0.52975701,\n",
              "        0.50127858, 0.43691824, 0.52975701, 0.52975701, 0.50127858,\n",
              "        0.41159942, 0.47681765, 0.47681765, 0.47681765, 0.41159942,\n",
              "        0.47681765, 0.47681765, 0.47681765, 0.42016056, 0.49245183,\n",
              "        0.49245183, 0.49245183, 0.42859918, 0.49245183, 0.49245183,\n",
              "        0.49245183, 0.43691824, 0.5274733 , 0.5274733 , 0.50127858,\n",
              "        0.46275903, 0.5274733 , 0.5274733 , 0.50127858, 0.42859918,\n",
              "        0.52975701, 0.52975701, 0.50127858, 0.39514731, 0.52975701,\n",
              "        0.52975701, 0.50127858, 0.43691824, 0.47681765, 0.47681765,\n",
              "        0.47681765, 0.39514731, 0.47681765, 0.47681765, 0.47681765,\n",
              "        0.41831179, 0.49245183, 0.49245183, 0.49245183, 0.44369968,\n",
              "        0.49245183, 0.49245183, 0.49245183, 0.42468086, 0.5274733 ,\n",
              "        0.5274733 , 0.50127858, 0.42016056, 0.5274733 , 0.5274733 ,\n",
              "        0.50127858, 0.44512061, 0.52975701, 0.52975701, 0.50127858,\n",
              "        0.42859918, 0.52975701, 0.52975701, 0.50127858, 0.43691824]),\n",
              " 'split1_test_precision': array([0.79034427, 0.79034427, 0.79034427, 0.48625946, 0.79034427,\n",
              "        0.79034427, 0.79034427, 0.6225272 , 0.69796912, 0.69796912,\n",
              "        0.69796912, 0.66136802, 0.69796912, 0.69796912, 0.69796912,\n",
              "        0.42098765, 0.63192226, 0.63192226, 0.66351027, 0.32664756,\n",
              "        0.63192226, 0.63192226, 0.66351027, 0.72994186, 0.60731114,\n",
              "        0.60731114, 0.66351027, 0.49837342, 0.6075784 , 0.6075784 ,\n",
              "        0.66351027, 0.74757046, 0.79034427, 0.79034427, 0.79034427,\n",
              "        0.72994186, 0.79034427, 0.79034427, 0.79034427, 0.72994186,\n",
              "        0.69796912, 0.69796912, 0.69796912, 0.74757046, 0.69796912,\n",
              "        0.69796912, 0.69796912, 0.32664756, 0.63192226, 0.63192226,\n",
              "        0.66351027, 0.32664756, 0.63192226, 0.63192226, 0.66351027,\n",
              "        0.74757046, 0.60731114, 0.60731114, 0.66351027, 0.57708934,\n",
              "        0.60731114, 0.60731114, 0.66351027, 0.57708934, 0.79034427,\n",
              "        0.79034427, 0.79034427, 0.44144224, 0.79034427, 0.79034427,\n",
              "        0.79034427, 0.49837342, 0.69796912, 0.69796912, 0.69796912,\n",
              "        0.32664756, 0.69796912, 0.69796912, 0.69796912, 0.72994186,\n",
              "        0.63192226, 0.63192226, 0.66351027, 0.72994186, 0.63192226,\n",
              "        0.63192226, 0.66351027, 0.64954276, 0.60731114, 0.60731114,\n",
              "        0.66351027, 0.74757046, 0.60731114, 0.60731114, 0.66351027,\n",
              "        0.66136802, 0.79034427, 0.79034427, 0.79034427, 0.66136802,\n",
              "        0.79034427, 0.79034427, 0.79034427, 0.70398551, 0.69796912,\n",
              "        0.69796912, 0.69796912, 0.72994186, 0.69796912, 0.69796912,\n",
              "        0.69796912, 0.74757046, 0.63192226, 0.63192226, 0.66351027,\n",
              "        0.65426439, 0.63192226, 0.63192226, 0.66351027, 0.72994186,\n",
              "        0.60731114, 0.60731114, 0.66351027, 0.32664756, 0.60731114,\n",
              "        0.60731114, 0.66351027, 0.74757046, 0.79034427, 0.79034427,\n",
              "        0.79034427, 0.32664756, 0.79034427, 0.79034427, 0.79034427,\n",
              "        0.43736604, 0.69796912, 0.69796912, 0.69796912, 0.7063783 ,\n",
              "        0.69796912, 0.69796912, 0.69796912, 0.57844575, 0.63192226,\n",
              "        0.63192226, 0.66351027, 0.70398551, 0.63192226, 0.63192226,\n",
              "        0.66351027, 0.76044277, 0.60731114, 0.60731114, 0.66351027,\n",
              "        0.72994186, 0.60731114, 0.60731114, 0.66351027, 0.74757046]),\n",
              " 'split1_test_recall': array([0.53912933, 0.53912933, 0.53912933, 0.49552342, 0.53912933,\n",
              "        0.53912933, 0.53912933, 0.51796071, 0.5427541 , 0.5427541 ,\n",
              "        0.5427541 , 0.50607148, 0.5427541 , 0.5427541 , 0.5427541 ,\n",
              "        0.47680151, 0.55388212, 0.55388212, 0.54443961, 0.5       ,\n",
              "        0.55388212, 0.55388212, 0.54443961, 0.51433594, 0.55118167,\n",
              "        0.55118167, 0.54443961, 0.49940191, 0.54924242, 0.54924242,\n",
              "        0.54443961, 0.51846817, 0.53912933, 0.53912933, 0.53912933,\n",
              "        0.51433594, 0.53912933, 0.53912933, 0.53912933, 0.51433594,\n",
              "        0.5427541 , 0.5427541 , 0.5427541 , 0.51846817, 0.5427541 ,\n",
              "        0.5427541 , 0.5427541 , 0.5       , 0.55388212, 0.55388212,\n",
              "        0.54443961, 0.5       , 0.55388212, 0.55388212, 0.54443961,\n",
              "        0.51846817, 0.55118167, 0.55118167, 0.54443961, 0.50193925,\n",
              "        0.55118167, 0.55118167, 0.54443961, 0.50193925, 0.53912933,\n",
              "        0.53912933, 0.53912933, 0.48802015, 0.53912933, 0.53912933,\n",
              "        0.53912933, 0.49940191, 0.5427541 , 0.5427541 , 0.5427541 ,\n",
              "        0.5       , 0.5427541 , 0.5427541 , 0.5427541 , 0.51433594,\n",
              "        0.55388212, 0.55388212, 0.54443961, 0.51433594, 0.55388212,\n",
              "        0.55388212, 0.54443961, 0.52015369, 0.55118167, 0.55118167,\n",
              "        0.54443961, 0.51846817, 0.55118167, 0.55118167, 0.54443961,\n",
              "        0.50607148, 0.53912933, 0.53912933, 0.53912933, 0.50607148,\n",
              "        0.53912933, 0.53912933, 0.53912933, 0.51020371, 0.5427541 ,\n",
              "        0.5427541 , 0.5427541 , 0.51433594, 0.5427541 , 0.5427541 ,\n",
              "        0.5427541 , 0.51846817, 0.55388212, 0.55388212, 0.54443961,\n",
              "        0.52622517, 0.55388212, 0.55388212, 0.54443961, 0.51433594,\n",
              "        0.55118167, 0.55118167, 0.54443961, 0.5       , 0.55118167,\n",
              "        0.55118167, 0.54443961, 0.51846817, 0.53912933, 0.53912933,\n",
              "        0.53912933, 0.5       , 0.53912933, 0.53912933, 0.53912933,\n",
              "        0.48093374, 0.5427541 , 0.5427541 , 0.5427541 , 0.52040742,\n",
              "        0.5427541 , 0.5427541 , 0.5427541 , 0.507757  , 0.55388212,\n",
              "        0.55388212, 0.54443961, 0.51020371, 0.55388212, 0.55388212,\n",
              "        0.54443961, 0.52260041, 0.55118167, 0.55118167, 0.54443961,\n",
              "        0.51433594, 0.55118167, 0.55118167, 0.54443961, 0.51846817]),\n",
              " 'split1_train_accuracy': array([0.66815287, 0.66815287, 0.66815287, 0.6410828 , 0.66815287,\n",
              "        0.66815287, 0.66815287, 0.65382166, 0.66178344, 0.66242038,\n",
              "        0.66178344, 0.65828025, 0.66178344, 0.66210191, 0.66210191,\n",
              "        0.63694268, 0.65987261, 0.65859873, 0.66178344, 0.65700637,\n",
              "        0.66050955, 0.66050955, 0.66146497, 0.65828025, 0.66528662,\n",
              "        0.66528662, 0.66178344, 0.63917197, 0.66433121, 0.66433121,\n",
              "        0.66146497, 0.65764331, 0.66783439, 0.66815287, 0.66815287,\n",
              "        0.65859873, 0.66815287, 0.66815287, 0.66783439, 0.65859873,\n",
              "        0.66178344, 0.66178344, 0.66178344, 0.65636943, 0.66178344,\n",
              "        0.66178344, 0.66178344, 0.65477707, 0.65859873, 0.65859873,\n",
              "        0.66082803, 0.65541401, 0.65923567, 0.65859873, 0.66146497,\n",
              "        0.65859873, 0.66496815, 0.66528662, 0.66082803, 0.65764331,\n",
              "        0.66528662, 0.66528662, 0.66210191, 0.65764331, 0.66815287,\n",
              "        0.66815287, 0.66815287, 0.64363057, 0.66815287, 0.66815287,\n",
              "        0.66815287, 0.63853503, 0.66178344, 0.66178344, 0.66210191,\n",
              "        0.65509554, 0.66178344, 0.66210191, 0.66178344, 0.65987261,\n",
              "        0.65859873, 0.65859873, 0.66082803, 0.65764331, 0.65859873,\n",
              "        0.65859873, 0.66082803, 0.65318471, 0.66528662, 0.66528662,\n",
              "        0.66082803, 0.65700637, 0.66528662, 0.66496815, 0.66178344,\n",
              "        0.65987261, 0.66783439, 0.66815287, 0.66783439, 0.65796178,\n",
              "        0.66815287, 0.66815287, 0.66815287, 0.65955414, 0.66146497,\n",
              "        0.66178344, 0.66210191, 0.65796178, 0.66178344, 0.66178344,\n",
              "        0.66178344, 0.65573248, 0.65859873, 0.65923567, 0.66178344,\n",
              "        0.65127389, 0.65923567, 0.65859873, 0.66178344, 0.65700637,\n",
              "        0.66528662, 0.66528662, 0.66178344, 0.65541401, 0.66496815,\n",
              "        0.66528662, 0.66082803, 0.65700637, 0.66815287, 0.66815287,\n",
              "        0.66815287, 0.65477707, 0.66815287, 0.66815287, 0.66815287,\n",
              "        0.63757962, 0.66178344, 0.66146497, 0.66273885, 0.65700637,\n",
              "        0.66178344, 0.66273885, 0.66273885, 0.6522293 , 0.65859873,\n",
              "        0.65859873, 0.66210191, 0.65987261, 0.65923567, 0.65859873,\n",
              "        0.66082803, 0.65955414, 0.66528662, 0.66528662, 0.66178344,\n",
              "        0.65796178, 0.66528662, 0.66528662, 0.66178344, 0.65573248]),\n",
              " 'split1_train_balanced_accuracy': array([0.53114815, 0.53114815, 0.53114815, 0.51004081, 0.53114815,\n",
              "        0.53114815, 0.53114815, 0.50755686, 0.53260819, 0.53309457,\n",
              "        0.53260819, 0.50660025, 0.53239012, 0.53285138, 0.53263332,\n",
              "        0.50055548, 0.54248837, 0.54151561, 0.53478883, 0.50344685,\n",
              "        0.54297475, 0.54297475, 0.53432757, 0.50834476, 0.55687162,\n",
              "        0.55687162, 0.53478883, 0.50858167, 0.55526979, 0.55526979,\n",
              "        0.53432757, 0.51069321, 0.53068689, 0.53114815, 0.53114815,\n",
              "        0.50815182, 0.53114815, 0.53114815, 0.53068689, 0.50815182,\n",
              "        0.53260819, 0.53260819, 0.53260819, 0.50775787, 0.53260819,\n",
              "        0.53260819, 0.53260819, 0.50109032, 0.54151561, 0.54151561,\n",
              "        0.53405926, 0.50135864, 0.54200199, 0.54151561, 0.53432757,\n",
              "        0.50967827, 0.55662843, 0.55687162, 0.53405926, 0.50436936,\n",
              "        0.55687162, 0.55687162, 0.53503202, 0.50415129, 0.53114815,\n",
              "        0.53114815, 0.53114815, 0.50086508, 0.53114815, 0.53114815,\n",
              "        0.53114815, 0.50940367, 0.53260819, 0.53260819, 0.53306944,\n",
              "        0.50046125, 0.53260819, 0.53306944, 0.53260819, 0.50890652,\n",
              "        0.54151561, 0.54151561, 0.53405926, 0.50676806, 0.54151561,\n",
              "        0.54151561, 0.53405926, 0.50663435, 0.55687162, 0.55687162,\n",
              "        0.53405926, 0.50868038, 0.55687162, 0.55662843, 0.53478883,\n",
              "        0.50803427, 0.53068689, 0.53114815, 0.53068689, 0.50504867,\n",
              "        0.53114815, 0.53114815, 0.53114815, 0.50713688, 0.53214693,\n",
              "        0.53260819, 0.53285138, 0.50722931, 0.53260819, 0.53260819,\n",
              "        0.53260819, 0.50705343, 0.54151561, 0.54200199, 0.53478883,\n",
              "        0.50997261, 0.54200199, 0.54151561, 0.53478883, 0.50693587,\n",
              "        0.55687162, 0.55687162, 0.53478883, 0.50135864, 0.55662843,\n",
              "        0.55687162, 0.53405926, 0.50889845, 0.53114815, 0.53114815,\n",
              "        0.53114815, 0.5       , 0.53114815, 0.53114815, 0.53114815,\n",
              "        0.50125993, 0.53260819, 0.53214693, 0.53333776, 0.51064296,\n",
              "        0.53260819, 0.53355582, 0.53355582, 0.50546865, 0.54151561,\n",
              "        0.54151561, 0.53503202, 0.5078162 , 0.54200199, 0.54151561,\n",
              "        0.53405926, 0.51237042, 0.55687162, 0.55687162, 0.53478883,\n",
              "        0.50722931, 0.55687162, 0.55687162, 0.53478883, 0.50705343]),\n",
              " 'split1_train_f1': array([0.47454569, 0.47454569, 0.47454569, 0.45798843, 0.47454569,\n",
              "        0.47454569, 0.47454569, 0.42645954, 0.48839033, 0.48878001,\n",
              "        0.48839033, 0.41247915, 0.48781544, 0.48858514, 0.48800983,\n",
              "        0.43740865, 0.5145508 , 0.51369903, 0.49403526, 0.40350207,\n",
              "        0.51497711, 0.51497711, 0.49328075, 0.4190374 , 0.53951871,\n",
              "        0.53951871, 0.49403526, 0.45692147, 0.53714286, 0.53714286,\n",
              "        0.49328075, 0.42898767, 0.47371914, 0.47454569, 0.47454569,\n",
              "        0.4175526 , 0.47454569, 0.47454569, 0.47371914, 0.4175526 ,\n",
              "        0.48839033, 0.48839033, 0.48839033, 0.42139822, 0.48839033,\n",
              "        0.48839033, 0.48839033, 0.40005824, 0.51369903, 0.51369903,\n",
              "        0.49343907, 0.3994308 , 0.51412477, 0.51369903, 0.49328075,\n",
              "        0.42317829, 0.53929149, 0.53951871, 0.49343907, 0.405468  ,\n",
              "        0.53951871, 0.53951871, 0.49423413, 0.40460977, 0.47454569,\n",
              "        0.47454569, 0.47454569, 0.42543834, 0.47454569, 0.47454569,\n",
              "        0.47454569, 0.46014398, 0.48839033, 0.48839033, 0.48915853,\n",
              "        0.39668682, 0.48839033, 0.48915853, 0.48839033, 0.41727814,\n",
              "        0.51369903, 0.51369903, 0.49343907, 0.41469422, 0.51369903,\n",
              "        0.51369903, 0.49343907, 0.42463932, 0.53951871, 0.53951871,\n",
              "        0.49343907, 0.42325865, 0.53951871, 0.53929149, 0.49403526,\n",
              "        0.41397067, 0.47371914, 0.47454569, 0.47371914, 0.40730102,\n",
              "        0.47454569, 0.47454569, 0.47454569, 0.41132699, 0.48762112,\n",
              "        0.48839033, 0.48858514, 0.41564855, 0.48839033, 0.48839033,\n",
              "        0.48839033, 0.42032574, 0.51369903, 0.51412477, 0.49403526,\n",
              "        0.43988483, 0.51412477, 0.51369903, 0.49403526, 0.41687165,\n",
              "        0.53951871, 0.53951871, 0.49403526, 0.3994308 , 0.53929149,\n",
              "        0.53951871, 0.49343907, 0.42404367, 0.47454569, 0.47454569,\n",
              "        0.47454569, 0.39568899, 0.47454569, 0.47454569, 0.47454569,\n",
              "        0.43839565, 0.48839033, 0.48762112, 0.48897494, 0.43022008,\n",
              "        0.48839033, 0.48954916, 0.48954916, 0.42266972, 0.51369903,\n",
              "        0.51369903, 0.49423413, 0.41313588, 0.51412477, 0.51369903,\n",
              "        0.49343907, 0.43064215, 0.53951871, 0.53951871, 0.49403526,\n",
              "        0.41564855, 0.53951871, 0.53951871, 0.49403526, 0.42032574]),\n",
              " 'split1_train_precision': array([0.65478261, 0.65478261, 0.65478261, 0.53330164, 0.65478261,\n",
              "        0.65478261, 0.65478261, 0.56964713, 0.60945935, 0.61199823,\n",
              "        0.60945935, 0.68889246, 0.60961425, 0.61072346, 0.61089003,\n",
              "        0.50225319, 0.59998564, 0.59669187, 0.60810411, 0.77261436,\n",
              "        0.60165962, 0.60165962, 0.60706564, 0.64637294, 0.61208135,\n",
              "        0.61208135, 0.60810411, 0.52779897, 0.61005092, 0.61005092,\n",
              "        0.60706564, 0.61246868, 0.65346263, 0.65478261, 0.65478261,\n",
              "        0.66258591, 0.65478261, 0.65478261, 0.65346263, 0.66258591,\n",
              "        0.60945935, 0.60945935, 0.60945935, 0.60190069, 0.60945935,\n",
              "        0.60945935, 0.60945935, 0.57763578, 0.59669187, 0.59669187,\n",
              "        0.60468062, 0.66102957, 0.59832983, 0.59669187, 0.60706564,\n",
              "        0.63961165, 0.61137468, 0.61208135, 0.60468062, 0.78292513,\n",
              "        0.61208135, 0.61208135, 0.60926276, 0.82832961, 0.65478261,\n",
              "        0.65478261, 0.65478261, 0.50527787, 0.65478261, 0.65478261,\n",
              "        0.65478261, 0.52890204, 0.60945935, 0.60945935, 0.61056091,\n",
              "        0.82749283, 0.60945935, 0.61056091, 0.60945935, 0.71275456,\n",
              "        0.59669187, 0.59669187, 0.60468062, 0.64711648, 0.59669187,\n",
              "        0.59669187, 0.60468062, 0.56269212, 0.61208135, 0.61208135,\n",
              "        0.60468062, 0.61008872, 0.61208135, 0.61137468, 0.60810411,\n",
              "        0.76103563, 0.65346263, 0.65478261, 0.65346263, 0.7571063 ,\n",
              "        0.65478261, 0.65478261, 0.65478261, 0.7995988 , 0.60834917,\n",
              "        0.60945935, 0.61072346, 0.65256998, 0.60945935, 0.60945935,\n",
              "        0.60945935, 0.59264771, 0.59669187, 0.59832983, 0.60810411,\n",
              "        0.55808426, 0.59832983, 0.59669187, 0.60810411, 0.62166002,\n",
              "        0.61208135, 0.61208135, 0.60810411, 0.66102957, 0.61137468,\n",
              "        0.61208135, 0.60468062, 0.60909951, 0.65478261, 0.65478261,\n",
              "        0.65478261, 0.32738854, 0.65478261, 0.65478261, 0.65478261,\n",
              "        0.50511062, 0.60945935, 0.60834917, 0.61328378, 0.60318651,\n",
              "        0.60945935, 0.61309775, 0.61309775, 0.55233956, 0.59669187,\n",
              "        0.59669187, 0.60926276, 0.77916667, 0.59832983, 0.59669187,\n",
              "        0.60468062, 0.63793621, 0.61208135, 0.61208135, 0.60810411,\n",
              "        0.65256998, 0.61208135, 0.61208135, 0.60810411, 0.59264771]),\n",
              " 'split1_train_recall': array([0.53114815, 0.53114815, 0.53114815, 0.51004081, 0.53114815,\n",
              "        0.53114815, 0.53114815, 0.50755686, 0.53260819, 0.53309457,\n",
              "        0.53260819, 0.50660025, 0.53239012, 0.53285138, 0.53263332,\n",
              "        0.50055548, 0.54248837, 0.54151561, 0.53478883, 0.50344685,\n",
              "        0.54297475, 0.54297475, 0.53432757, 0.50834476, 0.55687162,\n",
              "        0.55687162, 0.53478883, 0.50858167, 0.55526979, 0.55526979,\n",
              "        0.53432757, 0.51069321, 0.53068689, 0.53114815, 0.53114815,\n",
              "        0.50815182, 0.53114815, 0.53114815, 0.53068689, 0.50815182,\n",
              "        0.53260819, 0.53260819, 0.53260819, 0.50775787, 0.53260819,\n",
              "        0.53260819, 0.53260819, 0.50109032, 0.54151561, 0.54151561,\n",
              "        0.53405926, 0.50135864, 0.54200199, 0.54151561, 0.53432757,\n",
              "        0.50967827, 0.55662843, 0.55687162, 0.53405926, 0.50436936,\n",
              "        0.55687162, 0.55687162, 0.53503202, 0.50415129, 0.53114815,\n",
              "        0.53114815, 0.53114815, 0.50086508, 0.53114815, 0.53114815,\n",
              "        0.53114815, 0.50940367, 0.53260819, 0.53260819, 0.53306944,\n",
              "        0.50046125, 0.53260819, 0.53306944, 0.53260819, 0.50890652,\n",
              "        0.54151561, 0.54151561, 0.53405926, 0.50676806, 0.54151561,\n",
              "        0.54151561, 0.53405926, 0.50663435, 0.55687162, 0.55687162,\n",
              "        0.53405926, 0.50868038, 0.55687162, 0.55662843, 0.53478883,\n",
              "        0.50803427, 0.53068689, 0.53114815, 0.53068689, 0.50504867,\n",
              "        0.53114815, 0.53114815, 0.53114815, 0.50713688, 0.53214693,\n",
              "        0.53260819, 0.53285138, 0.50722931, 0.53260819, 0.53260819,\n",
              "        0.53260819, 0.50705343, 0.54151561, 0.54200199, 0.53478883,\n",
              "        0.50997261, 0.54200199, 0.54151561, 0.53478883, 0.50693587,\n",
              "        0.55687162, 0.55687162, 0.53478883, 0.50135864, 0.55662843,\n",
              "        0.55687162, 0.53405926, 0.50889845, 0.53114815, 0.53114815,\n",
              "        0.53114815, 0.5       , 0.53114815, 0.53114815, 0.53114815,\n",
              "        0.50125993, 0.53260819, 0.53214693, 0.53333776, 0.51064296,\n",
              "        0.53260819, 0.53355582, 0.53355582, 0.50546865, 0.54151561,\n",
              "        0.54151561, 0.53503202, 0.5078162 , 0.54200199, 0.54151561,\n",
              "        0.53405926, 0.51237042, 0.55687162, 0.55687162, 0.53478883,\n",
              "        0.50722931, 0.55687162, 0.55687162, 0.53478883, 0.50705343]),\n",
              " 'split2_test_accuracy': array([0.66475645, 0.66475645, 0.66475645, 0.65616046, 0.66475645,\n",
              "        0.66475645, 0.66475645, 0.65902579, 0.67335244, 0.67335244,\n",
              "        0.67335244, 0.66189112, 0.67335244, 0.67335244, 0.67335244,\n",
              "        0.65329513, 0.67908309, 0.67908309, 0.67335244, 0.65616046,\n",
              "        0.67908309, 0.67908309, 0.67335244, 0.6504298 , 0.67908309,\n",
              "        0.67908309, 0.67335244, 0.64183381, 0.67908309, 0.67908309,\n",
              "        0.67335244, 0.65329513, 0.66475645, 0.66475645, 0.66475645,\n",
              "        0.65902579, 0.66475645, 0.66475645, 0.66475645, 0.63037249,\n",
              "        0.67335244, 0.67335244, 0.67335244, 0.65616046, 0.67335244,\n",
              "        0.67335244, 0.67335244, 0.65329513, 0.67908309, 0.67908309,\n",
              "        0.67621777, 0.65329513, 0.67908309, 0.67908309, 0.67335244,\n",
              "        0.62750716, 0.67908309, 0.67908309, 0.67335244, 0.65616046,\n",
              "        0.67908309, 0.67908309, 0.67335244, 0.65902579, 0.66475645,\n",
              "        0.66475645, 0.66475645, 0.65329513, 0.66475645, 0.66475645,\n",
              "        0.66475645, 0.65616046, 0.67335244, 0.67335244, 0.67048711,\n",
              "        0.65902579, 0.67335244, 0.67335244, 0.67335244, 0.65616046,\n",
              "        0.67908309, 0.67908309, 0.67335244, 0.65902579, 0.67908309,\n",
              "        0.67908309, 0.67335244, 0.65616046, 0.67908309, 0.67908309,\n",
              "        0.67621777, 0.65616046, 0.67908309, 0.67908309, 0.67335244,\n",
              "        0.65329513, 0.66475645, 0.66475645, 0.66475645, 0.65329513,\n",
              "        0.66475645, 0.66475645, 0.66475645, 0.65329513, 0.67048711,\n",
              "        0.67335244, 0.67335244, 0.62750716, 0.67335244, 0.67335244,\n",
              "        0.67335244, 0.63323782, 0.67908309, 0.67908309, 0.67335244,\n",
              "        0.65902579, 0.67908309, 0.67908309, 0.67335244, 0.63037249,\n",
              "        0.67908309, 0.67908309, 0.67335244, 0.6504298 , 0.67908309,\n",
              "        0.67908309, 0.67335244, 0.65616046, 0.66475645, 0.66475645,\n",
              "        0.66475645, 0.65329513, 0.66475645, 0.66475645, 0.66475645,\n",
              "        0.65616046, 0.67335244, 0.67335244, 0.67335244, 0.63037249,\n",
              "        0.67335244, 0.67335244, 0.67335244, 0.65329513, 0.67908309,\n",
              "        0.67908309, 0.67335244, 0.65616046, 0.67908309, 0.67908309,\n",
              "        0.67335244, 0.65616046, 0.67908309, 0.67908309, 0.67335244,\n",
              "        0.65616046, 0.67908309, 0.67908309, 0.67335244, 0.6504298 ]),\n",
              " 'split2_test_balanced_accuracy': array([0.52622517, 0.52622517, 0.52622517, 0.51188923, 0.52622517,\n",
              "        0.52622517, 0.52622517, 0.50826446, 0.54443961, 0.54443961,\n",
              "        0.54443961, 0.51239669, 0.54443961, 0.54443961, 0.54443961,\n",
              "        0.507757  , 0.56821807, 0.56821807, 0.54831811, 0.50994998,\n",
              "        0.56821807, 0.56821807, 0.54831811, 0.49780702, 0.57791431,\n",
              "        0.57791431, 0.54831811, 0.50092431, 0.57791431, 0.57791431,\n",
              "        0.54831811, 0.50969624, 0.52622517, 0.52622517, 0.52622517,\n",
              "        0.51020371, 0.52622517, 0.52622517, 0.52622517, 0.49797013,\n",
              "        0.54443961, 0.54443961, 0.54443961, 0.51188923, 0.54443961,\n",
              "        0.54443961, 0.54443961, 0.50581775, 0.56821807, 0.56821807,\n",
              "        0.55245034, 0.50581775, 0.56821807, 0.56821807, 0.54831811,\n",
              "        0.49577715, 0.57791431, 0.57791431, 0.54831811, 0.50801073,\n",
              "        0.57791431, 0.57791431, 0.54831811, 0.50826446, 0.52622517,\n",
              "        0.52622517, 0.52622517, 0.50581775, 0.52622517, 0.52622517,\n",
              "        0.52622517, 0.50994998, 0.54443961, 0.54443961, 0.54224663,\n",
              "        0.50826446, 0.54443961, 0.54443961, 0.54443961, 0.50413223,\n",
              "        0.56821807, 0.56821807, 0.54831811, 0.51020371, 0.56821807,\n",
              "        0.56821807, 0.54831811, 0.51188923, 0.57791431, 0.57791431,\n",
              "        0.55245034, 0.50801073, 0.57791431, 0.57791431, 0.54831811,\n",
              "        0.50193925, 0.52622517, 0.52622517, 0.52622517, 0.50581775,\n",
              "        0.52622517, 0.52622517, 0.52622517, 0.5       , 0.54224663,\n",
              "        0.54443961, 0.54443961, 0.49577715, 0.54443961, 0.54443961,\n",
              "        0.54443961, 0.49822387, 0.56821807, 0.56821807, 0.54831811,\n",
              "        0.51020371, 0.56821807, 0.56821807, 0.54831811, 0.50184863,\n",
              "        0.57791431, 0.57791431, 0.54831811, 0.50556401, 0.57791431,\n",
              "        0.57791431, 0.54831811, 0.51188923, 0.52622517, 0.52622517,\n",
              "        0.52622517, 0.50969624, 0.52622517, 0.52622517, 0.52622517,\n",
              "        0.50801073, 0.54443961, 0.54443961, 0.54443961, 0.50184863,\n",
              "        0.54443961, 0.54443961, 0.54443961, 0.50581775, 0.56821807,\n",
              "        0.56821807, 0.54831811, 0.50413223, 0.56821807, 0.56821807,\n",
              "        0.54831811, 0.50801073, 0.57791431, 0.57791431, 0.54831811,\n",
              "        0.50607148, 0.57791431, 0.57791431, 0.54831811, 0.49780702]),\n",
              " 'split2_test_f1': array([0.46275903, 0.46275903, 0.46275903, 0.43282774, 0.46275903,\n",
              "        0.46275903, 0.46275903, 0.4127819 , 0.50127858, 0.50127858,\n",
              "        0.50127858, 0.42140609, 0.50127858, 0.50127858, 0.50127858,\n",
              "        0.42468086, 0.54984338, 0.54984338, 0.5112285 , 0.42598684,\n",
              "        0.54984338, 0.54984338, 0.5112285 , 0.39409722, 0.56795473,\n",
              "        0.56795473, 0.5112285 , 0.4260246 , 0.56795473, 0.56795473,\n",
              "        0.5112285 , 0.431466  , 0.46275903, 0.46275903, 0.46275903,\n",
              "        0.42016056, 0.46275903, 0.46275903, 0.46275903, 0.43853589,\n",
              "        0.50127858, 0.50127858, 0.50127858, 0.43282774, 0.50127858,\n",
              "        0.50127858, 0.50127858, 0.41766758, 0.54984338, 0.54984338,\n",
              "        0.51789098, 0.41766758, 0.54984338, 0.54984338, 0.5112285 ,\n",
              "        0.4370409 , 0.56795473, 0.56795473, 0.5112285 , 0.41891442,\n",
              "        0.56795473, 0.56795473, 0.5112285 , 0.4127819 , 0.46275903,\n",
              "        0.46275903, 0.46275903, 0.41766758, 0.46275903, 0.46275903,\n",
              "        0.46275903, 0.42598684, 0.50127858, 0.50127858, 0.49946998,\n",
              "        0.4127819 , 0.50127858, 0.50127858, 0.50127858, 0.40403005,\n",
              "        0.54984338, 0.54984338, 0.5112285 , 0.42016056, 0.54984338,\n",
              "        0.54984338, 0.5112285 , 0.43282774, 0.56795473, 0.56795473,\n",
              "        0.51789098, 0.41891442, 0.56795473, 0.56795473, 0.5112285 ,\n",
              "        0.40291269, 0.46275903, 0.46275903, 0.46275903, 0.41766758,\n",
              "        0.46275903, 0.46275903, 0.46275903, 0.39514731, 0.49946998,\n",
              "        0.50127858, 0.50127858, 0.4370409 , 0.50127858, 0.50127858,\n",
              "        0.50127858, 0.43418786, 0.54984338, 0.54984338, 0.5112285 ,\n",
              "        0.42016056, 0.54984338, 0.54984338, 0.5112285 , 0.44962776,\n",
              "        0.56795473, 0.56795473, 0.5112285 , 0.42337486, 0.56795473,\n",
              "        0.56795473, 0.5112285 , 0.43282774, 0.46275903, 0.46275903,\n",
              "        0.46275903, 0.431466  , 0.46275903, 0.46275903, 0.46275903,\n",
              "        0.41891442, 0.50127858, 0.50127858, 0.50127858, 0.44962776,\n",
              "        0.50127858, 0.50127858, 0.50127858, 0.41766758, 0.54984338,\n",
              "        0.54984338, 0.5112285 , 0.40403005, 0.54984338, 0.54984338,\n",
              "        0.5112285 , 0.41891442, 0.56795473, 0.56795473, 0.5112285 ,\n",
              "        0.41159942, 0.56795473, 0.56795473, 0.5112285 , 0.39409722]),\n",
              " 'split2_test_precision': array([0.65426439, 0.65426439, 0.65426439, 0.60718954, 0.65426439,\n",
              "        0.65426439, 0.65426439, 0.82853026, 0.66351027, 0.66351027,\n",
              "        0.66351027, 0.82947977, 0.66351027, 0.66351027, 0.66351027,\n",
              "        0.57844575, 0.64903389, 0.64903389, 0.65332413, 0.61466165,\n",
              "        0.64903389, 0.64903389, 0.65332413, 0.32614943, 0.64143308,\n",
              "        0.64143308, 0.65332413, 0.5054371 , 0.64143308, 0.64143308,\n",
              "        0.65332413, 0.57890855, 0.65426439, 0.65426439, 0.65426439,\n",
              "        0.70398551, 0.65426439, 0.65426439, 0.65426439, 0.49282051,\n",
              "        0.66351027, 0.66351027, 0.66351027, 0.60718954, 0.66351027,\n",
              "        0.66351027, 0.66351027, 0.57798834, 0.64903389, 0.64903389,\n",
              "        0.66099243, 0.57798834, 0.64903389, 0.64903389, 0.65332413,\n",
              "        0.48561728, 0.64143308, 0.64143308, 0.65332413, 0.62848837,\n",
              "        0.64143308, 0.64143308, 0.65332413, 0.82853026, 0.65426439,\n",
              "        0.65426439, 0.65426439, 0.57798834, 0.65426439, 0.65426439,\n",
              "        0.65426439, 0.61466165, 0.66351027, 0.66351027, 0.64942308,\n",
              "        0.82853026, 0.66351027, 0.66351027, 0.66351027, 0.82758621,\n",
              "        0.64903389, 0.64903389, 0.65332413, 0.70398551, 0.64903389,\n",
              "        0.64903389, 0.65332413, 0.60718954, 0.64143308, 0.64143308,\n",
              "        0.66099243, 0.62848837, 0.64143308, 0.64143308, 0.65332413,\n",
              "        0.57708934, 0.65426439, 0.65426439, 0.65426439, 0.57798834,\n",
              "        0.65426439, 0.65426439, 0.65426439, 0.32664756, 0.64942308,\n",
              "        0.66351027, 0.66351027, 0.48561728, 0.66351027, 0.66351027,\n",
              "        0.66351027, 0.49288618, 0.64903389, 0.64903389, 0.65332413,\n",
              "        0.70398551, 0.64903389, 0.64903389, 0.65332413, 0.50567423,\n",
              "        0.64143308, 0.64143308, 0.65332413, 0.5501634 , 0.64143308,\n",
              "        0.64143308, 0.65332413, 0.60718954, 0.65426439, 0.65426439,\n",
              "        0.65426439, 0.57890855, 0.65426439, 0.65426439, 0.65426439,\n",
              "        0.62848837, 0.66351027, 0.66351027, 0.66351027, 0.50567423,\n",
              "        0.66351027, 0.66351027, 0.66351027, 0.57798834, 0.64903389,\n",
              "        0.64903389, 0.65332413, 0.82758621, 0.64903389, 0.64903389,\n",
              "        0.65332413, 0.62848837, 0.64143308, 0.64143308, 0.65332413,\n",
              "        0.66136802, 0.64143308, 0.64143308, 0.65332413, 0.32614943]),\n",
              " 'split2_test_recall': array([0.52622517, 0.52622517, 0.52622517, 0.51188923, 0.52622517,\n",
              "        0.52622517, 0.52622517, 0.50826446, 0.54443961, 0.54443961,\n",
              "        0.54443961, 0.51239669, 0.54443961, 0.54443961, 0.54443961,\n",
              "        0.507757  , 0.56821807, 0.56821807, 0.54831811, 0.50994998,\n",
              "        0.56821807, 0.56821807, 0.54831811, 0.49780702, 0.57791431,\n",
              "        0.57791431, 0.54831811, 0.50092431, 0.57791431, 0.57791431,\n",
              "        0.54831811, 0.50969624, 0.52622517, 0.52622517, 0.52622517,\n",
              "        0.51020371, 0.52622517, 0.52622517, 0.52622517, 0.49797013,\n",
              "        0.54443961, 0.54443961, 0.54443961, 0.51188923, 0.54443961,\n",
              "        0.54443961, 0.54443961, 0.50581775, 0.56821807, 0.56821807,\n",
              "        0.55245034, 0.50581775, 0.56821807, 0.56821807, 0.54831811,\n",
              "        0.49577715, 0.57791431, 0.57791431, 0.54831811, 0.50801073,\n",
              "        0.57791431, 0.57791431, 0.54831811, 0.50826446, 0.52622517,\n",
              "        0.52622517, 0.52622517, 0.50581775, 0.52622517, 0.52622517,\n",
              "        0.52622517, 0.50994998, 0.54443961, 0.54443961, 0.54224663,\n",
              "        0.50826446, 0.54443961, 0.54443961, 0.54443961, 0.50413223,\n",
              "        0.56821807, 0.56821807, 0.54831811, 0.51020371, 0.56821807,\n",
              "        0.56821807, 0.54831811, 0.51188923, 0.57791431, 0.57791431,\n",
              "        0.55245034, 0.50801073, 0.57791431, 0.57791431, 0.54831811,\n",
              "        0.50193925, 0.52622517, 0.52622517, 0.52622517, 0.50581775,\n",
              "        0.52622517, 0.52622517, 0.52622517, 0.5       , 0.54224663,\n",
              "        0.54443961, 0.54443961, 0.49577715, 0.54443961, 0.54443961,\n",
              "        0.54443961, 0.49822387, 0.56821807, 0.56821807, 0.54831811,\n",
              "        0.51020371, 0.56821807, 0.56821807, 0.54831811, 0.50184863,\n",
              "        0.57791431, 0.57791431, 0.54831811, 0.50556401, 0.57791431,\n",
              "        0.57791431, 0.54831811, 0.51188923, 0.52622517, 0.52622517,\n",
              "        0.52622517, 0.50969624, 0.52622517, 0.52622517, 0.52622517,\n",
              "        0.50801073, 0.54443961, 0.54443961, 0.54443961, 0.50184863,\n",
              "        0.54443961, 0.54443961, 0.54443961, 0.50581775, 0.56821807,\n",
              "        0.56821807, 0.54831811, 0.50413223, 0.56821807, 0.56821807,\n",
              "        0.54831811, 0.50801073, 0.57791431, 0.57791431, 0.54831811,\n",
              "        0.50607148, 0.57791431, 0.57791431, 0.54831811, 0.49780702]),\n",
              " 'split2_train_accuracy': array([0.66719745, 0.66815287, 0.66815287, 0.6566879 , 0.66719745,\n",
              "        0.66783439, 0.66815287, 0.65700637, 0.66242038, 0.66242038,\n",
              "        0.66242038, 0.65700637, 0.66178344, 0.66242038, 0.66242038,\n",
              "        0.6611465 , 0.65700637, 0.65700637, 0.6633758 , 0.65859873,\n",
              "        0.66019108, 0.65987261, 0.66305732, 0.65573248, 0.66082803,\n",
              "        0.66082803, 0.6633758 , 0.65636943, 0.66019108, 0.66019108,\n",
              "        0.66305732, 0.6544586 , 0.66815287, 0.66719745, 0.66815287,\n",
              "        0.65828025, 0.66783439, 0.66719745, 0.66815287, 0.63949045,\n",
              "        0.66242038, 0.66242038, 0.66242038, 0.65859873, 0.66242038,\n",
              "        0.66242038, 0.66273885, 0.65923567, 0.65700637, 0.65732484,\n",
              "        0.66242038, 0.65732484, 0.65796178, 0.65700637, 0.66305732,\n",
              "        0.64044586, 0.66082803, 0.66050955, 0.6633758 , 0.6589172 ,\n",
              "        0.66050955, 0.66050955, 0.6633758 , 0.65509554, 0.66719745,\n",
              "        0.66719745, 0.66783439, 0.65923567, 0.66783439, 0.66815287,\n",
              "        0.66719745, 0.65859873, 0.66242038, 0.66178344, 0.66210191,\n",
              "        0.6566879 , 0.66242038, 0.66242038, 0.66242038, 0.65509554,\n",
              "        0.65700637, 0.65700637, 0.6633758 , 0.6589172 , 0.65700637,\n",
              "        0.65700637, 0.6633758 , 0.65477707, 0.66082803, 0.66082803,\n",
              "        0.6633758 , 0.65859873, 0.66050955, 0.66082803, 0.66305732,\n",
              "        0.65732484, 0.66783439, 0.66783439, 0.66719745, 0.65828025,\n",
              "        0.66815287, 0.66815287, 0.66815287, 0.65477707, 0.66242038,\n",
              "        0.66242038, 0.66242038, 0.64267516, 0.66273885, 0.66242038,\n",
              "        0.66242038, 0.63821656, 0.65700637, 0.65700637, 0.66305732,\n",
              "        0.65828025, 0.65700637, 0.65700637, 0.66305732, 0.63726115,\n",
              "        0.66082803, 0.66082803, 0.6633758 , 0.66242038, 0.66082803,\n",
              "        0.66082803, 0.6633758 , 0.6566879 , 0.66783439, 0.66815287,\n",
              "        0.66815287, 0.6522293 , 0.66815287, 0.66815287, 0.66719745,\n",
              "        0.65923567, 0.66242038, 0.66242038, 0.66178344, 0.63407643,\n",
              "        0.66242038, 0.66242038, 0.66178344, 0.6589172 , 0.65700637,\n",
              "        0.65796178, 0.66242038, 0.65541401, 0.65700637, 0.65700637,\n",
              "        0.6633758 , 0.65955414, 0.66082803, 0.66082803, 0.6633758 ,\n",
              "        0.6566879 , 0.66082803, 0.66082803, 0.6633758 , 0.65573248]),\n",
              " 'split2_train_balanced_accuracy': array([0.52823794, 0.5296217 , 0.5296217 , 0.50909138, 0.52823794,\n",
              "        0.52916044, 0.5296217 , 0.50344685, 0.53156812, 0.53156812,\n",
              "        0.53156812, 0.50366491, 0.53064561, 0.53156812, 0.53156812,\n",
              "        0.51336831, 0.53855514, 0.53855514, 0.53556865, 0.50924214,\n",
              "        0.54098705, 0.54052579, 0.5351074 , 0.50160183, 0.55019599,\n",
              "        0.55019599, 0.53556865, 0.5136456 , 0.54927348, 0.54927348,\n",
              "        0.5351074 , 0.5089155 , 0.5296217 , 0.52823794, 0.5296217 ,\n",
              "        0.50725444, 0.52916044, 0.52823794, 0.5296217 , 0.50773454,\n",
              "        0.53156812, 0.53156812, 0.53156812, 0.51207697, 0.53156812,\n",
              "        0.53156812, 0.53202938, 0.50972852, 0.53855514, 0.5390164 ,\n",
              "        0.53483908, 0.50826938, 0.53928471, 0.53855514, 0.5351074 ,\n",
              "        0.50999056, 0.55019599, 0.54973473, 0.53556865, 0.50883114,\n",
              "        0.54973473, 0.54973473, 0.53556865, 0.50067932, 0.52823794,\n",
              "        0.52823794, 0.52916044, 0.50907433, 0.52937851, 0.5296217 ,\n",
              "        0.52823794, 0.51055053, 0.53156812, 0.53064561, 0.53132493,\n",
              "        0.50320366, 0.53156812, 0.53156812, 0.53156812, 0.50067932,\n",
              "        0.53855514, 0.53855514, 0.53556865, 0.50774082, 0.53855514,\n",
              "        0.53855514, 0.53556865, 0.50828643, 0.55019599, 0.55019599,\n",
              "        0.53556865, 0.50836989, 0.54973473, 0.55019599, 0.5351074 ,\n",
              "        0.50412616, 0.52916044, 0.52916044, 0.52823794, 0.50834476,\n",
              "        0.5296217 , 0.5296217 , 0.5296217 , 0.5       , 0.53178619,\n",
              "        0.53156812, 0.53156812, 0.50907613, 0.53202938, 0.53156812,\n",
              "        0.53156812, 0.50370888, 0.53855514, 0.53855514, 0.5351074 ,\n",
              "        0.50703638, 0.53855514, 0.53855514, 0.5351074 , 0.5090851 ,\n",
              "        0.55019599, 0.55019599, 0.53556865, 0.51499526, 0.55019599,\n",
              "        0.55019599, 0.53556865, 0.50952751, 0.52916044, 0.5296217 ,\n",
              "        0.5296217 , 0.5063409 , 0.5296217 , 0.5296217 , 0.52823794,\n",
              "        0.50885627, 0.53156812, 0.53156812, 0.53064561, 0.50556287,\n",
              "        0.53156812, 0.53156812, 0.53064561, 0.50883114, 0.53855514,\n",
              "        0.53928471, 0.53418489, 0.50092251, 0.53855514, 0.53855514,\n",
              "        0.53556865, 0.50931752, 0.55019599, 0.55019599, 0.53556865,\n",
              "        0.50407591, 0.55019599, 0.55019599, 0.53556865, 0.50160183]),\n",
              " 'split2_train_f1': array([0.46746589, 0.46998639, 0.46998639, 0.42546053, 0.46746589,\n",
              "        0.46914744, 0.46998639, 0.40350207, 0.48470909, 0.48470909,\n",
              "        0.48470909, 0.4043619 , 0.48314632, 0.48470909, 0.48470909,\n",
              "        0.43060838, 0.50874604, 0.50874604, 0.4939122 , 0.42158605,\n",
              "        0.5108488 , 0.51014212, 0.49315247, 0.39955057, 0.52992367,\n",
              "        0.52992367, 0.4939122 , 0.44172406, 0.52860143, 0.52860143,\n",
              "        0.49315247, 0.42978453, 0.46998639, 0.46746589, 0.46998639,\n",
              "        0.41496186, 0.46914744, 0.46746589, 0.46998639, 0.45405563,\n",
              "        0.48470909, 0.48470909, 0.48470909, 0.4317264 , 0.48470909,\n",
              "        0.48470909, 0.4854889 , 0.42186605, 0.50874604, 0.50944794,\n",
              "        0.4933174 , 0.42102597, 0.50937614, 0.50874604, 0.49315247,\n",
              "        0.45883878, 0.52992367, 0.52926292, 0.4939122 , 0.41931283,\n",
              "        0.52926292, 0.52926292, 0.4939122 , 0.39756494, 0.46746589,\n",
              "        0.46746589, 0.46914744, 0.41945053, 0.46980746, 0.46998639,\n",
              "        0.46746589, 0.42632724, 0.48470909, 0.48314632, 0.48451721,\n",
              "        0.40337895, 0.48470909, 0.48470909, 0.48470909, 0.39756494,\n",
              "        0.50874604, 0.50874604, 0.4939122 , 0.41522945, 0.50874604,\n",
              "        0.50874604, 0.4939122 , 0.42689925, 0.52992367, 0.52992367,\n",
              "        0.4939122 , 0.41836539, 0.52926292, 0.52992367, 0.49315247,\n",
              "        0.40534322, 0.46914744, 0.46914744, 0.46746589, 0.4190374 ,\n",
              "        0.46998639, 0.46998639, 0.46998639, 0.39568899, 0.48529653,\n",
              "        0.48470909, 0.48470909, 0.45269143, 0.4854889 , 0.48470909,\n",
              "        0.48470909, 0.44458389, 0.50874604, 0.50874604, 0.49315247,\n",
              "        0.41413744, 0.50874604, 0.50874604, 0.49315247, 0.46117972,\n",
              "        0.52992367, 0.52992367, 0.4939122 , 0.43352014, 0.52992367,\n",
              "        0.52992367, 0.4939122 , 0.42700885, 0.46914744, 0.46998639,\n",
              "        0.46998639, 0.42572673, 0.46998639, 0.46998639, 0.46746589,\n",
              "        0.41863926, 0.48470909, 0.48470909, 0.48314632, 0.45644908,\n",
              "        0.48470909, 0.48470909, 0.48314632, 0.41931283, 0.50874604,\n",
              "        0.50937614, 0.49163001, 0.39768298, 0.50874604, 0.50874604,\n",
              "        0.4939122 , 0.41958823, 0.52992367, 0.52992367, 0.4939122 ,\n",
              "        0.40679505, 0.52992367, 0.52992367, 0.4939122 , 0.39955057]),\n",
              " 'split2_train_precision': array([0.65736214, 0.66159337, 0.66159337, 0.60292385, 0.65736214,\n",
              "        0.66020213, 0.66159337, 0.77261436, 0.61334287, 0.61334287,\n",
              "        0.61334287, 0.73731079, 0.61100276, 0.61334287, 0.61334287,\n",
              "        0.66667413, 0.59241766, 0.59241766, 0.61433368, 0.64472612,\n",
              "        0.60094828, 0.60008865, 0.61328611, 0.7277512 , 0.60229148,\n",
              "        0.60229148, 0.61433368, 0.59205186, 0.60084382, 0.60084382,\n",
              "        0.61328611, 0.57651873, 0.66159337, 0.65736214, 0.66159337,\n",
              "        0.66775439, 0.66020213, 0.65736214, 0.66159337, 0.52617513,\n",
              "        0.61334287, 0.61334287, 0.61334287, 0.6218492 , 0.61334287,\n",
              "        0.61334287, 0.61449817, 0.65916432, 0.59241766, 0.59327481,\n",
              "        0.61071471, 0.61928803, 0.59492636, 0.59241766, 0.61328611,\n",
              "        0.53236285, 0.60229148, 0.60156911, 0.61433368, 0.6627432 ,\n",
              "        0.60156911, 0.60156911, 0.61433368, 0.66087557, 0.65736214,\n",
              "        0.65736214, 0.66020213, 0.67157012, 0.65914985, 0.66159337,\n",
              "        0.65736214, 0.63154244, 0.61334287, 0.61100276, 0.61197993,\n",
              "        0.72811502, 0.61334287, 0.61334287, 0.61334287, 0.66087557,\n",
              "        0.59241766, 0.59241766, 0.61433368, 0.69122358, 0.59241766,\n",
              "        0.59241766, 0.61433368, 0.57930809, 0.60229148, 0.60229148,\n",
              "        0.61433368, 0.65825104, 0.60156911, 0.60229148, 0.61328611,\n",
              "        0.74499147, 0.66020213, 0.66020213, 0.65736214, 0.64637294,\n",
              "        0.66159337, 0.66159337, 0.66159337, 0.32738854, 0.61313649,\n",
              "        0.61334287, 0.61334287, 0.53346768, 0.61449817, 0.61334287,\n",
              "        0.61334287, 0.51392811, 0.59241766, 0.59241766, 0.61328611,\n",
              "        0.67382148, 0.59241766, 0.59241766, 0.61328611, 0.52694635,\n",
              "        0.60229148, 0.60229148, 0.61433368, 0.68084416, 0.60229148,\n",
              "        0.60229148, 0.61433368, 0.60164867, 0.66020213, 0.66159337,\n",
              "        0.66159337, 0.55505173, 0.66159337, 0.66159337, 0.65736214,\n",
              "        0.67663588, 0.61334287, 0.61334287, 0.61100276, 0.51649945,\n",
              "        0.61334287, 0.61334287, 0.61100276, 0.6627432 , 0.59241766,\n",
              "        0.59492636, 0.61116753, 0.8275972 , 0.59241766, 0.59241766,\n",
              "        0.61433368, 0.68087117, 0.60229148, 0.60229148, 0.61433368,\n",
              "        0.66164852, 0.60229148, 0.60229148, 0.61433368, 0.7277512 ]),\n",
              " 'split2_train_recall': array([0.52823794, 0.5296217 , 0.5296217 , 0.50909138, 0.52823794,\n",
              "        0.52916044, 0.5296217 , 0.50344685, 0.53156812, 0.53156812,\n",
              "        0.53156812, 0.50366491, 0.53064561, 0.53156812, 0.53156812,\n",
              "        0.51336831, 0.53855514, 0.53855514, 0.53556865, 0.50924214,\n",
              "        0.54098705, 0.54052579, 0.5351074 , 0.50160183, 0.55019599,\n",
              "        0.55019599, 0.53556865, 0.5136456 , 0.54927348, 0.54927348,\n",
              "        0.5351074 , 0.5089155 , 0.5296217 , 0.52823794, 0.5296217 ,\n",
              "        0.50725444, 0.52916044, 0.52823794, 0.5296217 , 0.50773454,\n",
              "        0.53156812, 0.53156812, 0.53156812, 0.51207697, 0.53156812,\n",
              "        0.53156812, 0.53202938, 0.50972852, 0.53855514, 0.5390164 ,\n",
              "        0.53483908, 0.50826938, 0.53928471, 0.53855514, 0.5351074 ,\n",
              "        0.50999056, 0.55019599, 0.54973473, 0.53556865, 0.50883114,\n",
              "        0.54973473, 0.54973473, 0.53556865, 0.50067932, 0.52823794,\n",
              "        0.52823794, 0.52916044, 0.50907433, 0.52937851, 0.5296217 ,\n",
              "        0.52823794, 0.51055053, 0.53156812, 0.53064561, 0.53132493,\n",
              "        0.50320366, 0.53156812, 0.53156812, 0.53156812, 0.50067932,\n",
              "        0.53855514, 0.53855514, 0.53556865, 0.50774082, 0.53855514,\n",
              "        0.53855514, 0.53556865, 0.50828643, 0.55019599, 0.55019599,\n",
              "        0.53556865, 0.50836989, 0.54973473, 0.55019599, 0.5351074 ,\n",
              "        0.50412616, 0.52916044, 0.52916044, 0.52823794, 0.50834476,\n",
              "        0.5296217 , 0.5296217 , 0.5296217 , 0.5       , 0.53178619,\n",
              "        0.53156812, 0.53156812, 0.50907613, 0.53202938, 0.53156812,\n",
              "        0.53156812, 0.50370888, 0.53855514, 0.53855514, 0.5351074 ,\n",
              "        0.50703638, 0.53855514, 0.53855514, 0.5351074 , 0.5090851 ,\n",
              "        0.55019599, 0.55019599, 0.53556865, 0.51499526, 0.55019599,\n",
              "        0.55019599, 0.53556865, 0.50952751, 0.52916044, 0.5296217 ,\n",
              "        0.5296217 , 0.5063409 , 0.5296217 , 0.5296217 , 0.52823794,\n",
              "        0.50885627, 0.53156812, 0.53156812, 0.53064561, 0.50556287,\n",
              "        0.53156812, 0.53156812, 0.53064561, 0.50883114, 0.53855514,\n",
              "        0.53928471, 0.53418489, 0.50092251, 0.53855514, 0.53855514,\n",
              "        0.53556865, 0.50931752, 0.55019599, 0.55019599, 0.53556865,\n",
              "        0.50407591, 0.55019599, 0.55019599, 0.53556865, 0.50160183]),\n",
              " 'split3_test_accuracy': array([0.6504298 , 0.6504298 , 0.6504298 , 0.65329513, 0.6504298 ,\n",
              "        0.6504298 , 0.6504298 , 0.65902579, 0.64183381, 0.64469914,\n",
              "        0.64469914, 0.66475645, 0.64469914, 0.64469914, 0.64469914,\n",
              "        0.65329513, 0.63896848, 0.63896848, 0.63610315, 0.65902579,\n",
              "        0.64183381, 0.64183381, 0.63896848, 0.65902579, 0.63037249,\n",
              "        0.63037249, 0.63896848, 0.66189112, 0.63037249, 0.63037249,\n",
              "        0.63896848, 0.66189112, 0.6504298 , 0.6504298 , 0.6504298 ,\n",
              "        0.66189112, 0.6504298 , 0.6504298 , 0.6504298 , 0.66189112,\n",
              "        0.64469914, 0.64469914, 0.64469914, 0.66475645, 0.64469914,\n",
              "        0.64469914, 0.64469914, 0.66475645, 0.63896848, 0.63896848,\n",
              "        0.63610315, 0.66189112, 0.63896848, 0.63896848, 0.63896848,\n",
              "        0.65616046, 0.63037249, 0.63037249, 0.63610315, 0.66762178,\n",
              "        0.63037249, 0.63037249, 0.63610315, 0.66189112, 0.6504298 ,\n",
              "        0.6504298 , 0.6504298 , 0.65902579, 0.6504298 , 0.6504298 ,\n",
              "        0.6504298 , 0.66189112, 0.64469914, 0.64469914, 0.64469914,\n",
              "        0.65902579, 0.64469914, 0.64469914, 0.64469914, 0.65329513,\n",
              "        0.63896848, 0.63896848, 0.63610315, 0.66189112, 0.63896848,\n",
              "        0.63896848, 0.63896848, 0.64756447, 0.63037249, 0.63037249,\n",
              "        0.63610315, 0.65902579, 0.63037249, 0.63037249, 0.63610315,\n",
              "        0.65902579, 0.6504298 , 0.6504298 , 0.6504298 , 0.66189112,\n",
              "        0.6504298 , 0.6504298 , 0.6504298 , 0.66762178, 0.64469914,\n",
              "        0.64469914, 0.64469914, 0.66189112, 0.64469914, 0.64469914,\n",
              "        0.64469914, 0.66189112, 0.63896848, 0.63896848, 0.63610315,\n",
              "        0.66189112, 0.63896848, 0.63896848, 0.63896848, 0.65902579,\n",
              "        0.63037249, 0.63037249, 0.63610315, 0.66189112, 0.63037249,\n",
              "        0.63037249, 0.63610315, 0.65902579, 0.6504298 , 0.6504298 ,\n",
              "        0.6504298 , 0.66475645, 0.6504298 , 0.6504298 , 0.6504298 ,\n",
              "        0.66189112, 0.64469914, 0.64469914, 0.64469914, 0.66189112,\n",
              "        0.64469914, 0.64469914, 0.64469914, 0.66189112, 0.63896848,\n",
              "        0.63896848, 0.63610315, 0.65902579, 0.63896848, 0.63896848,\n",
              "        0.63610315, 0.65329513, 0.63037249, 0.63037249, 0.63610315,\n",
              "        0.65329513, 0.63037249, 0.63037249, 0.63610315, 0.66189112]),\n",
              " 'split3_test_balanced_accuracy': array([0.51138176, 0.51138176, 0.51138176, 0.5       , 0.51138176,\n",
              "        0.51138176, 0.51138176, 0.51214296, 0.51449906, 0.51863129,\n",
              "        0.51863129, 0.52040742, 0.51863129, 0.51863129, 0.51863129,\n",
              "        0.5       , 0.52782007, 0.52782007, 0.51205234, 0.51020371,\n",
              "        0.53001305, 0.53001305, 0.51424532, 0.51214296, 0.52511962,\n",
              "        0.52511962, 0.51618457, 0.51627519, 0.52511962, 0.52511962,\n",
              "        0.51424532, 0.51627519, 0.51138176, 0.51138176, 0.51138176,\n",
              "        0.53178918, 0.51138176, 0.51138176, 0.51138176, 0.51239669,\n",
              "        0.51863129, 0.51863129, 0.51863129, 0.51846817, 0.51863129,\n",
              "        0.51863129, 0.51863129, 0.52040742, 0.52782007, 0.52782007,\n",
              "        0.51205234, 0.51627519, 0.52782007, 0.52782007, 0.51618457,\n",
              "        0.53128172, 0.52511962, 0.52511962, 0.51205234, 0.5264789 ,\n",
              "        0.52511962, 0.52511962, 0.51205234, 0.51821444, 0.51138176,\n",
              "        0.51138176, 0.51138176, 0.50826446, 0.51138176, 0.51138176,\n",
              "        0.51138176, 0.51239669, 0.51863129, 0.51863129, 0.51863129,\n",
              "        0.51214296, 0.51863129, 0.51863129, 0.51863129, 0.5       ,\n",
              "        0.52782007, 0.52782007, 0.51205234, 0.51239669, 0.52782007,\n",
              "        0.52782007, 0.51618457, 0.51694577, 0.52511962, 0.52511962,\n",
              "        0.51205234, 0.51214296, 0.52511962, 0.52511962, 0.51205234,\n",
              "        0.52377845, 0.51138176, 0.51138176, 0.51138176, 0.51239669,\n",
              "        0.51138176, 0.51138176, 0.51138176, 0.5303574 , 0.51863129,\n",
              "        0.51863129, 0.51863129, 0.51433594, 0.51863129, 0.51863129,\n",
              "        0.51863129, 0.51239669, 0.52782007, 0.52782007, 0.51205234,\n",
              "        0.51239669, 0.52782007, 0.52782007, 0.51618457, 0.51214296,\n",
              "        0.52511962, 0.52511962, 0.51205234, 0.51627519, 0.52511962,\n",
              "        0.52511962, 0.51205234, 0.50826446, 0.51138176, 0.51138176,\n",
              "        0.51138176, 0.52234667, 0.51138176, 0.51138176, 0.51138176,\n",
              "        0.51627519, 0.51863129, 0.51863129, 0.51863129, 0.51239669,\n",
              "        0.51863129, 0.51863129, 0.51863129, 0.51433594, 0.52782007,\n",
              "        0.52782007, 0.51205234, 0.51602146, 0.52782007, 0.52782007,\n",
              "        0.51205234, 0.5       , 0.52511962, 0.52511962, 0.51205234,\n",
              "        0.5       , 0.52511962, 0.52511962, 0.51205234, 0.51627519]),\n",
              " 'split3_test_f1': array([0.44292966, 0.44292966, 0.44292966, 0.39514731, 0.44292966,\n",
              "        0.44292966, 0.44292966, 0.42729291, 0.46669356, 0.47352798,\n",
              "        0.47352798, 0.44369968, 0.47352798, 0.47352798, 0.47352798,\n",
              "        0.39514731, 0.50206088, 0.50206088, 0.46845356, 0.42016056,\n",
              "        0.50395133, 0.50395133, 0.4701417 , 0.42729291, 0.50431049,\n",
              "        0.50431049, 0.47510027, 0.43555373, 0.50431049, 0.50431049,\n",
              "        0.4701417 , 0.43555373, 0.44292966, 0.44292966, 0.44292966,\n",
              "        0.48377958, 0.44292966, 0.44292966, 0.44292966, 0.42140609,\n",
              "        0.47352798, 0.47352798, 0.47352798, 0.43691824, 0.47352798,\n",
              "        0.47352798, 0.47352798, 0.44369968, 0.50206088, 0.50206088,\n",
              "        0.46845356, 0.43555373, 0.50206088, 0.50206088, 0.47510027,\n",
              "        0.49051095, 0.50431049, 0.50431049, 0.46845356, 0.45813256,\n",
              "        0.50431049, 0.50431049, 0.46845356, 0.44228061, 0.44292966,\n",
              "        0.44292966, 0.44292966, 0.4127819 , 0.44292966, 0.44292966,\n",
              "        0.44292966, 0.42140609, 0.47352798, 0.47352798, 0.47352798,\n",
              "        0.42729291, 0.47352798, 0.47352798, 0.47352798, 0.39514731,\n",
              "        0.50206088, 0.50206088, 0.46845356, 0.42140609, 0.50206088,\n",
              "        0.50206088, 0.47510027, 0.4646505 , 0.50431049, 0.50431049,\n",
              "        0.46845356, 0.42729291, 0.50431049, 0.50431049, 0.46845356,\n",
              "        0.46550237, 0.44292966, 0.44292966, 0.44292966, 0.42140609,\n",
              "        0.44292966, 0.44292966, 0.44292966, 0.47032656, 0.47352798,\n",
              "        0.47352798, 0.47352798, 0.42859918, 0.47352798, 0.47352798,\n",
              "        0.47352798, 0.42140609, 0.50206088, 0.50206088, 0.46845356,\n",
              "        0.42140609, 0.50206088, 0.50206088, 0.47510027, 0.42729291,\n",
              "        0.50431049, 0.50431049, 0.46845356, 0.43555373, 0.50431049,\n",
              "        0.50431049, 0.46845356, 0.4127819 , 0.44292966, 0.44292966,\n",
              "        0.44292966, 0.45026051, 0.44292966, 0.44292966, 0.44292966,\n",
              "        0.43555373, 0.47352798, 0.47352798, 0.47352798, 0.42140609,\n",
              "        0.47352798, 0.47352798, 0.47352798, 0.42859918, 0.50206088,\n",
              "        0.50206088, 0.46845356, 0.44086326, 0.50206088, 0.50206088,\n",
              "        0.46845356, 0.39514731, 0.50431049, 0.50431049, 0.46845356,\n",
              "        0.39514731, 0.50431049, 0.50431049, 0.46845356, 0.43555373]),\n",
              " 'split3_test_precision': array([0.56267465, 0.56267465, 0.56267465, 0.32664756, 0.56267465,\n",
              "        0.56267465, 0.56267465, 0.6627794 , 0.54450378, 0.55538793,\n",
              "        0.55538793, 0.7063783 , 0.55538793, 0.55538793, 0.55538793,\n",
              "        0.32664756, 0.5561038 , 0.5561038 , 0.532778  , 0.70398551,\n",
              "        0.56169896, 0.56169896, 0.5398661 , 0.6627794 , 0.54487179,\n",
              "        0.54487179, 0.54281741, 0.68755221, 0.54487179, 0.54487179,\n",
              "        0.5398661 , 0.68755221, 0.56267465, 0.56267465, 0.56267465,\n",
              "        0.61696452, 0.56267465, 0.56267465, 0.56267465, 0.82947977,\n",
              "        0.55538793, 0.55538793, 0.55538793, 0.74757046, 0.55538793,\n",
              "        0.55538793, 0.55538793, 0.7063783 , 0.5561038 , 0.5561038 ,\n",
              "        0.532778  , 0.68755221, 0.5561038 , 0.5561038 , 0.54281741,\n",
              "        0.59299569, 0.54487179, 0.54487179, 0.532778  , 0.6964766 ,\n",
              "        0.54487179, 0.54487179, 0.532778  , 0.66421569, 0.56267465,\n",
              "        0.56267465, 0.56267465, 0.82853026, 0.56267465, 0.56267465,\n",
              "        0.56267465, 0.82947977, 0.55538793, 0.55538793, 0.55538793,\n",
              "        0.6627794 , 0.55538793, 0.55538793, 0.55538793, 0.32664756,\n",
              "        0.5561038 , 0.5561038 , 0.532778  , 0.82947977, 0.5561038 ,\n",
              "        0.5561038 , 0.54281741, 0.5599359 , 0.54487179, 0.54487179,\n",
              "        0.532778  , 0.6627794 , 0.54487179, 0.54487179, 0.532778  ,\n",
              "        0.61010406, 0.56267465, 0.56267465, 0.56267465, 0.82947977,\n",
              "        0.56267465, 0.56267465, 0.56267465, 0.66716567, 0.55538793,\n",
              "        0.55538793, 0.55538793, 0.72994186, 0.55538793, 0.55538793,\n",
              "        0.55538793, 0.82947977, 0.5561038 , 0.5561038 , 0.532778  ,\n",
              "        0.82947977, 0.5561038 , 0.5561038 , 0.54281741, 0.6627794 ,\n",
              "        0.54487179, 0.54487179, 0.532778  , 0.68755221, 0.54487179,\n",
              "        0.54487179, 0.532778  , 0.82853026, 0.56267465, 0.56267465,\n",
              "        0.56267465, 0.68185841, 0.56267465, 0.56267465, 0.56267465,\n",
              "        0.68755221, 0.55538793, 0.55538793, 0.55538793, 0.82947977,\n",
              "        0.55538793, 0.55538793, 0.55538793, 0.72994186, 0.5561038 ,\n",
              "        0.5561038 , 0.532778  , 0.63038348, 0.5561038 , 0.5561038 ,\n",
              "        0.532778  , 0.32664756, 0.54487179, 0.54487179, 0.532778  ,\n",
              "        0.32664756, 0.54487179, 0.54487179, 0.532778  , 0.68755221]),\n",
              " 'split3_test_recall': array([0.51138176, 0.51138176, 0.51138176, 0.5       , 0.51138176,\n",
              "        0.51138176, 0.51138176, 0.51214296, 0.51449906, 0.51863129,\n",
              "        0.51863129, 0.52040742, 0.51863129, 0.51863129, 0.51863129,\n",
              "        0.5       , 0.52782007, 0.52782007, 0.51205234, 0.51020371,\n",
              "        0.53001305, 0.53001305, 0.51424532, 0.51214296, 0.52511962,\n",
              "        0.52511962, 0.51618457, 0.51627519, 0.52511962, 0.52511962,\n",
              "        0.51424532, 0.51627519, 0.51138176, 0.51138176, 0.51138176,\n",
              "        0.53178918, 0.51138176, 0.51138176, 0.51138176, 0.51239669,\n",
              "        0.51863129, 0.51863129, 0.51863129, 0.51846817, 0.51863129,\n",
              "        0.51863129, 0.51863129, 0.52040742, 0.52782007, 0.52782007,\n",
              "        0.51205234, 0.51627519, 0.52782007, 0.52782007, 0.51618457,\n",
              "        0.53128172, 0.52511962, 0.52511962, 0.51205234, 0.5264789 ,\n",
              "        0.52511962, 0.52511962, 0.51205234, 0.51821444, 0.51138176,\n",
              "        0.51138176, 0.51138176, 0.50826446, 0.51138176, 0.51138176,\n",
              "        0.51138176, 0.51239669, 0.51863129, 0.51863129, 0.51863129,\n",
              "        0.51214296, 0.51863129, 0.51863129, 0.51863129, 0.5       ,\n",
              "        0.52782007, 0.52782007, 0.51205234, 0.51239669, 0.52782007,\n",
              "        0.52782007, 0.51618457, 0.51694577, 0.52511962, 0.52511962,\n",
              "        0.51205234, 0.51214296, 0.52511962, 0.52511962, 0.51205234,\n",
              "        0.52377845, 0.51138176, 0.51138176, 0.51138176, 0.51239669,\n",
              "        0.51138176, 0.51138176, 0.51138176, 0.5303574 , 0.51863129,\n",
              "        0.51863129, 0.51863129, 0.51433594, 0.51863129, 0.51863129,\n",
              "        0.51863129, 0.51239669, 0.52782007, 0.52782007, 0.51205234,\n",
              "        0.51239669, 0.52782007, 0.52782007, 0.51618457, 0.51214296,\n",
              "        0.52511962, 0.52511962, 0.51205234, 0.51627519, 0.52511962,\n",
              "        0.52511962, 0.51205234, 0.50826446, 0.51138176, 0.51138176,\n",
              "        0.51138176, 0.52234667, 0.51138176, 0.51138176, 0.51138176,\n",
              "        0.51627519, 0.51863129, 0.51863129, 0.51863129, 0.51239669,\n",
              "        0.51863129, 0.51863129, 0.51863129, 0.51433594, 0.52782007,\n",
              "        0.52782007, 0.51205234, 0.51602146, 0.52782007, 0.52782007,\n",
              "        0.51205234, 0.5       , 0.52511962, 0.52511962, 0.51205234,\n",
              "        0.5       , 0.52511962, 0.52511962, 0.51205234, 0.51627519]),\n",
              " 'split3_train_accuracy': array([0.66624204, 0.66656051, 0.66687898, 0.65477707, 0.66719745,\n",
              "        0.66751592, 0.66687898, 0.65636943, 0.6656051 , 0.66528662,\n",
              "        0.66528662, 0.6566879 , 0.6656051 , 0.66528662, 0.6656051 ,\n",
              "        0.65477707, 0.66242038, 0.66242038, 0.6611465 , 0.65732484,\n",
              "        0.66146497, 0.66146497, 0.66305732, 0.65636943, 0.67261146,\n",
              "        0.67261146, 0.66146497, 0.6566879 , 0.67165605, 0.67165605,\n",
              "        0.66305732, 0.65732484, 0.66719745, 0.66687898, 0.66687898,\n",
              "        0.63757962, 0.66656051, 0.66687898, 0.66719745, 0.65923567,\n",
              "        0.66528662, 0.66528662, 0.66656051, 0.65859873, 0.6656051 ,\n",
              "        0.66528662, 0.66528662, 0.65254777, 0.66178344, 0.66242038,\n",
              "        0.6611465 , 0.65796178, 0.66210191, 0.66210191, 0.66146497,\n",
              "        0.63503185, 0.67261146, 0.67261146, 0.6611465 , 0.65828025,\n",
              "        0.67261146, 0.67261146, 0.6611465 , 0.65923567, 0.66687898,\n",
              "        0.66719745, 0.66687898, 0.6566879 , 0.66687898, 0.66719745,\n",
              "        0.66719745, 0.65955414, 0.66528662, 0.6656051 , 0.66528662,\n",
              "        0.6566879 , 0.66528662, 0.66528662, 0.66528662, 0.65477707,\n",
              "        0.66210191, 0.66242038, 0.66146497, 0.65796178, 0.66210191,\n",
              "        0.66210191, 0.66146497, 0.63375796, 0.67261146, 0.67261146,\n",
              "        0.6611465 , 0.6566879 , 0.67261146, 0.67261146, 0.6611465 ,\n",
              "        0.6455414 , 0.66687898, 0.66592357, 0.66656051, 0.65764331,\n",
              "        0.66719745, 0.66624204, 0.66687898, 0.65286624, 0.66528662,\n",
              "        0.66528662, 0.66528662, 0.65859873, 0.66528662, 0.66528662,\n",
              "        0.66528662, 0.6589172 , 0.66210191, 0.66210191, 0.6611465 ,\n",
              "        0.65764331, 0.66242038, 0.66242038, 0.66146497, 0.65095541,\n",
              "        0.67261146, 0.67261146, 0.6611465 , 0.65732484, 0.67261146,\n",
              "        0.67261146, 0.6611465 , 0.66019108, 0.66687898, 0.66687898,\n",
              "        0.66656051, 0.65796178, 0.66687898, 0.66687898, 0.66592357,\n",
              "        0.65764331, 0.66528662, 0.66528662, 0.66528662, 0.6589172 ,\n",
              "        0.66528662, 0.66528662, 0.66528662, 0.65796178, 0.66210191,\n",
              "        0.66242038, 0.6611465 , 0.65159236, 0.66242038, 0.66242038,\n",
              "        0.66146497, 0.65509554, 0.67261146, 0.67261146, 0.66146497,\n",
              "        0.65509554, 0.67261146, 0.67261146, 0.6611465 , 0.65700637]),\n",
              " 'split3_train_balanced_accuracy': array([0.52663611, 0.5268793 , 0.52712249, 0.5       , 0.52736568,\n",
              "        0.52760887, 0.52712249, 0.50753981, 0.53530841, 0.53506522,\n",
              "        0.53506522, 0.50909138, 0.53530841, 0.53506522, 0.53530841,\n",
              "        0.5       , 0.54639647, 0.54639647, 0.53495664, 0.50739712,\n",
              "        0.54479464, 0.54479464, 0.53641578, 0.50797594, 0.56639015,\n",
              "        0.56639015, 0.53541789, 0.50930945, 0.56500639, 0.56500639,\n",
              "        0.53641578, 0.50914164, 0.52736568, 0.52734055, 0.52712249,\n",
              "        0.50387669, 0.5268793 , 0.52712249, 0.52736568, 0.5092924 ,\n",
              "        0.53528329, 0.53528329, 0.5371283 , 0.50989633, 0.53530841,\n",
              "        0.53506522, 0.53528329, 0.50702022, 0.54547396, 0.54639647,\n",
              "        0.53495664, 0.51115446, 0.54593522, 0.54593522, 0.53541789,\n",
              "        0.50411181, 0.56639015, 0.56639015, 0.53495664, 0.51030734,\n",
              "        0.56639015, 0.56639015, 0.53495664, 0.50951046, 0.52734055,\n",
              "        0.52758374, 0.52734055, 0.50298559, 0.52734055, 0.52736568,\n",
              "        0.52736568, 0.50866333, 0.53528329, 0.53530841, 0.53506522,\n",
              "        0.507783  , 0.53528329, 0.53506522, 0.53528329, 0.5       ,\n",
              "        0.54593522, 0.54639647, 0.53519983, 0.50766544, 0.54593522,\n",
              "        0.54593522, 0.53541789, 0.49725132, 0.56639015, 0.56639015,\n",
              "        0.53495664, 0.50821913, 0.56639015, 0.56639015, 0.53495664,\n",
              "        0.50276035, 0.52734055, 0.52639292, 0.5268793 , 0.50698612,\n",
              "        0.52736568, 0.52663611, 0.52712249, 0.51009825, 0.53506522,\n",
              "        0.53506522, 0.53506522, 0.50989633, 0.53506522, 0.53506522,\n",
              "        0.53506522, 0.5097034 , 0.54593522, 0.54593522, 0.53495664,\n",
              "        0.50698612, 0.54639647, 0.54639647, 0.53541789, 0.50515008,\n",
              "        0.56639015, 0.56639015, 0.53495664, 0.50914164, 0.56639015,\n",
              "        0.56639015, 0.53495664, 0.50871358, 0.52712249, 0.52712249,\n",
              "        0.5268793 , 0.51050027, 0.52712249, 0.52712249, 0.52639292,\n",
              "        0.51025708, 0.53506522, 0.53506522, 0.53506522, 0.50795889,\n",
              "        0.53506522, 0.53528329, 0.53506522, 0.50788351, 0.54593522,\n",
              "        0.54639647, 0.53495664, 0.50280163, 0.54639647, 0.54639647,\n",
              "        0.53519983, 0.50046125, 0.56639015, 0.56639015, 0.53519983,\n",
              "        0.50046125, 0.56639015, 0.56639015, 0.53495664, 0.50911651]),\n",
              " 'split3_train_f1': array([0.46426292, 0.46443783, 0.46461278, 0.39568899, 0.46478777,\n",
              "        0.46496281, 0.46461278, 0.42060585, 0.49015222, 0.48995714,\n",
              "        0.48995714, 0.42546053, 0.49015222, 0.48995714, 0.49015222,\n",
              "        0.39568899, 0.52056484, 0.52056484, 0.49529277, 0.41781754,\n",
              "        0.51802041, 0.51802041, 0.49649316, 0.42218764, 0.5522509 ,\n",
              "        0.5522509 , 0.49604111, 0.42623612, 0.55033441, 0.55033441,\n",
              "        0.49649316, 0.42418667, 0.46478777, 0.46528532, 0.46461278,\n",
              "        0.44615867, 0.46443783, 0.46461278, 0.46478777, 0.42025875,\n",
              "        0.49053666, 0.49053666, 0.49362444, 0.42396995, 0.49015222,\n",
              "        0.48995714, 0.49053666, 0.42738648, 0.51918679, 0.52056484,\n",
              "        0.49529277, 0.429902  , 0.51987622, 0.51987622, 0.49604111,\n",
              "        0.45099501, 0.5522509 , 0.5522509 , 0.49529277, 0.42618278,\n",
              "        0.5522509 , 0.5522509 , 0.49529277, 0.42106391, 0.46528532,\n",
              "        0.46546087, 0.46528532, 0.40251666, 0.46528532, 0.46478777,\n",
              "        0.46478777, 0.41714281, 0.49053666, 0.49015222, 0.48995714,\n",
              "        0.4207459 , 0.49053666, 0.48995714, 0.49053666, 0.39568899,\n",
              "        0.51987622, 0.52056484, 0.49549267, 0.41728025, 0.51987622,\n",
              "        0.51987622, 0.49604111, 0.43313968, 0.5522509 , 0.5522509 ,\n",
              "        0.49529277, 0.42232917, 0.5522509 , 0.5522509 , 0.49529277,\n",
              "        0.42779451, 0.46528532, 0.46408806, 0.46443783, 0.41551393,\n",
              "        0.46478777, 0.46426292, 0.46461278, 0.43711533, 0.48995714,\n",
              "        0.48995714, 0.48995714, 0.42396995, 0.48995714, 0.48995714,\n",
              "        0.48995714, 0.42252442, 0.51987622, 0.51987622, 0.49529277,\n",
              "        0.41551393, 0.52056484, 0.52056484, 0.49604111, 0.42438323,\n",
              "        0.5522509 , 0.5522509 , 0.49529277, 0.42418667, 0.5522509 ,\n",
              "        0.5522509 , 0.49529277, 0.41576444, 0.46461278, 0.46461278,\n",
              "        0.46443783, 0.4275924 , 0.46461278, 0.46461278, 0.46408806,\n",
              "        0.42744651, 0.48995714, 0.48995714, 0.48995714, 0.41605234,\n",
              "        0.48995714, 0.49053666, 0.48995714, 0.41809149, 0.51987622,\n",
              "        0.52056484, 0.49529277, 0.4145541 , 0.52056484, 0.52056484,\n",
              "        0.49549267, 0.39668682, 0.5522509 , 0.5522509 , 0.49549267,\n",
              "        0.39668682, 0.5522509 , 0.5522509 , 0.49529277, 0.42482577]),\n",
              " 'split3_train_precision': array([0.65397784, 0.65655519, 0.65917254, 0.32738854, 0.66183085,\n",
              "        0.6645311 , 0.65917254, 0.60270703, 0.62567315, 0.62426953,\n",
              "        0.62426953, 0.60292385, 0.62567315, 0.62426953, 0.62567315,\n",
              "        0.32738854, 0.60642476, 0.60642476, 0.60552293, 0.62670233,\n",
              "        0.60406193, 0.60406193, 0.61233218, 0.60115461, 0.62747429,\n",
              "        0.62747429, 0.60653624, 0.60226533, 0.62555953, 0.62555953,\n",
              "        0.61233218, 0.61397659, 0.66183085, 0.65805009, 0.65917254,\n",
              "        0.51404193, 0.65655519, 0.65917254, 0.66183085, 0.66701613,\n",
              "        0.62397464, 0.62397464, 0.62827178, 0.63735552, 0.62567315,\n",
              "        0.62426953, 0.62397464, 0.55890569, 0.60483654, 0.60642476,\n",
              "        0.60552293, 0.61568171, 0.60563255, 0.60563255, 0.60653624,\n",
              "        0.51326816, 0.62747429, 0.62747429, 0.60552293, 0.62637323,\n",
              "        0.62747429, 0.62747429, 0.60552293, 0.6629008 , 0.65805009,\n",
              "        0.66065815, 0.65805009, 0.76556513, 0.65805009, 0.66183085,\n",
              "        0.66183085, 0.7003341 , 0.62397464, 0.62567315, 0.62426953,\n",
              "        0.60802361, 0.62397464, 0.62426953, 0.62397464, 0.32738854,\n",
              "        0.60563255, 0.60642476, 0.60663676, 0.64493196, 0.60563255,\n",
              "        0.60563255, 0.60653624, 0.48896168, 0.62747429, 0.62747429,\n",
              "        0.60552293, 0.60606587, 0.62747429, 0.62747429, 0.60552293,\n",
              "        0.5171127 , 0.65805009, 0.65143955, 0.65655519, 0.64327122,\n",
              "        0.66183085, 0.65397784, 0.65917254, 0.56636276, 0.62426953,\n",
              "        0.62426953, 0.62426953, 0.63735552, 0.62426953, 0.62426953,\n",
              "        0.62426953, 0.6487642 , 0.60563255, 0.60563255, 0.60552293,\n",
              "        0.64327122, 0.60642476, 0.60642476, 0.60653624, 0.54370174,\n",
              "        0.62747429, 0.62747429, 0.60552293, 0.61397659, 0.62747429,\n",
              "        0.62747429, 0.60552293, 0.749374  , 0.65917254, 0.65917254,\n",
              "        0.65655519, 0.61887394, 0.65917254, 0.65917254, 0.65143955,\n",
              "        0.61437148, 0.62426953, 0.62426953, 0.62426953, 0.68404424,\n",
              "        0.62426953, 0.62397464, 0.62426953, 0.64169355, 0.60563255,\n",
              "        0.60642476, 0.60552293, 0.5349303 , 0.60642476, 0.60642476,\n",
              "        0.60663676, 0.82749283, 0.62747429, 0.62747429, 0.60663676,\n",
              "        0.82749283, 0.62747429, 0.62747429, 0.60552293, 0.60817862]),\n",
              " 'split3_train_recall': array([0.52663611, 0.5268793 , 0.52712249, 0.5       , 0.52736568,\n",
              "        0.52760887, 0.52712249, 0.50753981, 0.53530841, 0.53506522,\n",
              "        0.53506522, 0.50909138, 0.53530841, 0.53506522, 0.53530841,\n",
              "        0.5       , 0.54639647, 0.54639647, 0.53495664, 0.50739712,\n",
              "        0.54479464, 0.54479464, 0.53641578, 0.50797594, 0.56639015,\n",
              "        0.56639015, 0.53541789, 0.50930945, 0.56500639, 0.56500639,\n",
              "        0.53641578, 0.50914164, 0.52736568, 0.52734055, 0.52712249,\n",
              "        0.50387669, 0.5268793 , 0.52712249, 0.52736568, 0.5092924 ,\n",
              "        0.53528329, 0.53528329, 0.5371283 , 0.50989633, 0.53530841,\n",
              "        0.53506522, 0.53528329, 0.50702022, 0.54547396, 0.54639647,\n",
              "        0.53495664, 0.51115446, 0.54593522, 0.54593522, 0.53541789,\n",
              "        0.50411181, 0.56639015, 0.56639015, 0.53495664, 0.51030734,\n",
              "        0.56639015, 0.56639015, 0.53495664, 0.50951046, 0.52734055,\n",
              "        0.52758374, 0.52734055, 0.50298559, 0.52734055, 0.52736568,\n",
              "        0.52736568, 0.50866333, 0.53528329, 0.53530841, 0.53506522,\n",
              "        0.507783  , 0.53528329, 0.53506522, 0.53528329, 0.5       ,\n",
              "        0.54593522, 0.54639647, 0.53519983, 0.50766544, 0.54593522,\n",
              "        0.54593522, 0.53541789, 0.49725132, 0.56639015, 0.56639015,\n",
              "        0.53495664, 0.50821913, 0.56639015, 0.56639015, 0.53495664,\n",
              "        0.50276035, 0.52734055, 0.52639292, 0.5268793 , 0.50698612,\n",
              "        0.52736568, 0.52663611, 0.52712249, 0.51009825, 0.53506522,\n",
              "        0.53506522, 0.53506522, 0.50989633, 0.53506522, 0.53506522,\n",
              "        0.53506522, 0.5097034 , 0.54593522, 0.54593522, 0.53495664,\n",
              "        0.50698612, 0.54639647, 0.54639647, 0.53541789, 0.50515008,\n",
              "        0.56639015, 0.56639015, 0.53495664, 0.50914164, 0.56639015,\n",
              "        0.56639015, 0.53495664, 0.50871358, 0.52712249, 0.52712249,\n",
              "        0.5268793 , 0.51050027, 0.52712249, 0.52712249, 0.52639292,\n",
              "        0.51025708, 0.53506522, 0.53506522, 0.53506522, 0.50795889,\n",
              "        0.53506522, 0.53528329, 0.53506522, 0.50788351, 0.54593522,\n",
              "        0.54639647, 0.53495664, 0.50280163, 0.54639647, 0.54639647,\n",
              "        0.53519983, 0.50046125, 0.56639015, 0.56639015, 0.53519983,\n",
              "        0.50046125, 0.56639015, 0.56639015, 0.53495664, 0.50911651]),\n",
              " 'split4_test_accuracy': array([0.67335244, 0.67335244, 0.67335244, 0.63323782, 0.67335244,\n",
              "        0.67335244, 0.67335244, 0.63323782, 0.66475645, 0.66475645,\n",
              "        0.66475645, 0.66189112, 0.66475645, 0.66475645, 0.66475645,\n",
              "        0.65329513, 0.65616046, 0.65616046, 0.66189112, 0.63896848,\n",
              "        0.65616046, 0.65616046, 0.66189112, 0.65902579, 0.65616046,\n",
              "        0.65616046, 0.66189112, 0.65616046, 0.65902579, 0.65902579,\n",
              "        0.66189112, 0.65616046, 0.67335244, 0.67335244, 0.67335244,\n",
              "        0.64469914, 0.67335244, 0.67335244, 0.67335244, 0.63037249,\n",
              "        0.66475645, 0.66475645, 0.66475645, 0.66475645, 0.66475645,\n",
              "        0.66475645, 0.66475645, 0.63323782, 0.65616046, 0.65616046,\n",
              "        0.66189112, 0.65902579, 0.65616046, 0.65616046, 0.66189112,\n",
              "        0.65616046, 0.65902579, 0.65902579, 0.66189112, 0.66475645,\n",
              "        0.65902579, 0.65902579, 0.65902579, 0.65902579, 0.67335244,\n",
              "        0.67335244, 0.67335244, 0.65902579, 0.67335244, 0.67335244,\n",
              "        0.67335244, 0.65902579, 0.66475645, 0.66475645, 0.66475645,\n",
              "        0.6504298 , 0.66475645, 0.66475645, 0.66475645, 0.65616046,\n",
              "        0.65616046, 0.65616046, 0.66189112, 0.63610315, 0.65616046,\n",
              "        0.65616046, 0.65902579, 0.66189112, 0.65616046, 0.65616046,\n",
              "        0.66189112, 0.65329513, 0.65616046, 0.65616046, 0.66189112,\n",
              "        0.66475645, 0.67335244, 0.67335244, 0.67335244, 0.64183381,\n",
              "        0.67335244, 0.67335244, 0.67335244, 0.65329513, 0.66475645,\n",
              "        0.66475645, 0.66475645, 0.63037249, 0.66475645, 0.66475645,\n",
              "        0.66475645, 0.65329513, 0.65616046, 0.65616046, 0.65902579,\n",
              "        0.66189112, 0.65616046, 0.65616046, 0.65902579, 0.65902579,\n",
              "        0.65616046, 0.65616046, 0.66189112, 0.65902579, 0.65616046,\n",
              "        0.65616046, 0.65902579, 0.66189112, 0.67335244, 0.67335244,\n",
              "        0.67335244, 0.65616046, 0.67335244, 0.67335244, 0.67621777,\n",
              "        0.65902579, 0.66475645, 0.66475645, 0.66475645, 0.65616046,\n",
              "        0.66475645, 0.66475645, 0.66475645, 0.6504298 , 0.65616046,\n",
              "        0.65616046, 0.65902579, 0.66475645, 0.65616046, 0.65616046,\n",
              "        0.66189112, 0.65902579, 0.65616046, 0.65616046, 0.65902579,\n",
              "        0.65616046, 0.65616046, 0.65616046, 0.66189112, 0.65329513]),\n",
              " 'split4_test_balanced_accuracy': array([0.54250036, 0.54250036, 0.54250036, 0.49046687, 0.54250036,\n",
              "        0.54250036, 0.54250036, 0.50016311, 0.54173916, 0.54173916,\n",
              "        0.54173916, 0.51821444, 0.54173916, 0.54173916, 0.54173916,\n",
              "        0.5       , 0.54679571, 0.54679571, 0.54342468, 0.50648833,\n",
              "        0.54679571, 0.54679571, 0.54148543, 0.51602146, 0.5545527 ,\n",
              "        0.5545527 , 0.54342468, 0.51382848, 0.55674569, 0.55674569,\n",
              "        0.54148543, 0.50801073, 0.54250036, 0.54250036, 0.54250036,\n",
              "        0.50505655, 0.54250036, 0.54250036, 0.54250036, 0.49021314,\n",
              "        0.54173916, 0.54173916, 0.54173916, 0.52040742, 0.54173916,\n",
              "        0.54173916, 0.54173916, 0.50210236, 0.54679571, 0.54679571,\n",
              "        0.54342468, 0.50826446, 0.54679571, 0.54679571, 0.54342468,\n",
              "        0.51382848, 0.55674569, 0.55674569, 0.54342468, 0.52040742,\n",
              "        0.55674569, 0.55674569, 0.53929245, 0.51214296, 0.54250036,\n",
              "        0.54250036, 0.54250036, 0.51408221, 0.54250036, 0.54250036,\n",
              "        0.54250036, 0.51602146, 0.54173916, 0.54173916, 0.54173916,\n",
              "        0.51332101, 0.54173916, 0.54173916, 0.54173916, 0.50413223,\n",
              "        0.54679571, 0.54679571, 0.54342468, 0.5023561 , 0.54679571,\n",
              "        0.54679571, 0.53929245, 0.51627519, 0.5545527 , 0.5545527 ,\n",
              "        0.54342468, 0.51163549, 0.5545527 , 0.5545527 , 0.54342468,\n",
              "        0.52040742, 0.54250036, 0.54250036, 0.54250036, 0.50674206,\n",
              "        0.54250036, 0.54250036, 0.54250036, 0.50193925, 0.54173916,\n",
              "        0.54173916, 0.54173916, 0.49215239, 0.54173916, 0.54173916,\n",
              "        0.54173916, 0.51163549, 0.54679571, 0.54679571, 0.53929245,\n",
              "        0.51627519, 0.54679571, 0.54679571, 0.53929245, 0.51214296,\n",
              "        0.5545527 , 0.5545527 , 0.54342468, 0.51602146, 0.5545527 ,\n",
              "        0.5545527 , 0.53929245, 0.51627519, 0.54250036, 0.54250036,\n",
              "        0.54250036, 0.51576773, 0.54250036, 0.54250036, 0.54663259,\n",
              "        0.51020371, 0.54173916, 0.54173916, 0.54173916, 0.50801073,\n",
              "        0.54173916, 0.54173916, 0.54173916, 0.49780702, 0.54679571,\n",
              "        0.54679571, 0.53929245, 0.52040742, 0.54679571, 0.54679571,\n",
              "        0.54342468, 0.51602146, 0.5545527 , 0.5545527 , 0.53929245,\n",
              "        0.50801073, 0.5545527 , 0.5545527 , 0.54342468, 0.51163549]),\n",
              " 'split4_test_f1': array([0.49607356, 0.49607356, 0.49607356, 0.40891288, 0.49607356,\n",
              "        0.49607356, 0.49607356, 0.44003209, 0.5056358 , 0.5056358 ,\n",
              "        0.5056358 , 0.44228061, 0.5056358 , 0.5056358 , 0.5056358 ,\n",
              "        0.39514731, 0.52577226, 0.52577226, 0.51293878, 0.44878159,\n",
              "        0.52577226, 0.52577226, 0.50842723, 0.44086326, 0.5406686 ,\n",
              "        0.5406686 , 0.51293878, 0.43944748, 0.54273603, 0.54273603,\n",
              "        0.50842723, 0.41891442, 0.49607356, 0.49607356, 0.49607356,\n",
              "        0.43379736, 0.49607356, 0.49607356, 0.49607356, 0.41422382,\n",
              "        0.5056358 , 0.5056358 , 0.5056358 , 0.44369968, 0.5056358 ,\n",
              "        0.5056358 , 0.5056358 , 0.44570181, 0.52577226, 0.52577226,\n",
              "        0.51293878, 0.4127819 , 0.52577226, 0.52577226, 0.51293878,\n",
              "        0.43944748, 0.54273603, 0.54273603, 0.51293878, 0.44369968,\n",
              "        0.54273603, 0.54273603, 0.50655261, 0.42729291, 0.49607356,\n",
              "        0.49607356, 0.49607356, 0.43419027, 0.49607356, 0.49607356,\n",
              "        0.49607356, 0.44086326, 0.5056358 , 0.5056358 , 0.5056358 ,\n",
              "        0.44904244, 0.5056358 , 0.5056358 , 0.5056358 , 0.40403005,\n",
              "        0.52577226, 0.52577226, 0.51293878, 0.44152964, 0.52577226,\n",
              "        0.52577226, 0.50655261, 0.43555373, 0.5406686 , 0.5406686 ,\n",
              "        0.51293878, 0.43803314, 0.5406686 , 0.5406686 , 0.51293878,\n",
              "        0.44369968, 0.49607356, 0.49607356, 0.49607356, 0.44452933,\n",
              "        0.49607356, 0.49607356, 0.49607356, 0.40291269, 0.5056358 ,\n",
              "        0.5056358 , 0.5056358 , 0.42058661, 0.5056358 , 0.5056358 ,\n",
              "        0.5056358 , 0.43803314, 0.52577226, 0.52577226, 0.50655261,\n",
              "        0.43555373, 0.52577226, 0.52577226, 0.50655261, 0.42729291,\n",
              "        0.5406686 , 0.5406686 , 0.51293878, 0.44086326, 0.5406686 ,\n",
              "        0.5406686 , 0.50655261, 0.43555373, 0.49607356, 0.49607356,\n",
              "        0.49607356, 0.44585583, 0.49607356, 0.49607356, 0.5030933 ,\n",
              "        0.42016056, 0.5056358 , 0.5056358 , 0.5056358 , 0.41891442,\n",
              "        0.5056358 , 0.5056358 , 0.5056358 , 0.39409722, 0.52577226,\n",
              "        0.52577226, 0.50655261, 0.44369968, 0.52577226, 0.52577226,\n",
              "        0.51293878, 0.44086326, 0.5406686 , 0.5406686 , 0.50655261,\n",
              "        0.41891442, 0.5406686 , 0.5406686 , 0.51293878, 0.43803314]),\n",
              " 'split4_test_precision': array([0.67022358, 0.67022358, 0.67022358, 0.43978938, 0.67022358,\n",
              "        0.67022358, 0.67022358, 0.50060016, 0.62032393, 0.62032393,\n",
              "        0.62032393, 0.66421569, 0.62032393, 0.62032393, 0.62032393,\n",
              "        0.32664756, 0.59437135, 0.59437135, 0.60900819, 0.52387303,\n",
              "        0.59437135, 0.59437135, 0.60975259, 0.63038348, 0.5959332 ,\n",
              "        0.5959332 , 0.60900819, 0.60260893, 0.60136623, 0.60136623,\n",
              "        0.60975259, 0.62848837, 0.67022358, 0.67022358, 0.67022358,\n",
              "        0.52784431, 0.67022358, 0.67022358, 0.67022358, 0.44932432,\n",
              "        0.62032393, 0.62032393, 0.62032393, 0.7063783 , 0.62032393,\n",
              "        0.62032393, 0.62032393, 0.50716049, 0.59437135, 0.59437135,\n",
              "        0.60900819, 0.82853026, 0.59437135, 0.59437135, 0.60900819,\n",
              "        0.60260893, 0.60136623, 0.60136623, 0.60900819, 0.7063783 ,\n",
              "        0.60136623, 0.60136623, 0.60121382, 0.6627794 , 0.67022358,\n",
              "        0.67022358, 0.67022358, 0.64241202, 0.67022358, 0.67022358,\n",
              "        0.67022358, 0.63038348, 0.62032393, 0.62032393, 0.62032393,\n",
              "        0.56511339, 0.62032393, 0.62032393, 0.62032393, 0.82758621,\n",
              "        0.59437135, 0.59437135, 0.60900819, 0.50903531, 0.59437135,\n",
              "        0.59437135, 0.60121382, 0.68755221, 0.5959332 , 0.5959332 ,\n",
              "        0.60900819, 0.57937685, 0.5959332 , 0.5959332 , 0.60900819,\n",
              "        0.7063783 , 0.67022358, 0.67022358, 0.67022358, 0.52826748,\n",
              "        0.67022358, 0.67022358, 0.67022358, 0.57708934, 0.62032393,\n",
              "        0.62032393, 0.62032393, 0.4636623 , 0.62032393, 0.62032393,\n",
              "        0.62032393, 0.57937685, 0.59437135, 0.59437135, 0.60121382,\n",
              "        0.68755221, 0.59437135, 0.59437135, 0.60121382, 0.6627794 ,\n",
              "        0.5959332 , 0.5959332 , 0.60900819, 0.63038348, 0.5959332 ,\n",
              "        0.5959332 , 0.60121382, 0.68755221, 0.67022358, 0.67022358,\n",
              "        0.67022358, 0.59958791, 0.67022358, 0.67022358, 0.67882958,\n",
              "        0.70398551, 0.62032393, 0.62032393, 0.62032393, 0.62848837,\n",
              "        0.62032393, 0.62032393, 0.62032393, 0.32614943, 0.59437135,\n",
              "        0.59437135, 0.60121382, 0.7063783 , 0.59437135, 0.59437135,\n",
              "        0.60900819, 0.63038348, 0.5959332 , 0.5959332 , 0.60121382,\n",
              "        0.62848837, 0.5959332 , 0.5959332 , 0.60900819, 0.57937685]),\n",
              " 'split4_test_recall': array([0.54250036, 0.54250036, 0.54250036, 0.49046687, 0.54250036,\n",
              "        0.54250036, 0.54250036, 0.50016311, 0.54173916, 0.54173916,\n",
              "        0.54173916, 0.51821444, 0.54173916, 0.54173916, 0.54173916,\n",
              "        0.5       , 0.54679571, 0.54679571, 0.54342468, 0.50648833,\n",
              "        0.54679571, 0.54679571, 0.54148543, 0.51602146, 0.5545527 ,\n",
              "        0.5545527 , 0.54342468, 0.51382848, 0.55674569, 0.55674569,\n",
              "        0.54148543, 0.50801073, 0.54250036, 0.54250036, 0.54250036,\n",
              "        0.50505655, 0.54250036, 0.54250036, 0.54250036, 0.49021314,\n",
              "        0.54173916, 0.54173916, 0.54173916, 0.52040742, 0.54173916,\n",
              "        0.54173916, 0.54173916, 0.50210236, 0.54679571, 0.54679571,\n",
              "        0.54342468, 0.50826446, 0.54679571, 0.54679571, 0.54342468,\n",
              "        0.51382848, 0.55674569, 0.55674569, 0.54342468, 0.52040742,\n",
              "        0.55674569, 0.55674569, 0.53929245, 0.51214296, 0.54250036,\n",
              "        0.54250036, 0.54250036, 0.51408221, 0.54250036, 0.54250036,\n",
              "        0.54250036, 0.51602146, 0.54173916, 0.54173916, 0.54173916,\n",
              "        0.51332101, 0.54173916, 0.54173916, 0.54173916, 0.50413223,\n",
              "        0.54679571, 0.54679571, 0.54342468, 0.5023561 , 0.54679571,\n",
              "        0.54679571, 0.53929245, 0.51627519, 0.5545527 , 0.5545527 ,\n",
              "        0.54342468, 0.51163549, 0.5545527 , 0.5545527 , 0.54342468,\n",
              "        0.52040742, 0.54250036, 0.54250036, 0.54250036, 0.50674206,\n",
              "        0.54250036, 0.54250036, 0.54250036, 0.50193925, 0.54173916,\n",
              "        0.54173916, 0.54173916, 0.49215239, 0.54173916, 0.54173916,\n",
              "        0.54173916, 0.51163549, 0.54679571, 0.54679571, 0.53929245,\n",
              "        0.51627519, 0.54679571, 0.54679571, 0.53929245, 0.51214296,\n",
              "        0.5545527 , 0.5545527 , 0.54342468, 0.51602146, 0.5545527 ,\n",
              "        0.5545527 , 0.53929245, 0.51627519, 0.54250036, 0.54250036,\n",
              "        0.54250036, 0.51576773, 0.54250036, 0.54250036, 0.54663259,\n",
              "        0.51020371, 0.54173916, 0.54173916, 0.54173916, 0.50801073,\n",
              "        0.54173916, 0.54173916, 0.54173916, 0.49780702, 0.54679571,\n",
              "        0.54679571, 0.53929245, 0.52040742, 0.54679571, 0.54679571,\n",
              "        0.54342468, 0.51602146, 0.5545527 , 0.5545527 , 0.53929245,\n",
              "        0.50801073, 0.5545527 , 0.5545527 , 0.54342468, 0.51163549]),\n",
              " 'split4_train_accuracy': array([0.66496815, 0.66464968, 0.66528662, 0.6388535 , 0.66528662,\n",
              "        0.66528662, 0.66496815, 0.63726115, 0.66687898, 0.66719745,\n",
              "        0.66751592, 0.6589172 , 0.66751592, 0.66687898, 0.66624204,\n",
              "        0.65477707, 0.66210191, 0.66210191, 0.66496815, 0.63980892,\n",
              "        0.66210191, 0.66210191, 0.6656051 , 0.65382166, 0.66401274,\n",
              "        0.66401274, 0.66496815, 0.65350318, 0.66369427, 0.66369427,\n",
              "        0.6656051 , 0.65859873, 0.66528662, 0.66433121, 0.66464968,\n",
              "        0.64426752, 0.66528662, 0.66528662, 0.66464968, 0.63566879,\n",
              "        0.66719745, 0.66687898, 0.66687898, 0.65700637, 0.66624204,\n",
              "        0.66687898, 0.66624204, 0.6366242 , 0.66210191, 0.66210191,\n",
              "        0.66496815, 0.6589172 , 0.66210191, 0.66210191, 0.66496815,\n",
              "        0.65286624, 0.66401274, 0.66401274, 0.66496815, 0.65764331,\n",
              "        0.66401274, 0.66401274, 0.66496815, 0.65923567, 0.66433121,\n",
              "        0.66433121, 0.66433121, 0.65859873, 0.66464968, 0.66528662,\n",
              "        0.66496815, 0.65509554, 0.66751592, 0.66624204, 0.66751592,\n",
              "        0.6455414 , 0.66624204, 0.66687898, 0.66719745, 0.65700637,\n",
              "        0.66210191, 0.66210191, 0.66496815, 0.6388535 , 0.66210191,\n",
              "        0.66210191, 0.66496815, 0.65732484, 0.66401274, 0.66401274,\n",
              "        0.66496815, 0.6544586 , 0.66401274, 0.66401274, 0.66496815,\n",
              "        0.6566879 , 0.66464968, 0.66496815, 0.66464968, 0.64076433,\n",
              "        0.66528662, 0.66464968, 0.66464968, 0.65541401, 0.66687898,\n",
              "        0.66719745, 0.66719745, 0.63630573, 0.66751592, 0.66687898,\n",
              "        0.66687898, 0.65605096, 0.66242038, 0.66210191, 0.66496815,\n",
              "        0.65859873, 0.66210191, 0.66210191, 0.66496815, 0.65859873,\n",
              "        0.66401274, 0.66401274, 0.66496815, 0.6566879 , 0.66401274,\n",
              "        0.66401274, 0.66496815, 0.65859873, 0.66496815, 0.66433121,\n",
              "        0.66496815, 0.65732484, 0.66528662, 0.66464968, 0.66464968,\n",
              "        0.65764331, 0.66687898, 0.66687898, 0.66719745, 0.6589172 ,\n",
              "        0.66719745, 0.66719745, 0.6656051 , 0.65541401, 0.66210191,\n",
              "        0.66210191, 0.66496815, 0.65700637, 0.66210191, 0.66210191,\n",
              "        0.66496815, 0.65605096, 0.66401274, 0.66401274, 0.66496815,\n",
              "        0.65828025, 0.66401274, 0.66401274, 0.66496815, 0.65414013]),\n",
              " 'split4_train_balanced_accuracy': array([0.52588141, 0.52542015, 0.52590654, 0.49961592, 0.52590654,\n",
              "        0.52568847, 0.52544528, 0.50625027, 0.5367173 , 0.53674243,\n",
              "        0.53720368, 0.51013953, 0.53676756, 0.5367173 , 0.53535867,\n",
              "        0.5       , 0.54288232, 0.54288232, 0.53765686, 0.50863192,\n",
              "        0.54266426, 0.54266426, 0.53814324, 0.5066846 , 0.55240983,\n",
              "        0.55240983, 0.53765686, 0.50818592, 0.55194858, 0.55194858,\n",
              "        0.53814324, 0.5070615 , 0.52590654, 0.5249589 , 0.52542015,\n",
              "        0.50636693, 0.52590654, 0.52568847, 0.52542015, 0.50023691,\n",
              "        0.53674243, 0.5367173 , 0.5367173 , 0.50889845, 0.53623092,\n",
              "        0.53628118, 0.53579479, 0.50794453, 0.54288232, 0.54288232,\n",
              "        0.53765686, 0.5066505 , 0.54288232, 0.54288232, 0.53787493,\n",
              "        0.50748148, 0.55240983, 0.55240983, 0.53787493, 0.5089487 ,\n",
              "        0.55240983, 0.55240983, 0.53765686, 0.50885627, 0.5249589 ,\n",
              "        0.5249589 , 0.5249589 , 0.50967827, 0.52542015, 0.52590654,\n",
              "        0.52544528, 0.50765736, 0.53720368, 0.53623092, 0.53720368,\n",
              "        0.50995646, 0.53579479, 0.5367173 , 0.53674243, 0.50366491,\n",
              "        0.54288232, 0.54288232, 0.53765686, 0.50724816, 0.54288232,\n",
              "        0.54288232, 0.53787493, 0.50717906, 0.55240983, 0.55240983,\n",
              "        0.53787493, 0.50913356, 0.55240983, 0.55240983, 0.53787493,\n",
              "        0.50800106, 0.52542015, 0.52544528, 0.52542015, 0.50478215,\n",
              "        0.52590654, 0.52542015, 0.52542015, 0.50179477, 0.5367173 ,\n",
              "        0.53674243, 0.53674243, 0.50203167, 0.53720368, 0.5367173 ,\n",
              "        0.5367173 , 0.50904113, 0.54312551, 0.54288232, 0.53765686,\n",
              "        0.50858795, 0.54288232, 0.54288232, 0.53787493, 0.5070615 ,\n",
              "        0.55240983, 0.55240983, 0.53765686, 0.50974557, 0.55240983,\n",
              "        0.55240983, 0.53787493, 0.50793376, 0.52544528, 0.5249589 ,\n",
              "        0.52588141, 0.5117584 , 0.52590654, 0.52542015, 0.52542015,\n",
              "        0.50545968, 0.53628118, 0.5367173 , 0.53674243, 0.50752276,\n",
              "        0.53674243, 0.53674243, 0.53574454, 0.50135864, 0.54288232,\n",
              "        0.54288232, 0.53787493, 0.50759006, 0.54288232, 0.54288232,\n",
              "        0.53765686, 0.50991338, 0.55240983, 0.55240983, 0.53787493,\n",
              "        0.50725444, 0.55240983, 0.55240983, 0.53765686, 0.50910843]),\n",
              " 'split4_train_f1': array([0.464233  , 0.46338907, 0.46373848, 0.4309336 , 0.46373848,\n",
              "        0.4630663 , 0.46289213, 0.45343811, 0.4920945 , 0.49171065,\n",
              "        0.49248707, 0.4241122 , 0.49132407, 0.4920945 , 0.48937512,\n",
              "        0.39568899, 0.5131058 , 0.5131058 , 0.49714142, 0.4560647 ,\n",
              "        0.5126106 , 0.5126106 , 0.49754212, 0.42338845, 0.53171024,\n",
              "        0.53171024, 0.49714142, 0.42933657, 0.53104502, 0.53104502,\n",
              "        0.49754212, 0.41344207, 0.46373848, 0.46254391, 0.46338907,\n",
              "        0.44183055, 0.46373848, 0.4630663 , 0.46338907, 0.438725  ,\n",
              "        0.49171065, 0.4920945 , 0.4920945 , 0.42404367, 0.49170219,\n",
              "        0.49093319, 0.49054258, 0.459062  , 0.5131058 , 0.5131058 ,\n",
              "        0.49714142, 0.41106761, 0.5131058 , 0.5131058 , 0.49769595,\n",
              "        0.42828756, 0.53171024, 0.53171024, 0.49769595, 0.42275375,\n",
              "        0.53171024, 0.53171024, 0.49714142, 0.41863926, 0.46254391,\n",
              "        0.46254391, 0.46254391, 0.42317829, 0.46338907, 0.46373848,\n",
              "        0.46289213, 0.4239634 , 0.49248707, 0.49170219, 0.49248707,\n",
              "        0.45041004, 0.49054258, 0.4920945 , 0.49171065, 0.4043619 ,\n",
              "        0.5131058 , 0.5131058 , 0.49714142, 0.45370493, 0.5131058 ,\n",
              "        0.5131058 , 0.49769595, 0.41700786, 0.53171024, 0.53171024,\n",
              "        0.49769595, 0.43053559, 0.53171024, 0.53171024, 0.49769595,\n",
              "        0.42153901, 0.46338907, 0.46289213, 0.46338907, 0.44333066,\n",
              "        0.46373848, 0.46338907, 0.46338907, 0.40116514, 0.4920945 ,\n",
              "        0.49171065, 0.49171065, 0.44293548, 0.49248707, 0.4920945 ,\n",
              "        0.4920945 , 0.42671709, 0.51331753, 0.5131058 , 0.49714142,\n",
              "        0.41917512, 0.5131058 , 0.5131058 , 0.49769595, 0.41344207,\n",
              "        0.53171024, 0.53171024, 0.49714142, 0.42777873, 0.53171024,\n",
              "        0.53171024, 0.49769595, 0.41673673, 0.46289213, 0.46254391,\n",
              "        0.464233  , 0.43339207, 0.46373848, 0.46338907, 0.46338907,\n",
              "        0.40970993, 0.49093319, 0.4920945 , 0.49171065, 0.41440343,\n",
              "        0.49171065, 0.49171065, 0.49131015, 0.3994308 , 0.5131058 ,\n",
              "        0.5131058 , 0.49769595, 0.41928926, 0.5131058 , 0.5131058 ,\n",
              "        0.49714142, 0.42977403, 0.53171024, 0.53171024, 0.49769595,\n",
              "        0.41496186, 0.53171024, 0.53171024, 0.49714142, 0.43113323]),\n",
              " 'split4_train_precision': array([0.64320116, 0.64165944, 0.6464766 , 0.49818319, 0.6464766 ,\n",
              "        0.64739753, 0.64492717, 0.52040502, 0.63068781, 0.63250078,\n",
              "        0.63357692, 0.64344385, 0.63436615, 0.63068781, 0.62921739,\n",
              "        0.32738854, 0.60620303, 0.60620303, 0.61966931, 0.52862887,\n",
              "        0.60625428, 0.60625428, 0.6221408 , 0.56837113, 0.60963469,\n",
              "        0.60963469, 0.61966931, 0.56792863, 0.60891177, 0.60891177,\n",
              "        0.6221408 , 0.69438269, 0.6464766 , 0.64009611, 0.64165944,\n",
              "        0.52890756, 0.6464766 , 0.64739753, 0.64165944, 0.50091504,\n",
              "        0.63250078, 0.63068781, 0.63068781, 0.60909951, 0.62784998,\n",
              "        0.63141557, 0.6285184 , 0.52389696, 0.60620303, 0.60620303,\n",
              "        0.61966931, 0.74995362, 0.60620303, 0.60620303, 0.61945816,\n",
              "        0.56208299, 0.60963469, 0.60963469, 0.61945816, 0.62189889,\n",
              "        0.60963469, 0.60963469, 0.61966931, 0.67663588, 0.64009611,\n",
              "        0.64009611, 0.64009611, 0.63961165, 0.64165944, 0.6464766 ,\n",
              "        0.64492717, 0.58288852, 0.63357692, 0.62784998, 0.63357692,\n",
              "        0.54059084, 0.6285184 , 0.63068781, 0.63250078, 0.73731079,\n",
              "        0.60620303, 0.60620303, 0.61966931, 0.52433066, 0.60620303,\n",
              "        0.60620303, 0.61945816, 0.62903226, 0.60963469, 0.60963469,\n",
              "        0.61945816, 0.57663839, 0.60963469, 0.60963469, 0.61945816,\n",
              "        0.60700655, 0.64165944, 0.64492717, 0.64165944, 0.51939779,\n",
              "        0.6464766 , 0.64165944, 0.64165944, 0.62779553, 0.63068781,\n",
              "        0.63250078, 0.63250078, 0.50749168, 0.63357692, 0.63068781,\n",
              "        0.63068781, 0.59376454, 0.60710322, 0.60620303, 0.61966931,\n",
              "        0.65435484, 0.60620303, 0.60620303, 0.61945816, 0.69438269,\n",
              "        0.60963469, 0.60963469, 0.61966931, 0.60107027, 0.60963469,\n",
              "        0.60963469, 0.61945816, 0.66743684, 0.64492717, 0.64009611,\n",
              "        0.64320116, 0.60450797, 0.6464766 , 0.64165944, 0.64165944,\n",
              "        0.68577383, 0.63141557, 0.63068781, 0.63250078, 0.69947413,\n",
              "        0.63250078, 0.63250078, 0.62506201, 0.66102957, 0.60620303,\n",
              "        0.60620303, 0.61945816, 0.61636434, 0.60620303, 0.60620303,\n",
              "        0.61966931, 0.59250699, 0.60963469, 0.60963469, 0.61945816,\n",
              "        0.66775439, 0.60963469, 0.60963469, 0.61966931, 0.57395264]),\n",
              " 'split4_train_recall': array([0.52588141, 0.52542015, 0.52590654, 0.49961592, 0.52590654,\n",
              "        0.52568847, 0.52544528, 0.50625027, 0.5367173 , 0.53674243,\n",
              "        0.53720368, 0.51013953, 0.53676756, 0.5367173 , 0.53535867,\n",
              "        0.5       , 0.54288232, 0.54288232, 0.53765686, 0.50863192,\n",
              "        0.54266426, 0.54266426, 0.53814324, 0.5066846 , 0.55240983,\n",
              "        0.55240983, 0.53765686, 0.50818592, 0.55194858, 0.55194858,\n",
              "        0.53814324, 0.5070615 , 0.52590654, 0.5249589 , 0.52542015,\n",
              "        0.50636693, 0.52590654, 0.52568847, 0.52542015, 0.50023691,\n",
              "        0.53674243, 0.5367173 , 0.5367173 , 0.50889845, 0.53623092,\n",
              "        0.53628118, 0.53579479, 0.50794453, 0.54288232, 0.54288232,\n",
              "        0.53765686, 0.5066505 , 0.54288232, 0.54288232, 0.53787493,\n",
              "        0.50748148, 0.55240983, 0.55240983, 0.53787493, 0.5089487 ,\n",
              "        0.55240983, 0.55240983, 0.53765686, 0.50885627, 0.5249589 ,\n",
              "        0.5249589 , 0.5249589 , 0.50967827, 0.52542015, 0.52590654,\n",
              "        0.52544528, 0.50765736, 0.53720368, 0.53623092, 0.53720368,\n",
              "        0.50995646, 0.53579479, 0.5367173 , 0.53674243, 0.50366491,\n",
              "        0.54288232, 0.54288232, 0.53765686, 0.50724816, 0.54288232,\n",
              "        0.54288232, 0.53787493, 0.50717906, 0.55240983, 0.55240983,\n",
              "        0.53787493, 0.50913356, 0.55240983, 0.55240983, 0.53787493,\n",
              "        0.50800106, 0.52542015, 0.52544528, 0.52542015, 0.50478215,\n",
              "        0.52590654, 0.52542015, 0.52542015, 0.50179477, 0.5367173 ,\n",
              "        0.53674243, 0.53674243, 0.50203167, 0.53720368, 0.5367173 ,\n",
              "        0.5367173 , 0.50904113, 0.54312551, 0.54288232, 0.53765686,\n",
              "        0.50858795, 0.54288232, 0.54288232, 0.53787493, 0.5070615 ,\n",
              "        0.55240983, 0.55240983, 0.53765686, 0.50974557, 0.55240983,\n",
              "        0.55240983, 0.53787493, 0.50793376, 0.52544528, 0.5249589 ,\n",
              "        0.52588141, 0.5117584 , 0.52590654, 0.52542015, 0.52542015,\n",
              "        0.50545968, 0.53628118, 0.5367173 , 0.53674243, 0.50752276,\n",
              "        0.53674243, 0.53674243, 0.53574454, 0.50135864, 0.54288232,\n",
              "        0.54288232, 0.53787493, 0.50759006, 0.54288232, 0.54288232,\n",
              "        0.53765686, 0.50991338, 0.55240983, 0.55240983, 0.53787493,\n",
              "        0.50725444, 0.55240983, 0.55240983, 0.53765686, 0.50910843]),\n",
              " 'split5_test_accuracy': array([0.65616046, 0.65616046, 0.65616046, 0.65902579, 0.65616046,\n",
              "        0.65616046, 0.65616046, 0.66475645, 0.67048711, 0.67048711,\n",
              "        0.67048711, 0.65616046, 0.67048711, 0.67048711, 0.67048711,\n",
              "        0.65616046, 0.66475645, 0.66475645, 0.66475645, 0.66475645,\n",
              "        0.66475645, 0.66475645, 0.66475645, 0.63610315, 0.67621777,\n",
              "        0.67621777, 0.66189112, 0.64183381, 0.67621777, 0.67621777,\n",
              "        0.66475645, 0.66189112, 0.65616046, 0.65616046, 0.65616046,\n",
              "        0.67048711, 0.65616046, 0.65616046, 0.65616046, 0.66762178,\n",
              "        0.67048711, 0.67048711, 0.67048711, 0.64183381, 0.67048711,\n",
              "        0.67048711, 0.67048711, 0.66762178, 0.66475645, 0.66475645,\n",
              "        0.66189112, 0.67048711, 0.66475645, 0.66475645, 0.66189112,\n",
              "        0.66762178, 0.67621777, 0.67621777, 0.66475645, 0.67335244,\n",
              "        0.67621777, 0.67621777, 0.66475645, 0.66762178, 0.65616046,\n",
              "        0.65616046, 0.65616046, 0.64183381, 0.65616046, 0.65616046,\n",
              "        0.65616046, 0.65616046, 0.67048711, 0.67048711, 0.67048711,\n",
              "        0.66762178, 0.67048711, 0.67048711, 0.67048711, 0.66762178,\n",
              "        0.66475645, 0.66475645, 0.66189112, 0.67048711, 0.66475645,\n",
              "        0.66475645, 0.66475645, 0.66762178, 0.67621777, 0.67621777,\n",
              "        0.66189112, 0.63610315, 0.67621777, 0.67621777, 0.66475645,\n",
              "        0.63323782, 0.65616046, 0.65616046, 0.65616046, 0.66762178,\n",
              "        0.65616046, 0.65616046, 0.65616046, 0.65616046, 0.67048711,\n",
              "        0.67048711, 0.67048711, 0.66475645, 0.67048711, 0.67048711,\n",
              "        0.67048711, 0.66189112, 0.66475645, 0.66475645, 0.66189112,\n",
              "        0.63896848, 0.66475645, 0.66475645, 0.66189112, 0.67048711,\n",
              "        0.67621777, 0.67621777, 0.66189112, 0.65616046, 0.67621777,\n",
              "        0.67621777, 0.66475645, 0.66762178, 0.65616046, 0.65616046,\n",
              "        0.65616046, 0.66189112, 0.65616046, 0.65616046, 0.65616046,\n",
              "        0.66762178, 0.67048711, 0.67048711, 0.67048711, 0.65616046,\n",
              "        0.67048711, 0.67048711, 0.67048711, 0.66762178, 0.66475645,\n",
              "        0.66475645, 0.66189112, 0.66189112, 0.66475645, 0.66475645,\n",
              "        0.66189112, 0.66762178, 0.67621777, 0.67621777, 0.66189112,\n",
              "        0.67048711, 0.67621777, 0.67621777, 0.66189112, 0.67048711]),\n",
              " 'split5_test_balanced_accuracy': array([0.51784934, 0.51784934, 0.51784934, 0.50614993, 0.51784934,\n",
              "        0.51784934, 0.51784934, 0.5125    , 0.5426492 , 0.5426492 ,\n",
              "        0.5426492 , 0.5       , 0.5426492 , 0.5426492 , 0.5426492 ,\n",
              "        0.5       , 0.54224891, 0.54224891, 0.53828239, 0.5125    ,\n",
              "        0.54224891, 0.54224891, 0.53828239, 0.49661572, 0.5628821 ,\n",
              "        0.5628821 , 0.53609898, 0.49503275, 0.5628821 , 0.5628821 ,\n",
              "        0.53828239, 0.51626638, 0.51784934, 0.51784934, 0.51784934,\n",
              "        0.52083333, 0.51784934, 0.51784934, 0.51784934, 0.51666667,\n",
              "        0.5426492 , 0.5426492 , 0.5426492 , 0.50494905, 0.5426492 ,\n",
              "        0.5426492 , 0.5426492 , 0.51666667, 0.54224891, 0.54224891,\n",
              "        0.53609898, 0.52083333, 0.54224891, 0.54224891, 0.53609898,\n",
              "        0.51666667, 0.5628821 , 0.5628821 , 0.53828239, 0.525     ,\n",
              "        0.5628821 , 0.5628821 , 0.53828239, 0.52063319, 0.51784934,\n",
              "        0.51784934, 0.51784934, 0.50494905, 0.51784934, 0.51784934,\n",
              "        0.51784934, 0.5       , 0.5426492 , 0.5426492 , 0.5426492 ,\n",
              "        0.51864993, 0.5426492 , 0.5426492 , 0.5426492 , 0.51666667,\n",
              "        0.54224891, 0.54224891, 0.53609898, 0.52281659, 0.54224891,\n",
              "        0.54224891, 0.53828239, 0.52063319, 0.5628821 , 0.5628821 ,\n",
              "        0.53609898, 0.49661572, 0.5628821 , 0.5628821 , 0.53828239,\n",
              "        0.49244905, 0.51784934, 0.51784934, 0.51784934, 0.51666667,\n",
              "        0.51784934, 0.51784934, 0.51784934, 0.5       , 0.5426492 ,\n",
              "        0.5426492 , 0.5426492 , 0.5125    , 0.5426492 , 0.5426492 ,\n",
              "        0.5426492 , 0.50833333, 0.54224891, 0.54224891, 0.53609898,\n",
              "        0.49284934, 0.54224891, 0.54224891, 0.53609898, 0.52281659,\n",
              "        0.5628821 , 0.5628821 , 0.53609898, 0.5       , 0.5628821 ,\n",
              "        0.5628821 , 0.53828239, 0.51666667, 0.51784934, 0.51784934,\n",
              "        0.51784934, 0.5202329 , 0.51784934, 0.51784934, 0.51784934,\n",
              "        0.51666667, 0.5426492 , 0.5426492 , 0.5426492 , 0.5       ,\n",
              "        0.5426492 , 0.5426492 , 0.5426492 , 0.51864993, 0.54224891,\n",
              "        0.54224891, 0.53609898, 0.50833333, 0.54224891, 0.54224891,\n",
              "        0.53609898, 0.52063319, 0.5628821 , 0.5628821 , 0.53609898,\n",
              "        0.52281659, 0.5628821 , 0.5628821 , 0.53609898, 0.52083333]),\n",
              " 'split5_test_f1': array([0.45807453, 0.45807453, 0.45807453, 0.4127819 , 0.45807453,\n",
              "        0.45807453, 0.45807453, 0.42265111, 0.50448782, 0.50448782,\n",
              "        0.50448782, 0.39619377, 0.50448782, 0.50448782, 0.50448782,\n",
              "        0.39619377, 0.51030761, 0.51030761, 0.50082517, 0.42265111,\n",
              "        0.51030761, 0.51030761, 0.50082517, 0.42956976, 0.54385417,\n",
              "        0.54385417, 0.49900243, 0.41267149, 0.54385417, 0.54385417,\n",
              "        0.50082517, 0.44228061, 0.45807453, 0.45807453, 0.45807453,\n",
              "        0.43965096, 0.45807453, 0.45807453, 0.45807453, 0.43121277,\n",
              "        0.50448782, 0.50448782, 0.50448782, 0.44452933, 0.50448782,\n",
              "        0.50448782, 0.50448782, 0.43121277, 0.51030761, 0.51030761,\n",
              "        0.49900243, 0.43965096, 0.51030761, 0.51030761, 0.49900243,\n",
              "        0.43121277, 0.54385417, 0.54385417, 0.50082517, 0.4479687 ,\n",
              "        0.54385417, 0.54385417, 0.50082517, 0.44512061, 0.45807453,\n",
              "        0.45807453, 0.45807453, 0.44452933, 0.45807453, 0.45807453,\n",
              "        0.45807453, 0.39619377, 0.50448782, 0.50448782, 0.50448782,\n",
              "        0.43828394, 0.50448782, 0.50448782, 0.50448782, 0.43121277,\n",
              "        0.51030761, 0.51030761, 0.49900243, 0.44654357, 0.51030761,\n",
              "        0.51030761, 0.50082517, 0.44512061, 0.54385417, 0.54385417,\n",
              "        0.49900243, 0.42956976, 0.54385417, 0.54385417, 0.50082517,\n",
              "        0.42194617, 0.45807453, 0.45807453, 0.45807453, 0.43121277,\n",
              "        0.45807453, 0.45807453, 0.45807453, 0.39619377, 0.50448782,\n",
              "        0.50448782, 0.50448782, 0.42265111, 0.50448782, 0.50448782,\n",
              "        0.50448782, 0.41396289, 0.51030761, 0.51030761, 0.49900243,\n",
              "        0.41141985, 0.51030761, 0.51030761, 0.49900243, 0.44654357,\n",
              "        0.54385417, 0.54385417, 0.49900243, 0.39619377, 0.54385417,\n",
              "        0.54385417, 0.50082517, 0.43121277, 0.45807453, 0.45807453,\n",
              "        0.45807453, 0.45509156, 0.45807453, 0.45807453, 0.45807453,\n",
              "        0.43121277, 0.50448782, 0.50448782, 0.50448782, 0.39619377,\n",
              "        0.50448782, 0.50448782, 0.50448782, 0.43828394, 0.51030761,\n",
              "        0.51030761, 0.49900243, 0.41396289, 0.51030761, 0.51030761,\n",
              "        0.49900243, 0.44512061, 0.54385417, 0.54385417, 0.49900243,\n",
              "        0.44654357, 0.54385417, 0.54385417, 0.49900243, 0.43965096]),\n",
              " 'split5_test_precision': array([0.58232628, 0.58232628, 0.58232628, 0.6628131 , 0.58232628,\n",
              "        0.58232628, 0.58232628, 0.83092486, 0.63480561, 0.63480561,\n",
              "        0.63480561, 0.32808023, 0.63480561, 0.63480561, 0.63480561,\n",
              "        0.32808023, 0.61133487, 0.61133487, 0.61336207, 0.83092486,\n",
              "        0.61133487, 0.61133487, 0.61336207, 0.48516746, 0.63683877,\n",
              "        0.63683877, 0.60365726, 0.46328671, 0.63683877, 0.63683877,\n",
              "        0.61336207, 0.63185841, 0.58232628, 0.58232628, 0.58232628,\n",
              "        0.83284884, 0.58232628, 0.58232628, 0.58232628, 0.83188406,\n",
              "        0.63480561, 0.63480561, 0.63480561, 0.51974448, 0.63480561,\n",
              "        0.63480561, 0.63480561, 0.83188406, 0.61133487, 0.61133487,\n",
              "        0.60365726, 0.83284884, 0.61133487, 0.61133487, 0.60365726,\n",
              "        0.83188406, 0.63683877, 0.63683877, 0.61336207, 0.83381924,\n",
              "        0.63683877, 0.63683877, 0.61336207, 0.70784457, 0.58232628,\n",
              "        0.58232628, 0.58232628, 0.51974448, 0.58232628, 0.58232628,\n",
              "        0.58232628, 0.32808023, 0.63480561, 0.63480561, 0.63480561,\n",
              "        0.74902818, 0.63480561, 0.63480561, 0.63480561, 0.83188406,\n",
              "        0.61133487, 0.61133487, 0.60365726, 0.76190476, 0.61133487,\n",
              "        0.61133487, 0.61336207, 0.70784457, 0.63683877, 0.63683877,\n",
              "        0.60365726, 0.48516746, 0.63683877, 0.63683877, 0.61336207,\n",
              "        0.46517288, 0.58232628, 0.58232628, 0.58232628, 0.83188406,\n",
              "        0.58232628, 0.58232628, 0.58232628, 0.32808023, 0.63480561,\n",
              "        0.63480561, 0.63480561, 0.83092486, 0.63480561, 0.63480561,\n",
              "        0.63480561, 0.82997118, 0.61133487, 0.61133487, 0.60365726,\n",
              "        0.4514095 , 0.61133487, 0.61133487, 0.60365726, 0.76190476,\n",
              "        0.63683877, 0.63683877, 0.60365726, 0.32808023, 0.63683877,\n",
              "        0.63683877, 0.61336207, 0.83188406, 0.58232628, 0.58232628,\n",
              "        0.58232628, 0.61855011, 0.58232628, 0.58232628, 0.58232628,\n",
              "        0.83188406, 0.63480561, 0.63480561, 0.63480561, 0.32808023,\n",
              "        0.63480561, 0.63480561, 0.63480561, 0.74902818, 0.61133487,\n",
              "        0.61133487, 0.60365726, 0.82997118, 0.61133487, 0.61133487,\n",
              "        0.60365726, 0.70784457, 0.63683877, 0.63683877, 0.60365726,\n",
              "        0.76190476, 0.63683877, 0.63683877, 0.60365726, 0.83284884]),\n",
              " 'split5_test_recall': array([0.51784934, 0.51784934, 0.51784934, 0.50614993, 0.51784934,\n",
              "        0.51784934, 0.51784934, 0.5125    , 0.5426492 , 0.5426492 ,\n",
              "        0.5426492 , 0.5       , 0.5426492 , 0.5426492 , 0.5426492 ,\n",
              "        0.5       , 0.54224891, 0.54224891, 0.53828239, 0.5125    ,\n",
              "        0.54224891, 0.54224891, 0.53828239, 0.49661572, 0.5628821 ,\n",
              "        0.5628821 , 0.53609898, 0.49503275, 0.5628821 , 0.5628821 ,\n",
              "        0.53828239, 0.51626638, 0.51784934, 0.51784934, 0.51784934,\n",
              "        0.52083333, 0.51784934, 0.51784934, 0.51784934, 0.51666667,\n",
              "        0.5426492 , 0.5426492 , 0.5426492 , 0.50494905, 0.5426492 ,\n",
              "        0.5426492 , 0.5426492 , 0.51666667, 0.54224891, 0.54224891,\n",
              "        0.53609898, 0.52083333, 0.54224891, 0.54224891, 0.53609898,\n",
              "        0.51666667, 0.5628821 , 0.5628821 , 0.53828239, 0.525     ,\n",
              "        0.5628821 , 0.5628821 , 0.53828239, 0.52063319, 0.51784934,\n",
              "        0.51784934, 0.51784934, 0.50494905, 0.51784934, 0.51784934,\n",
              "        0.51784934, 0.5       , 0.5426492 , 0.5426492 , 0.5426492 ,\n",
              "        0.51864993, 0.5426492 , 0.5426492 , 0.5426492 , 0.51666667,\n",
              "        0.54224891, 0.54224891, 0.53609898, 0.52281659, 0.54224891,\n",
              "        0.54224891, 0.53828239, 0.52063319, 0.5628821 , 0.5628821 ,\n",
              "        0.53609898, 0.49661572, 0.5628821 , 0.5628821 , 0.53828239,\n",
              "        0.49244905, 0.51784934, 0.51784934, 0.51784934, 0.51666667,\n",
              "        0.51784934, 0.51784934, 0.51784934, 0.5       , 0.5426492 ,\n",
              "        0.5426492 , 0.5426492 , 0.5125    , 0.5426492 , 0.5426492 ,\n",
              "        0.5426492 , 0.50833333, 0.54224891, 0.54224891, 0.53609898,\n",
              "        0.49284934, 0.54224891, 0.54224891, 0.53609898, 0.52281659,\n",
              "        0.5628821 , 0.5628821 , 0.53609898, 0.5       , 0.5628821 ,\n",
              "        0.5628821 , 0.53828239, 0.51666667, 0.51784934, 0.51784934,\n",
              "        0.51784934, 0.5202329 , 0.51784934, 0.51784934, 0.51784934,\n",
              "        0.51666667, 0.5426492 , 0.5426492 , 0.5426492 , 0.5       ,\n",
              "        0.5426492 , 0.5426492 , 0.5426492 , 0.51864993, 0.54224891,\n",
              "        0.54224891, 0.53609898, 0.50833333, 0.54224891, 0.54224891,\n",
              "        0.53609898, 0.52063319, 0.5628821 , 0.5628821 , 0.53609898,\n",
              "        0.52281659, 0.5628821 , 0.5628821 , 0.53609898, 0.52083333]),\n",
              " 'split5_train_accuracy': array([0.66815287, 0.66815287, 0.66815287, 0.65764331, 0.66815287,\n",
              "        0.66815287, 0.66910828, 0.65828025, 0.66624204, 0.66656051,\n",
              "        0.66624204, 0.65509554, 0.66624204, 0.66624204, 0.66624204,\n",
              "        0.6544586 , 0.65828025, 0.65828025, 0.65987261, 0.65796178,\n",
              "        0.65828025, 0.65828025, 0.65987261, 0.64012739, 0.66369427,\n",
              "        0.66369427, 0.65828025, 0.64617834, 0.66273885, 0.66273885,\n",
              "        0.65987261, 0.6544586 , 0.66815287, 0.66815287, 0.66815287,\n",
              "        0.65732484, 0.66815287, 0.66815287, 0.66815287, 0.65796178,\n",
              "        0.66624204, 0.66624204, 0.66624204, 0.63917197, 0.66624204,\n",
              "        0.66624204, 0.66656051, 0.65732484, 0.65828025, 0.65828025,\n",
              "        0.65828025, 0.65828025, 0.65828025, 0.65828025, 0.65828025,\n",
              "        0.6589172 , 0.66369427, 0.66369427, 0.65987261, 0.65764331,\n",
              "        0.66369427, 0.66369427, 0.66019108, 0.65732484, 0.66847134,\n",
              "        0.66910828, 0.66815287, 0.63821656, 0.66815287, 0.66815287,\n",
              "        0.66815287, 0.65541401, 0.66656051, 0.66656051, 0.66656051,\n",
              "        0.65605096, 0.66624204, 0.66624204, 0.66624204, 0.65764331,\n",
              "        0.65828025, 0.65828025, 0.65828025, 0.65541401, 0.65828025,\n",
              "        0.65828025, 0.65987261, 0.6544586 , 0.66369427, 0.66369427,\n",
              "        0.65828025, 0.6388535 , 0.66369427, 0.66369427, 0.65987261,\n",
              "        0.63980892, 0.66815287, 0.66815287, 0.66910828, 0.6589172 ,\n",
              "        0.66815287, 0.66815287, 0.66815287, 0.6544586 , 0.66624204,\n",
              "        0.66624204, 0.66624204, 0.65987261, 0.66656051, 0.66624204,\n",
              "        0.66624204, 0.65828025, 0.65828025, 0.65828025, 0.65828025,\n",
              "        0.64140127, 0.65828025, 0.65828025, 0.65859873, 0.65318471,\n",
              "        0.66369427, 0.66369427, 0.65859873, 0.65636943, 0.66369427,\n",
              "        0.66369427, 0.65987261, 0.65732484, 0.66815287, 0.66815287,\n",
              "        0.66815287, 0.64936306, 0.66847134, 0.66815287, 0.66815287,\n",
              "        0.65796178, 0.66624204, 0.66624204, 0.66624204, 0.65732484,\n",
              "        0.66624204, 0.66624204, 0.66624204, 0.65605096, 0.65828025,\n",
              "        0.65828025, 0.65828025, 0.6566879 , 0.65828025, 0.65828025,\n",
              "        0.65859873, 0.65031847, 0.66369427, 0.66369427, 0.65828025,\n",
              "        0.65159236, 0.66369427, 0.66369427, 0.65828025, 0.65700637]),\n",
              " 'split5_train_balanced_accuracy': array([0.53025665, 0.53025665, 0.53025665, 0.50808862, 0.53025665,\n",
              "        0.53025665, 0.53163914, 0.50640004, 0.53815018, 0.53861101,\n",
              "        0.53815018, 0.5013567 , 0.53815018, 0.53815018, 0.53815018,\n",
              "        0.5       , 0.54337852, 0.54337852, 0.53458912, 0.50724433,\n",
              "        0.54337852, 0.54337852, 0.53458912, 0.50732282, 0.55665063,\n",
              "        0.55665063, 0.53337258, 0.50585511, 0.5548331 , 0.5548331 ,\n",
              "        0.53458912, 0.51087602, 0.53025665, 0.53025665, 0.53025665,\n",
              "        0.51002052, 0.53025665, 0.53025665, 0.53025665, 0.50789689,\n",
              "        0.53815018, 0.53815018, 0.53815018, 0.51116082, 0.53815018,\n",
              "        0.53815018, 0.53861101, 0.50697523, 0.54337852, 0.54337852,\n",
              "        0.53337258, 0.50879276, 0.54337852, 0.54337852, 0.53337258,\n",
              "        0.50732169, 0.55665063, 0.55665063, 0.53458912, 0.50852366,\n",
              "        0.55665063, 0.55665063, 0.53504995, 0.51306581, 0.530935  ,\n",
              "        0.53163914, 0.53025665, 0.50847321, 0.53025665, 0.53025665,\n",
              "        0.53025665, 0.50225257, 0.53861101, 0.53861101, 0.53861101,\n",
              "        0.5092648 , 0.53815018, 0.53815018, 0.53815018, 0.50874118,\n",
              "        0.54337852, 0.54337852, 0.53337258, 0.50856066, 0.54337852,\n",
              "        0.54337852, 0.53458912, 0.51087602, 0.55665063, 0.55665063,\n",
              "        0.53337258, 0.50591454, 0.55665063, 0.55665063, 0.53458912,\n",
              "        0.50686199, 0.53025665, 0.53025665, 0.53163914, 0.50797426,\n",
              "        0.53025665, 0.53025665, 0.53025665, 0.5       , 0.53815018,\n",
              "        0.53815018, 0.53815018, 0.50848666, 0.53861101, 0.53815018,\n",
              "        0.53815018, 0.50596499, 0.54337852, 0.54337852, 0.53337258,\n",
              "        0.50394564, 0.54337852, 0.54337852, 0.53383341, 0.50881519,\n",
              "        0.55665063, 0.55665063, 0.53383341, 0.5029825 , 0.55665063,\n",
              "        0.55665063, 0.53458912, 0.50697523, 0.53025665, 0.53025665,\n",
              "        0.53025665, 0.5145963 , 0.53071748, 0.53025665, 0.53025665,\n",
              "        0.50680929, 0.53815018, 0.53815018, 0.53815018, 0.50436499,\n",
              "        0.53815018, 0.53815018, 0.53815018, 0.50730712, 0.54337852,\n",
              "        0.54337852, 0.53337258, 0.50344333, 0.54337852, 0.54337852,\n",
              "        0.53383341, 0.50423268, 0.55665063, 0.55665063, 0.53337258,\n",
              "        0.50651104, 0.55665063, 0.55665063, 0.53337258, 0.507602  ]),\n",
              " 'split5_train_f1': array([0.47130052, 0.47130052, 0.47130052, 0.41876193, 0.47130052,\n",
              "        0.47130052, 0.47379941, 0.41080814, 0.49626368, 0.49702514,\n",
              "        0.49626368, 0.39931099, 0.49626368, 0.49626368, 0.49626368,\n",
              "        0.39557267, 0.51775799, 0.51775799, 0.49558506, 0.41482805,\n",
              "        0.51775799, 0.51775799, 0.49558506, 0.45130668, 0.5400554 ,\n",
              "        0.5400554 , 0.49458397, 0.43594131, 0.53728271, 0.53728271,\n",
              "        0.49558506, 0.43571715, 0.47130052, 0.47130052, 0.47130052,\n",
              "        0.42652646, 0.47130052, 0.47130052, 0.47130052, 0.41728025,\n",
              "        0.49626368, 0.49626368, 0.49626368, 0.46343737, 0.49626368,\n",
              "        0.49626368, 0.49702514, 0.41537929, 0.51775799, 0.51775799,\n",
              "        0.49458397, 0.4198433 , 0.51775799, 0.51775799, 0.49458397,\n",
              "        0.41274189, 0.5400554 , 0.5400554 , 0.49558506, 0.42036765,\n",
              "        0.5400554 , 0.5400554 , 0.4963288 , 0.43710948, 0.47278651,\n",
              "        0.47379941, 0.47130052, 0.45758803, 0.47130052, 0.47130052,\n",
              "        0.47130052, 0.40202729, 0.49702514, 0.49702514, 0.49702514,\n",
              "        0.42671709, 0.49626368, 0.49626368, 0.49626368, 0.421166  ,\n",
              "        0.51775799, 0.51775799, 0.49458397, 0.42565546, 0.51775799,\n",
              "        0.51775799, 0.49558506, 0.43571715, 0.5400554 , 0.5400554 ,\n",
              "        0.49458397, 0.4493644 , 0.5400554 , 0.5400554 , 0.49558506,\n",
              "        0.45050798, 0.47130052, 0.47130052, 0.47379941, 0.41522945,\n",
              "        0.47130052, 0.47130052, 0.47130052, 0.39557267, 0.49626368,\n",
              "        0.49626368, 0.49626368, 0.41480228, 0.49702514, 0.49626368,\n",
              "        0.49626368, 0.40912429, 0.51775799, 0.51775799, 0.49458397,\n",
              "        0.43901917, 0.51775799, 0.51775799, 0.4953255 , 0.43142416,\n",
              "        0.5400554 , 0.5400554 , 0.4953255 , 0.40239437, 0.5400554 ,\n",
              "        0.5400554 , 0.49558506, 0.41537929, 0.47130052, 0.47130052,\n",
              "        0.47130052, 0.4569743 , 0.47213468, 0.47130052, 0.47130052,\n",
              "        0.41317766, 0.49626368, 0.49626368, 0.49626368, 0.40534322,\n",
              "        0.49626368, 0.49626368, 0.49626368, 0.41967122, 0.51775799,\n",
              "        0.51775799, 0.49458397, 0.40337895, 0.51775799, 0.51775799,\n",
              "        0.4953255 , 0.42180715, 0.5400554 , 0.5400554 , 0.49458397,\n",
              "        0.42694268, 0.5400554 , 0.5400554 , 0.49458397, 0.41848642]),\n",
              " 'split5_train_precision': array([0.66172667, 0.66172667, 0.66172667, 0.63860709, 0.66172667,\n",
              "        0.66172667, 0.66570906, 0.7286859 , 0.62658506, 0.62760577,\n",
              "        0.62658506, 0.66087003, 0.62658506, 0.62658506, 0.62658506,\n",
              "        0.3272293 , 0.59664269, 0.59664269, 0.60192612, 0.66759356,\n",
              "        0.59664269, 0.59664269, 0.60192612, 0.52618891, 0.60903569,\n",
              "        0.60903569, 0.59667453, 0.53151281, 0.60702031, 0.60702031,\n",
              "        0.60192612, 0.57976974, 0.66172667, 0.66172667, 0.66172667,\n",
              "        0.61525605, 0.66172667, 0.66172667, 0.66172667, 0.65336074,\n",
              "        0.62658506, 0.62658506, 0.62658506, 0.53346755, 0.62658506,\n",
              "        0.62658506, 0.62760577, 0.64311019, 0.59664269, 0.59664269,\n",
              "        0.59667453, 0.65067325, 0.59664269, 0.59664269, 0.59667453,\n",
              "        0.73798764, 0.60903569, 0.60903569, 0.60192612, 0.63353335,\n",
              "        0.60903569, 0.60903569, 0.60292863, 0.6049975 , 0.66202116,\n",
              "        0.66570906, 0.66172667, 0.52683704, 0.66172667, 0.66172667,\n",
              "        0.66172667, 0.64592231, 0.62760577, 0.62760577, 0.62760577,\n",
              "        0.59748748, 0.62658506, 0.62658506, 0.62658506, 0.63132007,\n",
              "        0.59664269, 0.59664269, 0.59667453, 0.59007829, 0.59664269,\n",
              "        0.59664269, 0.60192612, 0.57976974, 0.60903569, 0.60903569,\n",
              "        0.59667453, 0.52115242, 0.60903569, 0.60903569, 0.60192612,\n",
              "        0.52464816, 0.66172667, 0.66172667, 0.66570906, 0.70404884,\n",
              "        0.66172667, 0.66172667, 0.66172667, 0.3272293 , 0.62658506,\n",
              "        0.62658506, 0.62658506, 0.76394527, 0.62760577, 0.62658506,\n",
              "        0.62658506, 0.76608515, 0.59664269, 0.59664269, 0.59667453,\n",
              "        0.51752144, 0.59664269, 0.59664269, 0.59767914, 0.56864601,\n",
              "        0.60903569, 0.60903569, 0.59767914, 0.76540549, 0.60903569,\n",
              "        0.60903569, 0.60192612, 0.64311019, 0.66172667, 0.66172667,\n",
              "        0.66172667, 0.55923261, 0.66307143, 0.66172667, 0.66172667,\n",
              "        0.68063438, 0.62658506, 0.62658506, 0.62658506, 0.78276533,\n",
              "        0.62658506, 0.62658506, 0.62658506, 0.60341884, 0.59664269,\n",
              "        0.59664269, 0.59667453, 0.77245466, 0.59664269, 0.59664269,\n",
              "        0.59767914, 0.53808838, 0.60903569, 0.60903569, 0.59667453,\n",
              "        0.55346378, 0.60903569, 0.60903569, 0.59667453, 0.62442741]),\n",
              " 'split5_train_recall': array([0.53025665, 0.53025665, 0.53025665, 0.50808862, 0.53025665,\n",
              "        0.53025665, 0.53163914, 0.50640004, 0.53815018, 0.53861101,\n",
              "        0.53815018, 0.5013567 , 0.53815018, 0.53815018, 0.53815018,\n",
              "        0.5       , 0.54337852, 0.54337852, 0.53458912, 0.50724433,\n",
              "        0.54337852, 0.54337852, 0.53458912, 0.50732282, 0.55665063,\n",
              "        0.55665063, 0.53337258, 0.50585511, 0.5548331 , 0.5548331 ,\n",
              "        0.53458912, 0.51087602, 0.53025665, 0.53025665, 0.53025665,\n",
              "        0.51002052, 0.53025665, 0.53025665, 0.53025665, 0.50789689,\n",
              "        0.53815018, 0.53815018, 0.53815018, 0.51116082, 0.53815018,\n",
              "        0.53815018, 0.53861101, 0.50697523, 0.54337852, 0.54337852,\n",
              "        0.53337258, 0.50879276, 0.54337852, 0.54337852, 0.53337258,\n",
              "        0.50732169, 0.55665063, 0.55665063, 0.53458912, 0.50852366,\n",
              "        0.55665063, 0.55665063, 0.53504995, 0.51306581, 0.530935  ,\n",
              "        0.53163914, 0.53025665, 0.50847321, 0.53025665, 0.53025665,\n",
              "        0.53025665, 0.50225257, 0.53861101, 0.53861101, 0.53861101,\n",
              "        0.5092648 , 0.53815018, 0.53815018, 0.53815018, 0.50874118,\n",
              "        0.54337852, 0.54337852, 0.53337258, 0.50856066, 0.54337852,\n",
              "        0.54337852, 0.53458912, 0.51087602, 0.55665063, 0.55665063,\n",
              "        0.53337258, 0.50591454, 0.55665063, 0.55665063, 0.53458912,\n",
              "        0.50686199, 0.53025665, 0.53025665, 0.53163914, 0.50797426,\n",
              "        0.53025665, 0.53025665, 0.53025665, 0.5       , 0.53815018,\n",
              "        0.53815018, 0.53815018, 0.50848666, 0.53861101, 0.53815018,\n",
              "        0.53815018, 0.50596499, 0.54337852, 0.54337852, 0.53337258,\n",
              "        0.50394564, 0.54337852, 0.54337852, 0.53383341, 0.50881519,\n",
              "        0.55665063, 0.55665063, 0.53383341, 0.5029825 , 0.55665063,\n",
              "        0.55665063, 0.53458912, 0.50697523, 0.53025665, 0.53025665,\n",
              "        0.53025665, 0.5145963 , 0.53071748, 0.53025665, 0.53025665,\n",
              "        0.50680929, 0.53815018, 0.53815018, 0.53815018, 0.50436499,\n",
              "        0.53815018, 0.53815018, 0.53815018, 0.50730712, 0.54337852,\n",
              "        0.54337852, 0.53337258, 0.50344333, 0.54337852, 0.54337852,\n",
              "        0.53383341, 0.50423268, 0.55665063, 0.55665063, 0.53337258,\n",
              "        0.50651104, 0.55665063, 0.55665063, 0.53337258, 0.507602  ]),\n",
              " 'split6_test_accuracy': array([0.65902579, 0.65902579, 0.65902579, 0.62750716, 0.65902579,\n",
              "        0.66189112, 0.65902579, 0.65616046, 0.65329513, 0.65329513,\n",
              "        0.65329513, 0.63610315, 0.65329513, 0.65329513, 0.65329513,\n",
              "        0.64756447, 0.65616046, 0.65616046, 0.6504298 , 0.6217765 ,\n",
              "        0.65329513, 0.65329513, 0.6504298 , 0.63610315, 0.67048711,\n",
              "        0.67048711, 0.6504298 , 0.64756447, 0.67048711, 0.67048711,\n",
              "        0.6504298 , 0.64756447, 0.65902579, 0.65902579, 0.66189112,\n",
              "        0.64756447, 0.66189112, 0.65902579, 0.65616046, 0.64756447,\n",
              "        0.65329513, 0.65329513, 0.65329513, 0.65616046, 0.65329513,\n",
              "        0.65329513, 0.65329513, 0.65329513, 0.65616046, 0.65616046,\n",
              "        0.6504298 , 0.65616046, 0.65329513, 0.65616046, 0.6504298 ,\n",
              "        0.63610315, 0.67048711, 0.67048711, 0.6504298 , 0.64183381,\n",
              "        0.67048711, 0.67048711, 0.6504298 , 0.65616046, 0.65902579,\n",
              "        0.65902579, 0.65902579, 0.65616046, 0.65902579, 0.65902579,\n",
              "        0.65902579, 0.64756447, 0.65329513, 0.65329513, 0.65329513,\n",
              "        0.63610315, 0.65329513, 0.65329513, 0.65329513, 0.65616046,\n",
              "        0.65329513, 0.65616046, 0.6504298 , 0.6504298 , 0.65616046,\n",
              "        0.65616046, 0.6504298 , 0.6504298 , 0.67048711, 0.67048711,\n",
              "        0.6504298 , 0.62464183, 0.67048711, 0.67048711, 0.6504298 ,\n",
              "        0.63323782, 0.66189112, 0.65902579, 0.65616046, 0.62750716,\n",
              "        0.65902579, 0.65902579, 0.66189112, 0.63896848, 0.65329513,\n",
              "        0.65329513, 0.65329513, 0.65329513, 0.65329513, 0.65329513,\n",
              "        0.65329513, 0.65616046, 0.65616046, 0.65329513, 0.6504298 ,\n",
              "        0.65616046, 0.65616046, 0.65616046, 0.6504298 , 0.64469914,\n",
              "        0.67048711, 0.67048711, 0.6504298 , 0.65616046, 0.67048711,\n",
              "        0.67048711, 0.6504298 , 0.6504298 , 0.65902579, 0.66189112,\n",
              "        0.65902579, 0.63896848, 0.65902579, 0.65616046, 0.65616046,\n",
              "        0.64469914, 0.65329513, 0.65329513, 0.65329513, 0.63037249,\n",
              "        0.65329513, 0.65329513, 0.65329513, 0.62750716, 0.65616046,\n",
              "        0.65616046, 0.6504298 , 0.65616046, 0.65616046, 0.65616046,\n",
              "        0.6504298 , 0.63610315, 0.67048711, 0.67048711, 0.6504298 ,\n",
              "        0.63610315, 0.67048711, 0.67048711, 0.6504298 , 0.63610315]),\n",
              " 'split6_test_balanced_accuracy': array([0.51606623, 0.51606623, 0.51606623, 0.49403202, 0.51606623,\n",
              "        0.51824964, 0.51606623, 0.50198326, 0.51566594, 0.51566594,\n",
              "        0.51566594, 0.48669942, 0.51566594, 0.51566594, 0.51566594,\n",
              "        0.49543304, 0.52974891, 0.52974891, 0.51348253, 0.48966521,\n",
              "        0.52558224, 0.52558224, 0.51348253, 0.48669942, 0.54859898,\n",
              "        0.54859898, 0.51348253, 0.49543304, 0.54859898, 0.54859898,\n",
              "        0.51348253, 0.49543304, 0.51606623, 0.51606623, 0.51824964,\n",
              "        0.49543304, 0.51824964, 0.51606623, 0.51388282, 0.49543304,\n",
              "        0.51566594, 0.51566594, 0.51566594, 0.50198326, 0.51566594,\n",
              "        0.51566594, 0.51566594, 0.50376638, 0.52974891, 0.52974891,\n",
              "        0.51348253, 0.50198326, 0.52558224, 0.52974891, 0.51348253,\n",
              "        0.49859898, 0.54859898, 0.54859898, 0.51348253, 0.49106623,\n",
              "        0.54859898, 0.54859898, 0.51348253, 0.50198326, 0.51606623,\n",
              "        0.51606623, 0.51606623, 0.50198326, 0.51606623, 0.51606623,\n",
              "        0.51606623, 0.49543304, 0.51566594, 0.51566594, 0.51566594,\n",
              "        0.49859898, 0.51566594, 0.51566594, 0.51566594, 0.5       ,\n",
              "        0.52558224, 0.52974891, 0.51348253, 0.49761645, 0.52974891,\n",
              "        0.52974891, 0.51348253, 0.49761645, 0.54859898, 0.54859898,\n",
              "        0.51348253, 0.48193231, 0.54859898, 0.54859898, 0.51348253,\n",
              "        0.48451601, 0.51824964, 0.51606623, 0.51388282, 0.49403202,\n",
              "        0.51606623, 0.51606623, 0.51824964, 0.50871543, 0.51566594,\n",
              "        0.51566594, 0.51566594, 0.49979985, 0.51566594, 0.51566594,\n",
              "        0.51566594, 0.50198326, 0.52974891, 0.52558224, 0.51348253,\n",
              "        0.50396652, 0.52974891, 0.52974891, 0.51348253, 0.4952329 ,\n",
              "        0.54859898, 0.54859898, 0.51348253, 0.50198326, 0.54859898,\n",
              "        0.54859898, 0.51348253, 0.49761645, 0.51606623, 0.51824964,\n",
              "        0.51606623, 0.48888282, 0.51606623, 0.51388282, 0.51388282,\n",
              "        0.49324964, 0.51566594, 0.51566594, 0.51566594, 0.49621543,\n",
              "        0.51566594, 0.51566594, 0.51566594, 0.49204876, 0.52974891,\n",
              "        0.52974891, 0.51348253, 0.50198326, 0.52974891, 0.52974891,\n",
              "        0.51348253, 0.48669942, 0.54859898, 0.54859898, 0.51348253,\n",
              "        0.49859898, 0.54859898, 0.54859898, 0.51348253, 0.48669942]),\n",
              " 'split6_test_f1': array([0.44732184, 0.44732184, 0.44732184, 0.4370409 , 0.44732184,\n",
              "        0.44879002, 0.44732184, 0.40403005, 0.45651922, 0.45651922,\n",
              "        0.45651922, 0.39615264, 0.45651922, 0.45651922, 0.45651922,\n",
              "        0.40067016, 0.49051095, 0.49051095, 0.45496672, 0.43405405,\n",
              "        0.48375937, 0.48375937, 0.45496672, 0.39615264, 0.51867842,\n",
              "        0.51867842, 0.45496672, 0.40067016, 0.51867842, 0.51867842,\n",
              "        0.45496672, 0.40067016, 0.44732184, 0.44732184, 0.44879002,\n",
              "        0.40067016, 0.44879002, 0.44732184, 0.44585583, 0.40067016,\n",
              "        0.45651922, 0.45651922, 0.45651922, 0.40403005, 0.45651922,\n",
              "        0.45651922, 0.45651922, 0.41766758, 0.49051095, 0.49051095,\n",
              "        0.45496672, 0.40403005, 0.48375937, 0.49051095, 0.45496672,\n",
              "        0.4356418 , 0.51867842, 0.51867842, 0.45496672, 0.39841692,\n",
              "        0.51867842, 0.51867842, 0.45496672, 0.40403005, 0.44732184,\n",
              "        0.44732184, 0.44732184, 0.40403005, 0.44732184, 0.44732184,\n",
              "        0.44732184, 0.40067016, 0.45651922, 0.45651922, 0.45651922,\n",
              "        0.4356418 , 0.45651922, 0.45651922, 0.45651922, 0.39619377,\n",
              "        0.48375937, 0.49051095, 0.45496672, 0.40179274, 0.49051095,\n",
              "        0.49051095, 0.45496672, 0.40179274, 0.51867842, 0.51867842,\n",
              "        0.45496672, 0.40514202, 0.51867842, 0.51867842, 0.45496672,\n",
              "        0.39501625, 0.44879002, 0.44732184, 0.44585583, 0.4370409 ,\n",
              "        0.44732184, 0.44732184, 0.44879002, 0.45977887, 0.45651922,\n",
              "        0.45651922, 0.45651922, 0.40291269, 0.45651922, 0.45651922,\n",
              "        0.45651922, 0.40403005, 0.49051095, 0.48375937, 0.45496672,\n",
              "        0.41159942, 0.49051095, 0.49051095, 0.45496672, 0.40685307,\n",
              "        0.51867842, 0.51867842, 0.45496672, 0.40403005, 0.51867842,\n",
              "        0.51867842, 0.45496672, 0.40179274, 0.44732184, 0.44879002,\n",
              "        0.44732184, 0.39728618, 0.44732184, 0.44585583, 0.44585583,\n",
              "        0.3995449 , 0.45651922, 0.45651922, 0.45651922, 0.43853589,\n",
              "        0.45651922, 0.45651922, 0.45651922, 0.43128259, 0.49051095,\n",
              "        0.49051095, 0.45496672, 0.40403005, 0.49051095, 0.49051095,\n",
              "        0.45496672, 0.39615264, 0.51867842, 0.51867842, 0.45496672,\n",
              "        0.4356418 , 0.51867842, 0.51867842, 0.45496672, 0.39615264]),\n",
              " 'split6_test_precision': array([0.60107601, 0.60107601, 0.60107601, 0.48047154, 0.60107601,\n",
              "        0.62401088, 0.60107601, 0.57853026, 0.56866029, 0.56866029,\n",
              "        0.56866029, 0.38055556, 0.56866029, 0.56866029, 0.56866029,\n",
              "        0.42703488, 0.5854232 , 0.5854232 , 0.55630699, 0.46840231,\n",
              "        0.57575431, 0.57575431, 0.55630699, 0.38055556, 0.62806866,\n",
              "        0.62806866, 0.55630699, 0.42703488, 0.62806866, 0.62806866,\n",
              "        0.55630699, 0.42703488, 0.60107601, 0.60107601, 0.62401088,\n",
              "        0.42703488, 0.62401088, 0.60107601, 0.58134328, 0.42703488,\n",
              "        0.56866029, 0.56866029, 0.56866029, 0.57853026, 0.56866029,\n",
              "        0.56866029, 0.56866029, 0.54323308, 0.5854232 , 0.5854232 ,\n",
              "        0.55630699, 0.57853026, 0.57575431, 0.5854232 , 0.55630699,\n",
              "        0.49441057, 0.62806866, 0.62806866, 0.55630699, 0.39745196,\n",
              "        0.62806866, 0.62806866, 0.55630699, 0.57853026, 0.60107601,\n",
              "        0.60107601, 0.60107601, 0.57853026, 0.60107601, 0.60107601,\n",
              "        0.60107601, 0.42703488, 0.56866029, 0.56866029, 0.56866029,\n",
              "        0.49441057, 0.56866029, 0.56866029, 0.56866029, 0.32808023,\n",
              "        0.57575431, 0.5854232 , 0.55630699, 0.45253623, 0.5854232 ,\n",
              "        0.5854232 , 0.55630699, 0.45253623, 0.62806866, 0.62806866,\n",
              "        0.55630699, 0.41203047, 0.62806866, 0.62806866, 0.55630699,\n",
              "        0.37448378, 0.62401088, 0.60107601, 0.58134328, 0.48047154,\n",
              "        0.60107601, 0.60107601, 0.62401088, 0.52664664, 0.56866029,\n",
              "        0.56866029, 0.56866029, 0.49470135, 0.56866029, 0.56866029,\n",
              "        0.56866029, 0.57853026, 0.5854232 , 0.57575431, 0.55630699,\n",
              "        0.57898551, 0.5854232 , 0.5854232 , 0.55630699, 0.45197947,\n",
              "        0.62806866, 0.62806866, 0.55630699, 0.57853026, 0.62806866,\n",
              "        0.62806866, 0.55630699, 0.45253623, 0.60107601, 0.62401088,\n",
              "        0.60107601, 0.3880132 , 0.60107601, 0.58134328, 0.58134328,\n",
              "        0.40986395, 0.56866029, 0.56866029, 0.56866029, 0.48716049,\n",
              "        0.56866029, 0.56866029, 0.56866029, 0.47198718, 0.5854232 ,\n",
              "        0.5854232 , 0.55630699, 0.57853026, 0.5854232 , 0.5854232 ,\n",
              "        0.55630699, 0.38055556, 0.62806866, 0.62806866, 0.55630699,\n",
              "        0.49441057, 0.62806866, 0.62806866, 0.55630699, 0.38055556]),\n",
              " 'split6_test_recall': array([0.51606623, 0.51606623, 0.51606623, 0.49403202, 0.51606623,\n",
              "        0.51824964, 0.51606623, 0.50198326, 0.51566594, 0.51566594,\n",
              "        0.51566594, 0.48669942, 0.51566594, 0.51566594, 0.51566594,\n",
              "        0.49543304, 0.52974891, 0.52974891, 0.51348253, 0.48966521,\n",
              "        0.52558224, 0.52558224, 0.51348253, 0.48669942, 0.54859898,\n",
              "        0.54859898, 0.51348253, 0.49543304, 0.54859898, 0.54859898,\n",
              "        0.51348253, 0.49543304, 0.51606623, 0.51606623, 0.51824964,\n",
              "        0.49543304, 0.51824964, 0.51606623, 0.51388282, 0.49543304,\n",
              "        0.51566594, 0.51566594, 0.51566594, 0.50198326, 0.51566594,\n",
              "        0.51566594, 0.51566594, 0.50376638, 0.52974891, 0.52974891,\n",
              "        0.51348253, 0.50198326, 0.52558224, 0.52974891, 0.51348253,\n",
              "        0.49859898, 0.54859898, 0.54859898, 0.51348253, 0.49106623,\n",
              "        0.54859898, 0.54859898, 0.51348253, 0.50198326, 0.51606623,\n",
              "        0.51606623, 0.51606623, 0.50198326, 0.51606623, 0.51606623,\n",
              "        0.51606623, 0.49543304, 0.51566594, 0.51566594, 0.51566594,\n",
              "        0.49859898, 0.51566594, 0.51566594, 0.51566594, 0.5       ,\n",
              "        0.52558224, 0.52974891, 0.51348253, 0.49761645, 0.52974891,\n",
              "        0.52974891, 0.51348253, 0.49761645, 0.54859898, 0.54859898,\n",
              "        0.51348253, 0.48193231, 0.54859898, 0.54859898, 0.51348253,\n",
              "        0.48451601, 0.51824964, 0.51606623, 0.51388282, 0.49403202,\n",
              "        0.51606623, 0.51606623, 0.51824964, 0.50871543, 0.51566594,\n",
              "        0.51566594, 0.51566594, 0.49979985, 0.51566594, 0.51566594,\n",
              "        0.51566594, 0.50198326, 0.52974891, 0.52558224, 0.51348253,\n",
              "        0.50396652, 0.52974891, 0.52974891, 0.51348253, 0.4952329 ,\n",
              "        0.54859898, 0.54859898, 0.51348253, 0.50198326, 0.54859898,\n",
              "        0.54859898, 0.51348253, 0.49761645, 0.51606623, 0.51824964,\n",
              "        0.51606623, 0.48888282, 0.51606623, 0.51388282, 0.51388282,\n",
              "        0.49324964, 0.51566594, 0.51566594, 0.51566594, 0.49621543,\n",
              "        0.51566594, 0.51566594, 0.51566594, 0.49204876, 0.52974891,\n",
              "        0.52974891, 0.51348253, 0.50198326, 0.52974891, 0.52974891,\n",
              "        0.51348253, 0.48669942, 0.54859898, 0.54859898, 0.51348253,\n",
              "        0.49859898, 0.54859898, 0.54859898, 0.51348253, 0.48669942]),\n",
              " 'split6_train_accuracy': array([0.67070064, 0.67070064, 0.67070064, 0.63917197, 0.67006369,\n",
              "        0.67101911, 0.67038217, 0.65859873, 0.66528662, 0.66592357,\n",
              "        0.6656051 , 0.6544586 , 0.66624204, 0.66624204, 0.66528662,\n",
              "        0.65859873, 0.6589172 , 0.6589172 , 0.66433121, 0.63280255,\n",
              "        0.65987261, 0.65955414, 0.66433121, 0.6544586 , 0.66369427,\n",
              "        0.66369427, 0.66433121, 0.65573248, 0.66273885, 0.66273885,\n",
              "        0.66464968, 0.65764331, 0.67070064, 0.67006369, 0.67038217,\n",
              "        0.65859873, 0.67038217, 0.67038217, 0.67006369, 0.6566879 ,\n",
              "        0.66528662, 0.6656051 , 0.6656051 , 0.65796178, 0.66624204,\n",
              "        0.66624204, 0.66624204, 0.65764331, 0.65923567, 0.6589172 ,\n",
              "        0.66433121, 0.6589172 , 0.6589172 , 0.65923567, 0.66433121,\n",
              "        0.63566879, 0.6633758 , 0.66369427, 0.66433121, 0.65477707,\n",
              "        0.66369427, 0.66369427, 0.66433121, 0.65955414, 0.67070064,\n",
              "        0.67038217, 0.67006369, 0.65955414, 0.67038217, 0.67038217,\n",
              "        0.67038217, 0.65828025, 0.66496815, 0.66496815, 0.66592357,\n",
              "        0.63566879, 0.66496815, 0.66624204, 0.6656051 , 0.6544586 ,\n",
              "        0.65859873, 0.6589172 , 0.66433121, 0.65796178, 0.6589172 ,\n",
              "        0.6589172 , 0.66433121, 0.65796178, 0.66369427, 0.6633758 ,\n",
              "        0.66433121, 0.65095541, 0.66369427, 0.66369427, 0.66433121,\n",
              "        0.65350318, 0.67006369, 0.67038217, 0.66942675, 0.63853503,\n",
              "        0.67038217, 0.67070064, 0.67070064, 0.63949045, 0.66624204,\n",
              "        0.6656051 , 0.66592357, 0.6589172 , 0.66592357, 0.6656051 ,\n",
              "        0.66528662, 0.65955414, 0.6589172 , 0.65859873, 0.66433121,\n",
              "        0.65955414, 0.6589172 , 0.6589172 , 0.66433121, 0.65573248,\n",
              "        0.66369427, 0.66369427, 0.66433121, 0.65859873, 0.66369427,\n",
              "        0.66369427, 0.66433121, 0.65828025, 0.67038217, 0.67038217,\n",
              "        0.67006369, 0.65382166, 0.67038217, 0.66974522, 0.67006369,\n",
              "        0.65605096, 0.66528662, 0.66592357, 0.66496815, 0.63757962,\n",
              "        0.66528662, 0.6656051 , 0.66592357, 0.6433121 , 0.6589172 ,\n",
              "        0.6589172 , 0.66433121, 0.65923567, 0.65923567, 0.6589172 ,\n",
              "        0.66433121, 0.65350318, 0.66369427, 0.66369427, 0.66433121,\n",
              "        0.63694268, 0.66369427, 0.66369427, 0.66433121, 0.65541401]),\n",
              " 'split6_train_balanced_accuracy': array([0.53285569, 0.53285569, 0.53285569, 0.51050826, 0.53215155,\n",
              "        0.53309899, 0.53261238, 0.50990615, 0.53524505, 0.53573166,\n",
              "        0.53548836, 0.50913586, 0.53597497, 0.53597497, 0.53502753,\n",
              "        0.51229888, 0.54103737, 0.54103737, 0.53582024, 0.50281431,\n",
              "        0.54154978, 0.54130647, 0.53582024, 0.5095709 , 0.55469295,\n",
              "        0.55469295, 0.53582024, 0.50945654, 0.55331046, 0.55331046,\n",
              "        0.53606355, 0.50982879, 0.53285569, 0.53215155, 0.53261238,\n",
              "        0.51121128, 0.53261238, 0.53261238, 0.53236907, 0.51083902,\n",
              "        0.53524505, 0.53548836, 0.53548836, 0.50876697, 0.53597497,\n",
              "        0.53597497, 0.53597497, 0.51483176, 0.54128068, 0.54103737,\n",
              "        0.53582024, 0.51014946, 0.54081985, 0.54128068, 0.53582024,\n",
              "        0.49826096, 0.55444964, 0.55469295, 0.53582024, 0.50959669,\n",
              "        0.55469295, 0.55469295, 0.53582024, 0.51041856, 0.53285569,\n",
              "        0.53261238, 0.53215155, 0.50846087, 0.53261238, 0.53261238,\n",
              "        0.53261238, 0.51140301, 0.53500174, 0.53500174, 0.53551414,\n",
              "        0.50043616, 0.53500174, 0.53597497, 0.53548836, 0.5       ,\n",
              "        0.54057654, 0.54103737, 0.53582024, 0.5100721 , 0.54103737,\n",
              "        0.54103737, 0.53582024, 0.50963705, 0.55469295, 0.55444964,\n",
              "        0.53582024, 0.51189747, 0.55469295, 0.55469295, 0.53582024,\n",
              "        0.50971106, 0.53193403, 0.53261238, 0.53144741, 0.51002164,\n",
              "        0.53239486, 0.53285569, 0.53285569, 0.51118661, 0.53597497,\n",
              "        0.53548836, 0.53551414, 0.5105845 , 0.53573166, 0.53548836,\n",
              "        0.53524505, 0.50911344, 0.54103737, 0.54057654, 0.53582024,\n",
              "        0.51128864, 0.54103737, 0.54103737, 0.53582024, 0.51380694,\n",
              "        0.55469295, 0.55469295, 0.53582024, 0.50860103, 0.55469295,\n",
              "        0.55469295, 0.53582024, 0.51096797, 0.53261238, 0.53239486,\n",
              "        0.53215155, 0.50799668, 0.53261238, 0.53212576, 0.53236907,\n",
              "        0.50991736, 0.53524505, 0.53573166, 0.53500174, 0.50820411,\n",
              "        0.53524505, 0.53548836, 0.53573166, 0.51084351, 0.54103737,\n",
              "        0.54103737, 0.53582024, 0.50952269, 0.54128068, 0.54103737,\n",
              "        0.53582024, 0.50927602, 0.55469295, 0.55469295, 0.53582024,\n",
              "        0.50227948, 0.55469295, 0.55469295, 0.53582024, 0.51225851]),\n",
              " 'split6_train_f1': array([0.47470876, 0.47470876, 0.47470876, 0.46168369, 0.47369273,\n",
              "        0.4748908 , 0.47452678, 0.42317829, 0.48995714, 0.49034737,\n",
              "        0.49015222, 0.42978453, 0.49054258, 0.49054258, 0.48937567,\n",
              "        0.4317264 , 0.51197171, 0.51197171, 0.49281292, 0.45037884,\n",
              "        0.51211706, 0.51190558, 0.49281292, 0.43128391, 0.53626457,\n",
              "        0.53626457, 0.49281292, 0.42810391, 0.53430334, 0.53430334,\n",
              "        0.4930103 , 0.42511323, 0.47470876, 0.47369273, 0.47452678,\n",
              "        0.42788419, 0.47452678, 0.47452678, 0.47434485, 0.43083   ,\n",
              "        0.48995714, 0.49015222, 0.49015222, 0.42050692, 0.49054258,\n",
              "        0.49054258, 0.49054258, 0.44236097, 0.51218336, 0.51197171,\n",
              "        0.49281292, 0.4233198 , 0.51148283, 0.51218336, 0.49281292,\n",
              "        0.43208752, 0.53603897, 0.53626457, 0.49281292, 0.4306856 ,\n",
              "        0.53626457, 0.53626457, 0.49281292, 0.42280593, 0.47470876,\n",
              "        0.47452678, 0.47369273, 0.41549698, 0.47452678, 0.47452678,\n",
              "        0.47452678, 0.42928229, 0.48976213, 0.48976213, 0.48976505,\n",
              "        0.438725  , 0.48976213, 0.49054258, 0.49015222, 0.39557267,\n",
              "        0.51127156, 0.51197171, 0.49281292, 0.42525695, 0.51197171,\n",
              "        0.51197171, 0.49281292, 0.42368544, 0.53626457, 0.53603897,\n",
              "        0.49281292, 0.44595669, 0.53626457, 0.53626457, 0.49281292,\n",
              "        0.43379015, 0.47303829, 0.47452678, 0.47267669, 0.46132096,\n",
              "        0.47387413, 0.47470876, 0.47470876, 0.46303704, 0.49054258,\n",
              "        0.49015222, 0.48976505, 0.42490164, 0.49034737, 0.49015222,\n",
              "        0.48995714, 0.41796104, 0.51197171, 0.51127156, 0.49281292,\n",
              "        0.42597559, 0.51197171, 0.51197171, 0.49281292, 0.44283098,\n",
              "        0.53626457, 0.53626457, 0.49281292, 0.41836539, 0.53626457,\n",
              "        0.53626457, 0.49281292, 0.42773829, 0.47452678, 0.47387413,\n",
              "        0.47369273, 0.42722029, 0.47452678, 0.47416298, 0.47434485,\n",
              "        0.42901401, 0.48995714, 0.49034737, 0.48976213, 0.45782686,\n",
              "        0.48995714, 0.49015222, 0.49034737, 0.45616237, 0.51197171,\n",
              "        0.51197171, 0.49281292, 0.42025875, 0.51218336, 0.51197171,\n",
              "        0.49281292, 0.43231638, 0.53626457, 0.53626457, 0.49281292,\n",
              "        0.44198349, 0.53626457, 0.53626457, 0.49281292, 0.4383677 ]),\n",
              " 'split6_train_precision': array([0.67806403, 0.67806403, 0.67806403, 0.53219397, 0.67424789,\n",
              "        0.6806415 , 0.67552308, 0.64583155, 0.62550226, 0.62834764,\n",
              "        0.62691855, 0.57935209, 0.6297897 , 0.6297897 , 0.62581838,\n",
              "        0.62584959, 0.59814901, 0.59814901, 0.61980982, 0.50881414,\n",
              "        0.60071468, 0.59985515, 0.61980982, 0.57945609, 0.60917618,\n",
              "        0.60917618, 0.61980982, 0.59293295, 0.6070852 , 0.6070852 ,\n",
              "        0.62111107, 0.62259728, 0.67806403, 0.67424789, 0.67552308,\n",
              "        0.63309356, 0.67552308, 0.67552308, 0.67301787, 0.60246936,\n",
              "        0.62550226, 0.62691855, 0.62691855, 0.64035182, 0.6297897 ,\n",
              "        0.6297897 , 0.6297897 , 0.60473675, 0.59899395, 0.59814901,\n",
              "        0.61980982, 0.65247682, 0.59815054, 0.59899395, 0.61980982,\n",
              "        0.4925271 , 0.60846511, 0.60917618, 0.61980982, 0.58240108,\n",
              "        0.60917618, 0.60917618, 0.61980982, 0.67052795, 0.67806403,\n",
              "        0.67552308, 0.67424789, 0.73300479, 0.67552308, 0.67552308,\n",
              "        0.67552308, 0.62531792, 0.62409859, 0.62409859, 0.62869332,\n",
              "        0.50169344, 0.62409859, 0.6297897 , 0.62691855, 0.3272293 ,\n",
              "        0.59730538, 0.59814901, 0.61980982, 0.62779475, 0.59814901,\n",
              "        0.59814901, 0.61980982, 0.63133286, 0.60917618, 0.60846511,\n",
              "        0.61980982, 0.56108457, 0.60917618, 0.60917618, 0.61980982,\n",
              "        0.57192112, 0.67551612, 0.67552308, 0.67043176, 0.53048248,\n",
              "        0.67679858, 0.67806403, 0.67806403, 0.53390424, 0.6297897 ,\n",
              "        0.62691855, 0.62869332, 0.6469709 , 0.62834764, 0.62691855,\n",
              "        0.62550226, 0.70431145, 0.59814901, 0.59730538, 0.61980982,\n",
              "        0.65674821, 0.59814901, 0.59814901, 0.61980982, 0.5892412 ,\n",
              "        0.60917618, 0.60917618, 0.61980982, 0.66703539, 0.60917618,\n",
              "        0.60917618, 0.61980982, 0.62814668, 0.67552308, 0.67679858,\n",
              "        0.67424789, 0.57283497, 0.67552308, 0.67054762, 0.67301787,\n",
              "        0.59619358, 0.62550226, 0.62834764, 0.62409859, 0.52559928,\n",
              "        0.62550226, 0.62691855, 0.62834764, 0.53878011, 0.59814901,\n",
              "        0.59814901, 0.61980982, 0.67556371, 0.59899395, 0.59814901,\n",
              "        0.61980982, 0.57149771, 0.60917618, 0.60917618, 0.61980982,\n",
              "        0.50868419, 0.60917618, 0.60917618, 0.61980982, 0.58737704]),\n",
              " 'split6_train_recall': array([0.53285569, 0.53285569, 0.53285569, 0.51050826, 0.53215155,\n",
              "        0.53309899, 0.53261238, 0.50990615, 0.53524505, 0.53573166,\n",
              "        0.53548836, 0.50913586, 0.53597497, 0.53597497, 0.53502753,\n",
              "        0.51229888, 0.54103737, 0.54103737, 0.53582024, 0.50281431,\n",
              "        0.54154978, 0.54130647, 0.53582024, 0.5095709 , 0.55469295,\n",
              "        0.55469295, 0.53582024, 0.50945654, 0.55331046, 0.55331046,\n",
              "        0.53606355, 0.50982879, 0.53285569, 0.53215155, 0.53261238,\n",
              "        0.51121128, 0.53261238, 0.53261238, 0.53236907, 0.51083902,\n",
              "        0.53524505, 0.53548836, 0.53548836, 0.50876697, 0.53597497,\n",
              "        0.53597497, 0.53597497, 0.51483176, 0.54128068, 0.54103737,\n",
              "        0.53582024, 0.51014946, 0.54081985, 0.54128068, 0.53582024,\n",
              "        0.49826096, 0.55444964, 0.55469295, 0.53582024, 0.50959669,\n",
              "        0.55469295, 0.55469295, 0.53582024, 0.51041856, 0.53285569,\n",
              "        0.53261238, 0.53215155, 0.50846087, 0.53261238, 0.53261238,\n",
              "        0.53261238, 0.51140301, 0.53500174, 0.53500174, 0.53551414,\n",
              "        0.50043616, 0.53500174, 0.53597497, 0.53548836, 0.5       ,\n",
              "        0.54057654, 0.54103737, 0.53582024, 0.5100721 , 0.54103737,\n",
              "        0.54103737, 0.53582024, 0.50963705, 0.55469295, 0.55444964,\n",
              "        0.53582024, 0.51189747, 0.55469295, 0.55469295, 0.53582024,\n",
              "        0.50971106, 0.53193403, 0.53261238, 0.53144741, 0.51002164,\n",
              "        0.53239486, 0.53285569, 0.53285569, 0.51118661, 0.53597497,\n",
              "        0.53548836, 0.53551414, 0.5105845 , 0.53573166, 0.53548836,\n",
              "        0.53524505, 0.50911344, 0.54103737, 0.54057654, 0.53582024,\n",
              "        0.51128864, 0.54103737, 0.54103737, 0.53582024, 0.51380694,\n",
              "        0.55469295, 0.55469295, 0.53582024, 0.50860103, 0.55469295,\n",
              "        0.55469295, 0.53582024, 0.51096797, 0.53261238, 0.53239486,\n",
              "        0.53215155, 0.50799668, 0.53261238, 0.53212576, 0.53236907,\n",
              "        0.50991736, 0.53524505, 0.53573166, 0.53500174, 0.50820411,\n",
              "        0.53524505, 0.53548836, 0.53573166, 0.51084351, 0.54103737,\n",
              "        0.54103737, 0.53582024, 0.50952269, 0.54128068, 0.54103737,\n",
              "        0.53582024, 0.50927602, 0.55469295, 0.55469295, 0.53582024,\n",
              "        0.50227948, 0.55469295, 0.55469295, 0.53582024, 0.51225851]),\n",
              " 'split7_test_accuracy': array([0.67335244, 0.67335244, 0.67335244, 0.64756447, 0.67335244,\n",
              "        0.67335244, 0.67335244, 0.65902579, 0.67335244, 0.67335244,\n",
              "        0.67335244, 0.65616046, 0.67335244, 0.67335244, 0.67335244,\n",
              "        0.65902579, 0.67048711, 0.67048711, 0.67048711, 0.65616046,\n",
              "        0.67048711, 0.67048711, 0.67048711, 0.65616046, 0.68767908,\n",
              "        0.68767908, 0.67048711, 0.65616046, 0.68481375, 0.68481375,\n",
              "        0.67048711, 0.65329513, 0.67335244, 0.67335244, 0.67335244,\n",
              "        0.65902579, 0.67335244, 0.67335244, 0.67335244, 0.64756447,\n",
              "        0.67335244, 0.67335244, 0.67335244, 0.65329513, 0.67335244,\n",
              "        0.67335244, 0.67335244, 0.65616046, 0.67048711, 0.67048711,\n",
              "        0.67048711, 0.65616046, 0.67048711, 0.67048711, 0.67048711,\n",
              "        0.64756447, 0.68767908, 0.68767908, 0.67048711, 0.65902579,\n",
              "        0.68767908, 0.68767908, 0.67048711, 0.65616046, 0.67335244,\n",
              "        0.67335244, 0.67335244, 0.65616046, 0.67335244, 0.67335244,\n",
              "        0.67335244, 0.65902579, 0.67335244, 0.67335244, 0.67335244,\n",
              "        0.65902579, 0.67335244, 0.67335244, 0.67335244, 0.6504298 ,\n",
              "        0.67048711, 0.67048711, 0.67048711, 0.65329513, 0.67048711,\n",
              "        0.67048711, 0.67048711, 0.6504298 , 0.68767908, 0.68767908,\n",
              "        0.67048711, 0.65329513, 0.68767908, 0.68767908, 0.67048711,\n",
              "        0.65616046, 0.67335244, 0.67335244, 0.67335244, 0.65329513,\n",
              "        0.67335244, 0.67335244, 0.67335244, 0.64756447, 0.67335244,\n",
              "        0.67335244, 0.67335244, 0.6504298 , 0.67335244, 0.67335244,\n",
              "        0.67335244, 0.65329513, 0.67048711, 0.67048711, 0.67048711,\n",
              "        0.6504298 , 0.67048711, 0.67048711, 0.67048711, 0.65616046,\n",
              "        0.68767908, 0.68767908, 0.67048711, 0.64469914, 0.68767908,\n",
              "        0.68767908, 0.67048711, 0.65616046, 0.67335244, 0.67335244,\n",
              "        0.67335244, 0.64469914, 0.67335244, 0.67335244, 0.67335244,\n",
              "        0.66189112, 0.67335244, 0.67335244, 0.67335244, 0.65329513,\n",
              "        0.67335244, 0.67335244, 0.67335244, 0.65616046, 0.67048711,\n",
              "        0.67048711, 0.67048711, 0.65616046, 0.67048711, 0.67048711,\n",
              "        0.67048711, 0.64756447, 0.68767908, 0.68767908, 0.67048711,\n",
              "        0.65616046, 0.68767908, 0.68767908, 0.67335244, 0.65329513]),\n",
              " 'split7_test_balanced_accuracy': array([0.5349163 , 0.5349163 , 0.5349163 , 0.51328239, 0.5349163 ,\n",
              "        0.5349163 , 0.5349163 , 0.50416667, 0.54086608, 0.54086608,\n",
              "        0.54086608, 0.50396652, 0.54086608, 0.54086608, 0.54086608,\n",
              "        0.50416667, 0.54859898, 0.54859898, 0.53868268, 0.50594978,\n",
              "        0.54859898, 0.54859898, 0.53868268, 0.50396652, 0.57558224,\n",
              "        0.57558224, 0.53868268, 0.50396652, 0.57141557, 0.57141557,\n",
              "        0.53868268, 0.50178311, 0.5349163 , 0.5349163 , 0.5349163 ,\n",
              "        0.50416667, 0.5349163 , 0.5349163 , 0.5349163 , 0.49939956,\n",
              "        0.54086608, 0.54086608, 0.54086608, 0.50178311, 0.54086608,\n",
              "        0.54086608, 0.54086608, 0.50396652, 0.54859898, 0.54859898,\n",
              "        0.53868268, 0.50396652, 0.54859898, 0.54859898, 0.53868268,\n",
              "        0.51129913, 0.57558224, 0.57558224, 0.53868268, 0.50813319,\n",
              "        0.57558224, 0.57558224, 0.53868268, 0.5       , 0.5349163 ,\n",
              "        0.5349163 , 0.5349163 , 0.50594978, 0.5349163 , 0.5349163 ,\n",
              "        0.5349163 , 0.50416667, 0.54086608, 0.54086608, 0.54086608,\n",
              "        0.50416667, 0.54086608, 0.54086608, 0.54086608, 0.49959971,\n",
              "        0.54859898, 0.54859898, 0.53868268, 0.50376638, 0.54859898,\n",
              "        0.54859898, 0.53868268, 0.49959971, 0.57558224, 0.57558224,\n",
              "        0.53868268, 0.50178311, 0.57558224, 0.57558224, 0.53868268,\n",
              "        0.50396652, 0.5349163 , 0.5349163 , 0.5349163 , 0.50178311,\n",
              "        0.5349163 , 0.5349163 , 0.5349163 , 0.4974163 , 0.54086608,\n",
              "        0.54086608, 0.54086608, 0.50158297, 0.54086608, 0.54086608,\n",
              "        0.54086608, 0.50178311, 0.54859898, 0.54859898, 0.53868268,\n",
              "        0.50158297, 0.54859898, 0.54859898, 0.53868268, 0.50594978,\n",
              "        0.57558224, 0.57558224, 0.53868268, 0.5051492 , 0.57558224,\n",
              "        0.57558224, 0.53868268, 0.50396652, 0.5349163 , 0.5349163 ,\n",
              "        0.5349163 , 0.49721616, 0.5349163 , 0.5349163 , 0.5349163 ,\n",
              "        0.50833333, 0.54086608, 0.54086608, 0.54086608, 0.50178311,\n",
              "        0.54086608, 0.54086608, 0.54086608, 0.5       , 0.54859898,\n",
              "        0.54859898, 0.53868268, 0.50793304, 0.54859898, 0.54859898,\n",
              "        0.53868268, 0.51328239, 0.57558224, 0.57558224, 0.53868268,\n",
              "        0.5       , 0.57558224, 0.57558224, 0.54284934, 0.50178311]),\n",
              " 'split7_test_f1': array([0.47945887, 0.47945887, 0.47945887, 0.45911926, 0.47945887,\n",
              "        0.47945887, 0.47945887, 0.40514488, 0.49607356, 0.49607356,\n",
              "        0.49607356, 0.41159942, 0.49607356, 0.49607356, 0.49607356,\n",
              "        0.40514488, 0.51867842, 0.51867842, 0.49429849, 0.41891442,\n",
              "        0.51867842, 0.51867842, 0.49429849, 0.41159942, 0.56000093,\n",
              "        0.56000093, 0.49429849, 0.41159942, 0.55401952, 0.55401952,\n",
              "        0.49429849, 0.41041536, 0.47945887, 0.47945887, 0.47945887,\n",
              "        0.40514488, 0.47945887, 0.47945887, 0.47945887, 0.41517146,\n",
              "        0.49607356, 0.49607356, 0.49607356, 0.41041536, 0.49607356,\n",
              "        0.49607356, 0.49607356, 0.41159942, 0.51867842, 0.51867842,\n",
              "        0.49429849, 0.41159942, 0.51867842, 0.51867842, 0.49429849,\n",
              "        0.45341686, 0.56000093, 0.56000093, 0.49429849, 0.42016056,\n",
              "        0.56000093, 0.56000093, 0.49429849, 0.39619377, 0.47945887,\n",
              "        0.47945887, 0.47945887, 0.41891442, 0.47945887, 0.47945887,\n",
              "        0.47945887, 0.40514488, 0.49607356, 0.49607356, 0.49607356,\n",
              "        0.40514488, 0.49607356, 0.49607356, 0.49607356, 0.40922966,\n",
              "        0.51867842, 0.51867842, 0.49429849, 0.41766758, 0.51867842,\n",
              "        0.51867842, 0.49429849, 0.40922966, 0.56000093, 0.56000093,\n",
              "        0.49429849, 0.41041536, 0.56000093, 0.56000093, 0.49429849,\n",
              "        0.41159942, 0.47945887, 0.47945887, 0.47945887, 0.41041536,\n",
              "        0.47945887, 0.47945887, 0.47945887, 0.40804225, 0.49607356,\n",
              "        0.49607356, 0.49607356, 0.41641996, 0.49607356, 0.49607356,\n",
              "        0.49607356, 0.41041536, 0.51867842, 0.51867842, 0.49429849,\n",
              "        0.41641996, 0.51867842, 0.51867842, 0.49429849, 0.41891442,\n",
              "        0.56000093, 0.56000093, 0.49429849, 0.44001035, 0.56000093,\n",
              "        0.56000093, 0.49429849, 0.41159942, 0.47945887, 0.47945887,\n",
              "        0.47945887, 0.41392199, 0.47945887, 0.47945887, 0.47945887,\n",
              "        0.41396289, 0.49607356, 0.49607356, 0.49607356, 0.41041536,\n",
              "        0.49607356, 0.49607356, 0.49607356, 0.39619377, 0.51867842,\n",
              "        0.51867842, 0.49429849, 0.42598684, 0.51867842, 0.51867842,\n",
              "        0.49429849, 0.45911926, 0.56000093, 0.56000093, 0.49429849,\n",
              "        0.39619377, 0.56000093, 0.56000093, 0.50127858, 0.41041536]),\n",
              " 'split7_test_precision': array([0.68008634, 0.68008634, 0.68008634, 0.54867965, 0.68008634,\n",
              "        0.68008634, 0.68008634, 0.82902299, 0.65610231, 0.65610231,\n",
              "        0.65610231, 0.57898551, 0.65610231, 0.65610231, 0.65610231,\n",
              "        0.82902299, 0.62806866, 0.62806866, 0.64177114, 0.57944606,\n",
              "        0.62806866, 0.62806866, 0.64177114, 0.57898551, 0.66447577,\n",
              "        0.66447577, 0.64177114, 0.57898551, 0.65877832, 0.65877832,\n",
              "        0.64177114, 0.52848837, 0.68008634, 0.68008634, 0.68008634,\n",
              "        0.82902299, 0.68008634, 0.68008634, 0.68008634, 0.49460784,\n",
              "        0.65610231, 0.65610231, 0.65610231, 0.52848837, 0.65610231,\n",
              "        0.65610231, 0.65610231, 0.57898551, 0.62806866, 0.62806866,\n",
              "        0.64177114, 0.57898551, 0.62806866, 0.62806866, 0.64177114,\n",
              "        0.5450784 , 0.66447577, 0.66447577, 0.64177114, 0.62994186,\n",
              "        0.66447577, 0.66447577, 0.64177114, 0.32808023, 0.68008634,\n",
              "        0.68008634, 0.68008634, 0.57944606, 0.68008634, 0.68008634,\n",
              "        0.68008634, 0.82902299, 0.65610231, 0.65610231, 0.65610231,\n",
              "        0.82902299, 0.65610231, 0.65610231, 0.65610231, 0.494655  ,\n",
              "        0.62806866, 0.62806866, 0.64177114, 0.54323308, 0.62806866,\n",
              "        0.62806866, 0.64177114, 0.494655  , 0.66447577, 0.66447577,\n",
              "        0.64177114, 0.52848837, 0.66447577, 0.66447577, 0.64177114,\n",
              "        0.57898551, 0.68008634, 0.68008634, 0.68008634, 0.52848837,\n",
              "        0.68008634, 0.68008634, 0.68008634, 0.47034252, 0.65610231,\n",
              "        0.65610231, 0.65610231, 0.51594575, 0.65610231, 0.65610231,\n",
              "        0.65610231, 0.52848837, 0.62806866, 0.62806866, 0.64177114,\n",
              "        0.51594575, 0.62806866, 0.62806866, 0.64177114, 0.57944606,\n",
              "        0.66447577, 0.66447577, 0.64177114, 0.52374958, 0.66447577,\n",
              "        0.66447577, 0.64177114, 0.57898551, 0.68008634, 0.68008634,\n",
              "        0.68008634, 0.47743363, 0.68008634, 0.68008634, 0.68008634,\n",
              "        0.82997118, 0.65610231, 0.65610231, 0.65610231, 0.52848837,\n",
              "        0.65610231, 0.65610231, 0.65610231, 0.32808023, 0.62806866,\n",
              "        0.62806866, 0.64177114, 0.57991202, 0.62806866, 0.62806866,\n",
              "        0.64177114, 0.54867965, 0.66447577, 0.66447577, 0.64177114,\n",
              "        0.32808023, 0.66447577, 0.66447577, 0.65096154, 0.52848837]),\n",
              " 'split7_test_recall': array([0.5349163 , 0.5349163 , 0.5349163 , 0.51328239, 0.5349163 ,\n",
              "        0.5349163 , 0.5349163 , 0.50416667, 0.54086608, 0.54086608,\n",
              "        0.54086608, 0.50396652, 0.54086608, 0.54086608, 0.54086608,\n",
              "        0.50416667, 0.54859898, 0.54859898, 0.53868268, 0.50594978,\n",
              "        0.54859898, 0.54859898, 0.53868268, 0.50396652, 0.57558224,\n",
              "        0.57558224, 0.53868268, 0.50396652, 0.57141557, 0.57141557,\n",
              "        0.53868268, 0.50178311, 0.5349163 , 0.5349163 , 0.5349163 ,\n",
              "        0.50416667, 0.5349163 , 0.5349163 , 0.5349163 , 0.49939956,\n",
              "        0.54086608, 0.54086608, 0.54086608, 0.50178311, 0.54086608,\n",
              "        0.54086608, 0.54086608, 0.50396652, 0.54859898, 0.54859898,\n",
              "        0.53868268, 0.50396652, 0.54859898, 0.54859898, 0.53868268,\n",
              "        0.51129913, 0.57558224, 0.57558224, 0.53868268, 0.50813319,\n",
              "        0.57558224, 0.57558224, 0.53868268, 0.5       , 0.5349163 ,\n",
              "        0.5349163 , 0.5349163 , 0.50594978, 0.5349163 , 0.5349163 ,\n",
              "        0.5349163 , 0.50416667, 0.54086608, 0.54086608, 0.54086608,\n",
              "        0.50416667, 0.54086608, 0.54086608, 0.54086608, 0.49959971,\n",
              "        0.54859898, 0.54859898, 0.53868268, 0.50376638, 0.54859898,\n",
              "        0.54859898, 0.53868268, 0.49959971, 0.57558224, 0.57558224,\n",
              "        0.53868268, 0.50178311, 0.57558224, 0.57558224, 0.53868268,\n",
              "        0.50396652, 0.5349163 , 0.5349163 , 0.5349163 , 0.50178311,\n",
              "        0.5349163 , 0.5349163 , 0.5349163 , 0.4974163 , 0.54086608,\n",
              "        0.54086608, 0.54086608, 0.50158297, 0.54086608, 0.54086608,\n",
              "        0.54086608, 0.50178311, 0.54859898, 0.54859898, 0.53868268,\n",
              "        0.50158297, 0.54859898, 0.54859898, 0.53868268, 0.50594978,\n",
              "        0.57558224, 0.57558224, 0.53868268, 0.5051492 , 0.57558224,\n",
              "        0.57558224, 0.53868268, 0.50396652, 0.5349163 , 0.5349163 ,\n",
              "        0.5349163 , 0.49721616, 0.5349163 , 0.5349163 , 0.5349163 ,\n",
              "        0.50833333, 0.54086608, 0.54086608, 0.54086608, 0.50178311,\n",
              "        0.54086608, 0.54086608, 0.54086608, 0.5       , 0.54859898,\n",
              "        0.54859898, 0.53868268, 0.50793304, 0.54859898, 0.54859898,\n",
              "        0.53868268, 0.51328239, 0.57558224, 0.57558224, 0.53868268,\n",
              "        0.5       , 0.57558224, 0.57558224, 0.54284934, 0.50178311]),\n",
              " 'split7_train_accuracy': array([0.66592357, 0.6656051 , 0.6656051 , 0.63757962, 0.66528662,\n",
              "        0.6656051 , 0.6656051 , 0.65828025, 0.66369427, 0.66369427,\n",
              "        0.66369427, 0.65987261, 0.66369427, 0.66369427, 0.6633758 ,\n",
              "        0.65923567, 0.66019108, 0.66019108, 0.66146497, 0.65796178,\n",
              "        0.66019108, 0.66019108, 0.66369427, 0.65732484, 0.6633758 ,\n",
              "        0.6633758 , 0.6611465 , 0.65732484, 0.66369427, 0.66305732,\n",
              "        0.66369427, 0.6589172 , 0.66592357, 0.6656051 , 0.66528662,\n",
              "        0.65987261, 0.66592357, 0.6656051 , 0.66592357, 0.65286624,\n",
              "        0.66369427, 0.66369427, 0.66369427, 0.65796178, 0.66369427,\n",
              "        0.66369427, 0.66369427, 0.65955414, 0.66019108, 0.66019108,\n",
              "        0.66146497, 0.65636943, 0.66019108, 0.66019108, 0.6633758 ,\n",
              "        0.63949045, 0.6633758 , 0.6633758 , 0.66146497, 0.66019108,\n",
              "        0.6633758 , 0.6633758 , 0.66146497, 0.65477707, 0.6656051 ,\n",
              "        0.66592357, 0.6656051 , 0.6589172 , 0.66464968, 0.6656051 ,\n",
              "        0.66496815, 0.6566879 , 0.6633758 , 0.66369427, 0.66369427,\n",
              "        0.65923567, 0.66305732, 0.66369427, 0.66369427, 0.6544586 ,\n",
              "        0.66019108, 0.66019108, 0.6611465 , 0.65796178, 0.66019108,\n",
              "        0.66019108, 0.66146497, 0.65509554, 0.6633758 , 0.6633758 ,\n",
              "        0.66369427, 0.65732484, 0.6633758 , 0.6633758 , 0.66369427,\n",
              "        0.65732484, 0.6656051 , 0.66592357, 0.66592357, 0.65700637,\n",
              "        0.66528662, 0.6656051 , 0.6656051 , 0.6522293 , 0.66369427,\n",
              "        0.66369427, 0.66369427, 0.6544586 , 0.66305732, 0.66369427,\n",
              "        0.66305732, 0.65732484, 0.66019108, 0.66019108, 0.66146497,\n",
              "        0.65509554, 0.66019108, 0.66019108, 0.66178344, 0.65828025,\n",
              "        0.6633758 , 0.6633758 , 0.6611465 , 0.64203822, 0.6633758 ,\n",
              "        0.6633758 , 0.66146497, 0.65859873, 0.66592357, 0.66528662,\n",
              "        0.6656051 , 0.65477707, 0.6656051 , 0.6656051 , 0.6656051 ,\n",
              "        0.65828025, 0.66369427, 0.66369427, 0.66369427, 0.6566879 ,\n",
              "        0.66305732, 0.66305732, 0.66369427, 0.65509554, 0.66019108,\n",
              "        0.66019108, 0.6633758 , 0.65796178, 0.66019108, 0.66019108,\n",
              "        0.6611465 , 0.63853503, 0.6633758 , 0.6633758 , 0.66146497,\n",
              "        0.6544586 , 0.6633758 , 0.6633758 , 0.66146497, 0.6566879 ]),\n",
              " 'split7_train_balanced_accuracy': array([0.52877101, 0.5285277 , 0.52874522, 0.50689899, 0.52784935,\n",
              "        0.52831018, 0.52831018, 0.50618252, 0.53576867, 0.53576867,\n",
              "        0.53576867, 0.51174947, 0.53576867, 0.53576867, 0.53509032,\n",
              "        0.50734748, 0.54331573, 0.54331573, 0.53689327, 0.5122473 ,\n",
              "        0.54331573, 0.54331573, 0.53859643, 0.50893292, 0.55531972,\n",
              "        0.55531972, 0.53664996, 0.5087154 , 0.55512799, 0.55464137,\n",
              "        0.53859643, 0.51123706, 0.52898853, 0.52874522, 0.52806687,\n",
              "        0.50848666, 0.52877101, 0.5285277 , 0.52898853, 0.50835436,\n",
              "        0.53576867, 0.53576867, 0.53576867, 0.51028962, 0.53576867,\n",
              "        0.53576867, 0.53576867, 0.51063608, 0.54331573, 0.54331573,\n",
              "        0.53689327, 0.50820299, 0.54331573, 0.54331573, 0.53835312,\n",
              "        0.5068362 , 0.55531972, 0.55531972, 0.53689327, 0.5111227 ,\n",
              "        0.55531972, 0.55531972, 0.53689327, 0.50046083, 0.5285277 ,\n",
              "        0.52898853, 0.5285277 , 0.51123706, 0.52779777, 0.5285277 ,\n",
              "        0.5282586 , 0.50344333, 0.53530784, 0.53576867, 0.53576867,\n",
              "        0.50778252, 0.53484701, 0.53576867, 0.53576867, 0.50804826,\n",
              "        0.54331573, 0.54331573, 0.53664996, 0.51094218, 0.54331573,\n",
              "        0.54331573, 0.53689327, 0.50853488, 0.55531972, 0.55531972,\n",
              "        0.53859643, 0.50936796, 0.55531972, 0.55531972, 0.53859643,\n",
              "        0.50893292, 0.5285277 , 0.52877101, 0.52877101, 0.50977721,\n",
              "        0.52828439, 0.52831018, 0.52831018, 0.50656262, 0.53576867,\n",
              "        0.53576867, 0.53576867, 0.5095709 , 0.53484701, 0.53576867,\n",
              "        0.53484701, 0.51067308, 0.54331573, 0.54331573, 0.53689327,\n",
              "        0.51027504, 0.54331573, 0.54331573, 0.53713658, 0.51140301,\n",
              "        0.55531972, 0.55531972, 0.53664996, 0.50834763, 0.55531972,\n",
              "        0.55531972, 0.53689327, 0.50838351, 0.52898853, 0.52828439,\n",
              "        0.52831018, 0.5128595 , 0.52831018, 0.5285277 , 0.5285277 ,\n",
              "        0.50727012, 0.53576867, 0.53576867, 0.53576867, 0.5095339 ,\n",
              "        0.53484701, 0.53484701, 0.53576867, 0.5013567 , 0.54331573,\n",
              "        0.54331573, 0.53835312, 0.5122473 , 0.54331573, 0.54331573,\n",
              "        0.53664996, 0.50741139, 0.55531972, 0.55531972, 0.53689327,\n",
              "        0.5       , 0.55531972, 0.55531972, 0.53689327, 0.51018646]),\n",
              " 'split7_train_f1': array([0.47069164, 0.47051149, 0.47115866, 0.45422165, 0.46903128,\n",
              "        0.46986205, 0.46986205, 0.40996783, 0.49354839, 0.49354839,\n",
              "        0.49354839, 0.42690507, 0.49354839, 0.49354839, 0.49222118,\n",
              "        0.41203688, 0.51572732, 0.51572732, 0.49929428, 0.43294195,\n",
              "        0.51572732, 0.51572732, 0.50071403, 0.42261222, 0.53773564,\n",
              "        0.53773564, 0.49909173, 0.42182058, 0.53711599, 0.53666388,\n",
              "        0.50071403, 0.42725234, 0.47133932, 0.47115866, 0.46968247,\n",
              "        0.41480228, 0.47069164, 0.47051149, 0.47133932, 0.43053059,\n",
              "        0.49354839, 0.49354839, 0.49354839, 0.42603833, 0.49354839,\n",
              "        0.49354839, 0.49354839, 0.42360281, 0.51572732, 0.51572732,\n",
              "        0.49929428, 0.42218764, 0.51572732, 0.51572732, 0.500511  ,\n",
              "        0.45096077, 0.53773564, 0.53773564, 0.49929428, 0.42388581,\n",
              "        0.53773564, 0.53773564, 0.49929428, 0.39656964, 0.47051149,\n",
              "        0.47133932, 0.47051149, 0.42725234, 0.46997134, 0.47051149,\n",
              "        0.47079748, 0.40337895, 0.49278672, 0.49354839, 0.49354839,\n",
              "        0.41370641, 0.49202406, 0.49354839, 0.49354839, 0.42598773,\n",
              "        0.51572732, 0.51572732, 0.49909173, 0.42836512, 0.51572732,\n",
              "        0.51572732, 0.49929428, 0.42627947, 0.53773564, 0.53773564,\n",
              "        0.50071403, 0.42418667, 0.53773564, 0.53773564, 0.50071403,\n",
              "        0.42261222, 0.47051149, 0.47069164, 0.47069164, 0.42638129,\n",
              "        0.47033139, 0.46986205, 0.46986205, 0.42572673, 0.49354839,\n",
              "        0.49354839, 0.49354839, 0.43128391, 0.49202406, 0.49354839,\n",
              "        0.49202406, 0.42884036, 0.51572732, 0.51572732, 0.49929428,\n",
              "        0.43233223, 0.51572732, 0.51572732, 0.4994969 , 0.42928229,\n",
              "        0.53773564, 0.53773564, 0.49909173, 0.45108447, 0.53773564,\n",
              "        0.53773564, 0.49929428, 0.4175526 , 0.47133932, 0.47033139,\n",
              "        0.46986205, 0.44164042, 0.46986205, 0.47051149, 0.47051149,\n",
              "        0.41413744, 0.49354839, 0.49354839, 0.49354839, 0.42623612,\n",
              "        0.49202406, 0.49202406, 0.49354839, 0.39931099, 0.51572732,\n",
              "        0.51572732, 0.500511  , 0.43294195, 0.51572732, 0.51572732,\n",
              "        0.49909173, 0.45414096, 0.53773564, 0.53773564, 0.49929428,\n",
              "        0.39557267, 0.53773564, 0.53773564, 0.49929428, 0.42854577]),\n",
              " 'split7_train_precision': array([0.6448683 , 0.64272684, 0.64200523, 0.52253268, 0.64205351,\n",
              "        0.64346968, 0.64346968, 0.74530216, 0.61682353, 0.61682353,\n",
              "        0.61682353, 0.66012065, 0.61682353, 0.61682353, 0.61597021,\n",
              "        0.77626941, 0.60144958, 0.60144958, 0.60687336, 0.61578285,\n",
              "        0.60144958, 0.60144958, 0.61452665, 0.62173692, 0.60840696,\n",
              "        0.60840696, 0.6058107 , 0.62335041, 0.60913864, 0.60773295,\n",
              "        0.61452665, 0.64016313, 0.64411371, 0.64200523, 0.64132281,\n",
              "        0.76394527, 0.6448683 , 0.64272684, 0.64411371, 0.56573538,\n",
              "        0.61682353, 0.61682353, 0.61682353, 0.62621095, 0.61682353,\n",
              "        0.61682353, 0.61682353, 0.66662685, 0.60144958, 0.60144958,\n",
              "        0.60687336, 0.60590374, 0.60144958, 0.60144958, 0.6134097 ,\n",
              "        0.52423773, 0.60840696, 0.60840696, 0.60687336, 0.68205309,\n",
              "        0.60840696, 0.60840696, 0.60687336, 0.82733355, 0.64272684,\n",
              "        0.64411371, 0.64272684, 0.64016313, 0.63646692, 0.64272684,\n",
              "        0.63786867, 0.77245466, 0.61577138, 0.61682353, 0.61682353,\n",
              "        0.74204572, 0.61471127, 0.61682353, 0.61682353, 0.57909328,\n",
              "        0.60144958, 0.60144958, 0.6058107 , 0.62206379, 0.60144958,\n",
              "        0.60144958, 0.60687336, 0.58614914, 0.60840696, 0.60840696,\n",
              "        0.61452665, 0.61886064, 0.60840696, 0.60840696, 0.61452665,\n",
              "        0.62173692, 0.64272684, 0.6448683 , 0.6448683 , 0.61073635,\n",
              "        0.64061315, 0.64346968, 0.64346968, 0.55766934, 0.61682353,\n",
              "        0.61682353, 0.61682353, 0.57945609, 0.61471127, 0.61682353,\n",
              "        0.61471127, 0.61230586, 0.60144958, 0.60144958, 0.60687336,\n",
              "        0.58530174, 0.60144958, 0.60144958, 0.60794352, 0.62531792,\n",
              "        0.60840696, 0.60840696, 0.6058107 , 0.53121797, 0.60840696,\n",
              "        0.60840696, 0.60687336, 0.67200368, 0.64411371, 0.64061315,\n",
              "        0.64346968, 0.58242185, 0.64346968, 0.64272684, 0.64272684,\n",
              "        0.68603103, 0.61682353, 0.61682353, 0.61682353, 0.60635397,\n",
              "        0.61471127, 0.61471127, 0.61682353, 0.66087003, 0.60144958,\n",
              "        0.60144958, 0.6134097 , 0.61578285, 0.60144958, 0.60144958,\n",
              "        0.6058107 , 0.52468997, 0.60840696, 0.60840696, 0.60687336,\n",
              "        0.3272293 , 0.60840696, 0.60840696, 0.60687336, 0.60423407]),\n",
              " 'split7_train_recall': array([0.52877101, 0.5285277 , 0.52874522, 0.50689899, 0.52784935,\n",
              "        0.52831018, 0.52831018, 0.50618252, 0.53576867, 0.53576867,\n",
              "        0.53576867, 0.51174947, 0.53576867, 0.53576867, 0.53509032,\n",
              "        0.50734748, 0.54331573, 0.54331573, 0.53689327, 0.5122473 ,\n",
              "        0.54331573, 0.54331573, 0.53859643, 0.50893292, 0.55531972,\n",
              "        0.55531972, 0.53664996, 0.5087154 , 0.55512799, 0.55464137,\n",
              "        0.53859643, 0.51123706, 0.52898853, 0.52874522, 0.52806687,\n",
              "        0.50848666, 0.52877101, 0.5285277 , 0.52898853, 0.50835436,\n",
              "        0.53576867, 0.53576867, 0.53576867, 0.51028962, 0.53576867,\n",
              "        0.53576867, 0.53576867, 0.51063608, 0.54331573, 0.54331573,\n",
              "        0.53689327, 0.50820299, 0.54331573, 0.54331573, 0.53835312,\n",
              "        0.5068362 , 0.55531972, 0.55531972, 0.53689327, 0.5111227 ,\n",
              "        0.55531972, 0.55531972, 0.53689327, 0.50046083, 0.5285277 ,\n",
              "        0.52898853, 0.5285277 , 0.51123706, 0.52779777, 0.5285277 ,\n",
              "        0.5282586 , 0.50344333, 0.53530784, 0.53576867, 0.53576867,\n",
              "        0.50778252, 0.53484701, 0.53576867, 0.53576867, 0.50804826,\n",
              "        0.54331573, 0.54331573, 0.53664996, 0.51094218, 0.54331573,\n",
              "        0.54331573, 0.53689327, 0.50853488, 0.55531972, 0.55531972,\n",
              "        0.53859643, 0.50936796, 0.55531972, 0.55531972, 0.53859643,\n",
              "        0.50893292, 0.5285277 , 0.52877101, 0.52877101, 0.50977721,\n",
              "        0.52828439, 0.52831018, 0.52831018, 0.50656262, 0.53576867,\n",
              "        0.53576867, 0.53576867, 0.5095709 , 0.53484701, 0.53576867,\n",
              "        0.53484701, 0.51067308, 0.54331573, 0.54331573, 0.53689327,\n",
              "        0.51027504, 0.54331573, 0.54331573, 0.53713658, 0.51140301,\n",
              "        0.55531972, 0.55531972, 0.53664996, 0.50834763, 0.55531972,\n",
              "        0.55531972, 0.53689327, 0.50838351, 0.52898853, 0.52828439,\n",
              "        0.52831018, 0.5128595 , 0.52831018, 0.5285277 , 0.5285277 ,\n",
              "        0.50727012, 0.53576867, 0.53576867, 0.53576867, 0.5095339 ,\n",
              "        0.53484701, 0.53484701, 0.53576867, 0.5013567 , 0.54331573,\n",
              "        0.54331573, 0.53835312, 0.5122473 , 0.54331573, 0.54331573,\n",
              "        0.53664996, 0.50741139, 0.55531972, 0.55531972, 0.53689327,\n",
              "        0.5       , 0.55531972, 0.55531972, 0.53689327, 0.51018646]),\n",
              " 'split8_test_accuracy': array([0.66475645, 0.66475645, 0.66475645, 0.65902579, 0.66762178,\n",
              "        0.66762178, 0.66475645, 0.66475645, 0.65902579, 0.65902579,\n",
              "        0.65902579, 0.62464183, 0.65902579, 0.65902579, 0.65902579,\n",
              "        0.65329513, 0.67335244, 0.67335244, 0.66762178, 0.65329513,\n",
              "        0.67335244, 0.67335244, 0.66762178, 0.63896848, 0.68481375,\n",
              "        0.68481375, 0.66762178, 0.65329513, 0.68481375, 0.68481375,\n",
              "        0.66762178, 0.65616046, 0.66475645, 0.66475645, 0.66475645,\n",
              "        0.65902579, 0.66762178, 0.66762178, 0.66475645, 0.65329513,\n",
              "        0.65902579, 0.65902579, 0.65902579, 0.63037249, 0.65902579,\n",
              "        0.65902579, 0.65902579, 0.63896848, 0.67335244, 0.67335244,\n",
              "        0.66762178, 0.66189112, 0.67335244, 0.67335244, 0.66762178,\n",
              "        0.65329513, 0.68481375, 0.68481375, 0.66762178, 0.65329513,\n",
              "        0.68481375, 0.68481375, 0.66762178, 0.65616046, 0.66762178,\n",
              "        0.66762178, 0.66762178, 0.65329513, 0.66762178, 0.66475645,\n",
              "        0.66475645, 0.65329513, 0.65902579, 0.65902579, 0.65902579,\n",
              "        0.65902579, 0.65902579, 0.65902579, 0.65902579, 0.63896848,\n",
              "        0.67335244, 0.67335244, 0.66762178, 0.66189112, 0.67335244,\n",
              "        0.67335244, 0.66762178, 0.65329513, 0.68481375, 0.68481375,\n",
              "        0.66762178, 0.65329513, 0.68481375, 0.68481375, 0.66762178,\n",
              "        0.65329513, 0.66762178, 0.66475645, 0.66475645, 0.65616046,\n",
              "        0.66475645, 0.66762178, 0.66475645, 0.63323782, 0.65902579,\n",
              "        0.65902579, 0.65902579, 0.65329513, 0.65902579, 0.65902579,\n",
              "        0.65902579, 0.65616046, 0.67335244, 0.67335244, 0.66762178,\n",
              "        0.65902579, 0.67335244, 0.67335244, 0.66762178, 0.65329513,\n",
              "        0.68481375, 0.68481375, 0.66762178, 0.65902579, 0.68481375,\n",
              "        0.68481375, 0.66762178, 0.65329513, 0.66475645, 0.66475645,\n",
              "        0.66475645, 0.65329513, 0.66762178, 0.66475645, 0.66475645,\n",
              "        0.65329513, 0.65902579, 0.65902579, 0.65902579, 0.6504298 ,\n",
              "        0.65902579, 0.65902579, 0.65902579, 0.65902579, 0.67335244,\n",
              "        0.67335244, 0.66762178, 0.65616046, 0.67335244, 0.67335244,\n",
              "        0.66762178, 0.65329513, 0.68481375, 0.68481375, 0.66762178,\n",
              "        0.65616046, 0.68481375, 0.68481375, 0.66762178, 0.6504298 ]),\n",
              " 'split8_test_balanced_accuracy': array([0.5224163 , 0.5224163 , 0.5224163 , 0.50416667, 0.52459971,\n",
              "        0.52459971, 0.5224163 , 0.51646652, 0.52201601, 0.52201601,\n",
              "        0.52201601, 0.49581514, 0.52201601, 0.52201601, 0.52201601,\n",
              "        0.50178311, 0.55276565, 0.55276565, 0.53649927, 0.50178311,\n",
              "        0.55276565, 0.55276565, 0.53649927, 0.51069869, 0.57339884,\n",
              "        0.57339884, 0.53649927, 0.50178311, 0.57339884, 0.57339884,\n",
              "        0.53649927, 0.5       , 0.5224163 , 0.5224163 , 0.5224163 ,\n",
              "        0.50416667, 0.52459971, 0.52459971, 0.5224163 , 0.5077329 ,\n",
              "        0.52201601, 0.52201601, 0.52201601, 0.50613173, 0.52201601,\n",
              "        0.52201601, 0.52201601, 0.50276565, 0.55276565, 0.55276565,\n",
              "        0.53649927, 0.50833333, 0.55276565, 0.55276565, 0.53649927,\n",
              "        0.50178311, 0.57339884, 0.57339884, 0.53649927, 0.5077329 ,\n",
              "        0.57339884, 0.57339884, 0.53649927, 0.5       , 0.52459971,\n",
              "        0.52459971, 0.52459971, 0.50178311, 0.52459971, 0.5224163 ,\n",
              "        0.5224163 , 0.50178311, 0.52201601, 0.52201601, 0.52201601,\n",
              "        0.50614993, 0.52201601, 0.52201601, 0.52201601, 0.50673217,\n",
              "        0.55276565, 0.55276565, 0.53649927, 0.50833333, 0.55276565,\n",
              "        0.55276565, 0.53649927, 0.50178311, 0.57339884, 0.57339884,\n",
              "        0.53649927, 0.50178311, 0.57339884, 0.57339884, 0.53649927,\n",
              "        0.5077329 , 0.52459971, 0.5224163 , 0.5224163 , 0.50396652,\n",
              "        0.5224163 , 0.52459971, 0.5224163 , 0.5102984 , 0.52201601,\n",
              "        0.52201601, 0.52201601, 0.50178311, 0.52201601, 0.52201601,\n",
              "        0.52201601, 0.50594978, 0.55276565, 0.55276565, 0.53649927,\n",
              "        0.50614993, 0.55276565, 0.55276565, 0.53649927, 0.5077329 ,\n",
              "        0.57339884, 0.57339884, 0.53649927, 0.50614993, 0.57339884,\n",
              "        0.57339884, 0.53649927, 0.50971616, 0.5224163 , 0.5224163 ,\n",
              "        0.5224163 , 0.51169942, 0.52459971, 0.5224163 , 0.5224163 ,\n",
              "        0.50178311, 0.52201601, 0.52201601, 0.52201601, 0.50356623,\n",
              "        0.52201601, 0.52201601, 0.52201601, 0.50614993, 0.55276565,\n",
              "        0.55276565, 0.53649927, 0.50594978, 0.55276565, 0.55276565,\n",
              "        0.53649927, 0.50178311, 0.57339884, 0.57339884, 0.53649927,\n",
              "        0.50396652, 0.57339884, 0.57339884, 0.53649927, 0.51149927]),\n",
              " 'split8_test_f1': array([0.45661055, 0.45661055, 0.45661055, 0.40514488, 0.45813256,\n",
              "        0.45813256, 0.45661055, 0.43691824, 0.46550237, 0.46550237,\n",
              "        0.46550237, 0.44648111, 0.46550237, 0.46550237, 0.46550237,\n",
              "        0.41041536, 0.52509072, 0.52509072, 0.49252908, 0.41041536,\n",
              "        0.52509072, 0.52509072, 0.49252908, 0.4650365 , 0.55788189,\n",
              "        0.55788189, 0.49252908, 0.41041536, 0.55788189, 0.55788189,\n",
              "        0.49252908, 0.39619377, 0.45661055, 0.45661055, 0.45661055,\n",
              "        0.40514488, 0.45813256, 0.45813256, 0.45661055, 0.431466  ,\n",
              "        0.46550237, 0.46550237, 0.46550237, 0.46508644, 0.46550237,\n",
              "        0.46550237, 0.46550237, 0.44302868, 0.52509072, 0.52509072,\n",
              "        0.49252908, 0.41396289, 0.52509072, 0.52509072, 0.49252908,\n",
              "        0.41041536, 0.55788189, 0.55788189, 0.49252908, 0.431466  ,\n",
              "        0.55788189, 0.55788189, 0.49252908, 0.39619377, 0.45813256,\n",
              "        0.45813256, 0.45813256, 0.41041536, 0.45813256, 0.45661055,\n",
              "        0.45661055, 0.41041536, 0.46550237, 0.46550237, 0.46550237,\n",
              "        0.4127819 , 0.46550237, 0.46550237, 0.46550237, 0.45436272,\n",
              "        0.52509072, 0.52509072, 0.49252908, 0.41396289, 0.52509072,\n",
              "        0.52509072, 0.49252908, 0.41041536, 0.55788189, 0.55788189,\n",
              "        0.49252908, 0.41041536, 0.55788189, 0.55788189, 0.49252908,\n",
              "        0.431466  , 0.45813256, 0.45661055, 0.45661055, 0.41159942,\n",
              "        0.45661055, 0.45813256, 0.45661055, 0.47166241, 0.46550237,\n",
              "        0.46550237, 0.46550237, 0.41041536, 0.46550237, 0.46550237,\n",
              "        0.46550237, 0.41891442, 0.52509072, 0.52509072, 0.49252908,\n",
              "        0.4127819 , 0.52509072, 0.52509072, 0.49252908, 0.431466  ,\n",
              "        0.55788189, 0.55788189, 0.49252908, 0.4127819 , 0.55788189,\n",
              "        0.55788189, 0.49252908, 0.43803314, 0.45661055, 0.45661055,\n",
              "        0.45661055, 0.44439182, 0.45813256, 0.45661055, 0.45661055,\n",
              "        0.41041536, 0.46550237, 0.46550237, 0.46550237, 0.42337486,\n",
              "        0.46550237, 0.46550237, 0.46550237, 0.4127819 , 0.52509072,\n",
              "        0.52509072, 0.49252908, 0.41891442, 0.52509072, 0.52509072,\n",
              "        0.49252908, 0.41041536, 0.55788189, 0.55788189, 0.49252908,\n",
              "        0.41159942, 0.55788189, 0.55788189, 0.49252908, 0.44904244]),\n",
              " 'split8_test_precision': array([0.64102564, 0.64102564, 0.64102564, 0.82902299, 0.66716123,\n",
              "        0.66716123, 0.64102564, 0.6890142 , 0.59649123, 0.59649123,\n",
              "        0.59649123, 0.48833435, 0.59649123, 0.59649123, 0.59649123,\n",
              "        0.52848837, 0.63538749, 0.63538749, 0.62858974, 0.52848837,\n",
              "        0.63538749, 0.63538749, 0.62858974, 0.530721  , 0.65642935,\n",
              "        0.65642935, 0.62858974, 0.52848837, 0.65642935, 0.65642935,\n",
              "        0.62858974, 0.32808023, 0.64102564, 0.64102564, 0.64102564,\n",
              "        0.82902299, 0.66716123, 0.66716123, 0.64102564, 0.55715438,\n",
              "        0.59649123, 0.59649123, 0.59649123, 0.51533212, 0.59649123,\n",
              "        0.59649123, 0.59649123, 0.51056436, 0.63538749, 0.63538749,\n",
              "        0.62858974, 0.82997118, 0.63538749, 0.63538749, 0.62858974,\n",
              "        0.52848837, 0.65642935, 0.65642935, 0.62858974, 0.55715438,\n",
              "        0.65642935, 0.65642935, 0.62858974, 0.32808023, 0.66716123,\n",
              "        0.66716123, 0.66716123, 0.52848837, 0.66716123, 0.64102564,\n",
              "        0.64102564, 0.52848837, 0.59649123, 0.59649123, 0.59649123,\n",
              "        0.6628131 , 0.59649123, 0.59649123, 0.59649123, 0.52202905,\n",
              "        0.63538749, 0.63538749, 0.62858974, 0.82997118, 0.63538749,\n",
              "        0.63538749, 0.62858974, 0.52848837, 0.65642935, 0.65642935,\n",
              "        0.62858974, 0.52848837, 0.65642935, 0.65642935, 0.62858974,\n",
              "        0.55715438, 0.66716123, 0.64102564, 0.64102564, 0.57898551,\n",
              "        0.64102564, 0.66716123, 0.64102564, 0.52511537, 0.59649123,\n",
              "        0.59649123, 0.59649123, 0.52848837, 0.59649123, 0.59649123,\n",
              "        0.59649123, 0.57944606, 0.63538749, 0.63538749, 0.62858974,\n",
              "        0.6628131 , 0.63538749, 0.63538749, 0.62858974, 0.55715438,\n",
              "        0.65642935, 0.65642935, 0.62858974, 0.6628131 , 0.65642935,\n",
              "        0.65642935, 0.62858974, 0.56112637, 0.64102564, 0.64102564,\n",
              "        0.64102564, 0.56417166, 0.66716123, 0.64102564, 0.64102564,\n",
              "        0.52848837, 0.59649123, 0.59649123, 0.59649123, 0.52890855,\n",
              "        0.59649123, 0.59649123, 0.59649123, 0.6628131 , 0.63538749,\n",
              "        0.63538749, 0.62858974, 0.57944606, 0.63538749, 0.63538749,\n",
              "        0.62858974, 0.52848837, 0.65642935, 0.65642935, 0.62858974,\n",
              "        0.57898551, 0.65642935, 0.65642935, 0.62858974, 0.55303793]),\n",
              " 'split8_test_recall': array([0.5224163 , 0.5224163 , 0.5224163 , 0.50416667, 0.52459971,\n",
              "        0.52459971, 0.5224163 , 0.51646652, 0.52201601, 0.52201601,\n",
              "        0.52201601, 0.49581514, 0.52201601, 0.52201601, 0.52201601,\n",
              "        0.50178311, 0.55276565, 0.55276565, 0.53649927, 0.50178311,\n",
              "        0.55276565, 0.55276565, 0.53649927, 0.51069869, 0.57339884,\n",
              "        0.57339884, 0.53649927, 0.50178311, 0.57339884, 0.57339884,\n",
              "        0.53649927, 0.5       , 0.5224163 , 0.5224163 , 0.5224163 ,\n",
              "        0.50416667, 0.52459971, 0.52459971, 0.5224163 , 0.5077329 ,\n",
              "        0.52201601, 0.52201601, 0.52201601, 0.50613173, 0.52201601,\n",
              "        0.52201601, 0.52201601, 0.50276565, 0.55276565, 0.55276565,\n",
              "        0.53649927, 0.50833333, 0.55276565, 0.55276565, 0.53649927,\n",
              "        0.50178311, 0.57339884, 0.57339884, 0.53649927, 0.5077329 ,\n",
              "        0.57339884, 0.57339884, 0.53649927, 0.5       , 0.52459971,\n",
              "        0.52459971, 0.52459971, 0.50178311, 0.52459971, 0.5224163 ,\n",
              "        0.5224163 , 0.50178311, 0.52201601, 0.52201601, 0.52201601,\n",
              "        0.50614993, 0.52201601, 0.52201601, 0.52201601, 0.50673217,\n",
              "        0.55276565, 0.55276565, 0.53649927, 0.50833333, 0.55276565,\n",
              "        0.55276565, 0.53649927, 0.50178311, 0.57339884, 0.57339884,\n",
              "        0.53649927, 0.50178311, 0.57339884, 0.57339884, 0.53649927,\n",
              "        0.5077329 , 0.52459971, 0.5224163 , 0.5224163 , 0.50396652,\n",
              "        0.5224163 , 0.52459971, 0.5224163 , 0.5102984 , 0.52201601,\n",
              "        0.52201601, 0.52201601, 0.50178311, 0.52201601, 0.52201601,\n",
              "        0.52201601, 0.50594978, 0.55276565, 0.55276565, 0.53649927,\n",
              "        0.50614993, 0.55276565, 0.55276565, 0.53649927, 0.5077329 ,\n",
              "        0.57339884, 0.57339884, 0.53649927, 0.50614993, 0.57339884,\n",
              "        0.57339884, 0.53649927, 0.50971616, 0.5224163 , 0.5224163 ,\n",
              "        0.5224163 , 0.51169942, 0.52459971, 0.5224163 , 0.5224163 ,\n",
              "        0.50178311, 0.52201601, 0.52201601, 0.52201601, 0.50356623,\n",
              "        0.52201601, 0.52201601, 0.52201601, 0.50614993, 0.55276565,\n",
              "        0.55276565, 0.53649927, 0.50594978, 0.55276565, 0.55276565,\n",
              "        0.53649927, 0.50178311, 0.57339884, 0.57339884, 0.53649927,\n",
              "        0.50396652, 0.57339884, 0.57339884, 0.53649927, 0.51149927]),\n",
              " 'split8_train_accuracy': array([0.66464968, 0.66528662, 0.66464968, 0.65605096, 0.66496815,\n",
              "        0.66496815, 0.66433121, 0.6589172 , 0.66273885, 0.66273885,\n",
              "        0.66273885, 0.64171975, 0.66273885, 0.66273885, 0.66273885,\n",
              "        0.65859873, 0.65923567, 0.6589172 , 0.66496815, 0.65764331,\n",
              "        0.6589172 , 0.6589172 , 0.66464968, 0.63821656, 0.65955414,\n",
              "        0.65955414, 0.66401274, 0.65764331, 0.6589172 , 0.6589172 ,\n",
              "        0.66464968, 0.65509554, 0.66433121, 0.66433121, 0.66433121,\n",
              "        0.65636943, 0.66496815, 0.66433121, 0.66464968, 0.65414013,\n",
              "        0.66273885, 0.66273885, 0.66273885, 0.6388535 , 0.66273885,\n",
              "        0.66273885, 0.66273885, 0.63821656, 0.65923567, 0.65923567,\n",
              "        0.66528662, 0.65636943, 0.65923567, 0.65923567, 0.66433121,\n",
              "        0.65732484, 0.65955414, 0.65955414, 0.66433121, 0.65286624,\n",
              "        0.65955414, 0.65955414, 0.66464968, 0.65509554, 0.66496815,\n",
              "        0.66496815, 0.66496815, 0.65859873, 0.66496815, 0.66464968,\n",
              "        0.66496815, 0.65923567, 0.66273885, 0.66273885, 0.66273885,\n",
              "        0.65859873, 0.66273885, 0.66273885, 0.66273885, 0.63471338,\n",
              "        0.6589172 , 0.65923567, 0.66401274, 0.6566879 , 0.6589172 ,\n",
              "        0.65923567, 0.66464968, 0.65796178, 0.65955414, 0.65955414,\n",
              "        0.66496815, 0.65955414, 0.65955414, 0.65955414, 0.66433121,\n",
              "        0.65318471, 0.66433121, 0.66464968, 0.66528662, 0.65859873,\n",
              "        0.66528662, 0.66464968, 0.66464968, 0.63694268, 0.66273885,\n",
              "        0.66273885, 0.66273885, 0.65828025, 0.66273885, 0.66273885,\n",
              "        0.66273885, 0.65764331, 0.65923567, 0.65923567, 0.66464968,\n",
              "        0.65859873, 0.6589172 , 0.65923567, 0.66433121, 0.65127389,\n",
              "        0.65955414, 0.65955414, 0.66433121, 0.65859873, 0.65955414,\n",
              "        0.65955414, 0.66464968, 0.65031847, 0.66433121, 0.66496815,\n",
              "        0.66464968, 0.65318471, 0.66496815, 0.66496815, 0.66433121,\n",
              "        0.6589172 , 0.66273885, 0.66273885, 0.66273885, 0.65191083,\n",
              "        0.66273885, 0.66273885, 0.66273885, 0.65859873, 0.65923567,\n",
              "        0.6589172 , 0.66464968, 0.66050955, 0.6589172 , 0.65923567,\n",
              "        0.66496815, 0.65796178, 0.65955414, 0.65955414, 0.66433121,\n",
              "        0.65796178, 0.65955414, 0.65955414, 0.66464968, 0.64904459]),\n",
              " 'split8_train_balanced_accuracy': array([0.52497001, 0.52545662, 0.52497001, 0.50252167, 0.5249958 ,\n",
              "        0.5249958 , 0.52450918, 0.51254219, 0.53134089, 0.53134089,\n",
              "        0.53134089, 0.50571159, 0.53134089, 0.53134089, 0.53134089,\n",
              "        0.51055871, 0.54062812, 0.54038481, 0.53608934, 0.50939375,\n",
              "        0.53994977, 0.53973225, 0.53562851, 0.50586296, 0.54739704,\n",
              "        0.54739704, 0.53514189, 0.50917623, 0.54625786, 0.54625786,\n",
              "        0.53562851, 0.50113918, 0.52450918, 0.52450918, 0.52450918,\n",
              "        0.5029825 , 0.5249958 , 0.52450918, 0.52497001, 0.50802247,\n",
              "        0.53134089, 0.53134089, 0.53134089, 0.50678462, 0.53134089,\n",
              "        0.53134089, 0.53134089, 0.49803222, 0.54062812, 0.54062812,\n",
              "        0.53655017, 0.50385258, 0.54062812, 0.54062812, 0.5353852 ,\n",
              "        0.50915044, 0.54739704, 0.54739704, 0.53560272, 0.50726675,\n",
              "        0.54739704, 0.54739704, 0.53584603, 0.50092166, 0.5249958 ,\n",
              "        0.5249958 , 0.5249958 , 0.50881855, 0.5249958 , 0.52497001,\n",
              "        0.5249958 , 0.51082781, 0.53134089, 0.53134089, 0.53134089,\n",
              "        0.50773095, 0.53134089, 0.53134089, 0.53134089, 0.49840111,\n",
              "        0.54038481, 0.54062812, 0.53514189, 0.50387837, 0.54016729,\n",
              "        0.54062812, 0.53562851, 0.5100721 , 0.54739704, 0.54739704,\n",
              "        0.53630686, 0.51063608, 0.54739704, 0.54739704, 0.5353852 ,\n",
              "        0.50729254, 0.52450918, 0.52497001, 0.52545662, 0.50838351,\n",
              "        0.52545662, 0.52497001, 0.52497001, 0.50749997, 0.53134089,\n",
              "        0.53134089, 0.53134089, 0.5103154 , 0.53134089, 0.53134089,\n",
              "        0.53134089, 0.51004631, 0.54062812, 0.54062812, 0.53584603,\n",
              "        0.50751343, 0.54038481, 0.54062812, 0.5353852 , 0.50583269,\n",
              "        0.54739704, 0.54739704, 0.5353852 , 0.50751343, 0.54739704,\n",
              "        0.54739704, 0.53584603, 0.50532028, 0.52450918, 0.5249958 ,\n",
              "        0.52497001, 0.51055535, 0.5249958 , 0.5249958 , 0.52450918,\n",
              "        0.51014946, 0.53134089, 0.53134089, 0.53134089, 0.50566675,\n",
              "        0.53134089, 0.53134089, 0.53134089, 0.50751343, 0.54062812,\n",
              "        0.54016729, 0.53562851, 0.51245361, 0.54038481, 0.54062812,\n",
              "        0.53630686, 0.5100721 , 0.54739704, 0.54739704, 0.5353852 ,\n",
              "        0.50746185, 0.54739704, 0.54739704, 0.53562851, 0.50826241]),\n",
              " 'split8_train_f1': array([0.4613687 , 0.46171478, 0.4613687 , 0.40140815, 0.46086291,\n",
              "        0.46086291, 0.46051804, 0.43187583, 0.48312548, 0.48312548,\n",
              "        0.48312548, 0.44382963, 0.48312548, 0.48312548, 0.48312548,\n",
              "        0.42554441, 0.51071111, 0.51050041, 0.49263827, 0.42354318,\n",
              "        0.50951175, 0.50901506, 0.49187027, 0.45026927, 0.52504748,\n",
              "        0.52504748, 0.49147721, 0.42275375, 0.52325604, 0.52325604,\n",
              "        0.49187027, 0.39843966, 0.46051804, 0.46051804, 0.46051804,\n",
              "        0.40239437, 0.46086291, 0.46051804, 0.4613687 , 0.42660611,\n",
              "        0.48312548, 0.48312548, 0.48312548, 0.45185716, 0.48312548,\n",
              "        0.48312548, 0.48312548, 0.42644123, 0.51071111, 0.51071111,\n",
              "        0.49340526, 0.40582023, 0.51071111, 0.51071111, 0.49167371,\n",
              "        0.42340091, 0.52504748, 0.52504748, 0.49224427, 0.42677852,\n",
              "        0.52504748, 0.52504748, 0.49244124, 0.39756494, 0.46086291,\n",
              "        0.46086291, 0.46086291, 0.41917512, 0.46086291, 0.4613687 ,\n",
              "        0.46086291, 0.42504463, 0.48312548, 0.48312548, 0.48312548,\n",
              "        0.41509566, 0.48312548, 0.48312548, 0.48312548, 0.43428658,\n",
              "        0.51050041, 0.51071111, 0.49147721, 0.40509357, 0.51000686,\n",
              "        0.51071111, 0.49187027, 0.42525695, 0.52504748, 0.52504748,\n",
              "        0.49320775, 0.42360281, 0.52504748, 0.52504748, 0.49167371,\n",
              "        0.42616641, 0.46051804, 0.4613687 , 0.46171478, 0.4175526 ,\n",
              "        0.46171478, 0.4613687 , 0.4613687 , 0.45687401, 0.48312548,\n",
              "        0.48312548, 0.48312548, 0.42540068, 0.48312548, 0.48312548,\n",
              "        0.48312548, 0.42589388, 0.51071111, 0.51071111, 0.49244124,\n",
              "        0.41427044, 0.51050041, 0.51071111, 0.49167371, 0.42528707,\n",
              "        0.52504748, 0.52504748, 0.49167371, 0.41427044, 0.52504748,\n",
              "        0.52504748, 0.49244124, 0.42560057, 0.46051804, 0.46086291,\n",
              "        0.4613687 , 0.43727182, 0.46086291, 0.46086291, 0.46051804,\n",
              "        0.4233198 , 0.48312548, 0.48312548, 0.48312548, 0.42329372,\n",
              "        0.48312548, 0.48312548, 0.48312548, 0.41427044, 0.51071111,\n",
              "        0.51000686, 0.49187027, 0.42797828, 0.51050041, 0.51071111,\n",
              "        0.49320775, 0.42525695, 0.52504748, 0.52504748, 0.49167371,\n",
              "        0.41564855, 0.52504748, 0.52504748, 0.49187027, 0.43806773]),\n",
              " 'split8_train_precision': array([0.64660421, 0.6517712 , 0.64660421, 0.75637226, 0.65018553,\n",
              "        0.65018553, 0.64500166, 0.63013029, 0.61667201, 0.61667201,\n",
              "        0.61667201, 0.52365535, 0.61667201, 0.61667201, 0.61667201,\n",
              "        0.63875055, 0.59901626, 0.59815624, 0.62268725, 0.62568709,\n",
              "        0.59816552, 0.59817154, 0.62161964, 0.52043539, 0.59985789,\n",
              "        0.59985789, 0.61897857, 0.6274163 , 0.59834096, 0.59834096,\n",
              "        0.62161964, 0.70248724, 0.64500166, 0.64500166, 0.64500166,\n",
              "        0.76540549, 0.65018553, 0.64500166, 0.64660421, 0.57584238,\n",
              "        0.61667201, 0.61667201, 0.61667201, 0.52344989, 0.61667201,\n",
              "        0.61667201, 0.61667201, 0.49002775, 0.59901626, 0.59901626,\n",
              "        0.62374652, 0.67185499, 0.59901626, 0.59901626, 0.6202935 ,\n",
              "        0.62024459, 0.59985789, 0.59985789, 0.62004897, 0.563857  ,\n",
              "        0.59985789, 0.59985789, 0.62136254, 0.82743786, 0.65018553,\n",
              "        0.65018553, 0.65018553, 0.66258196, 0.65018553, 0.64660421,\n",
              "        0.65018553, 0.65324777, 0.61667201, 0.61667201, 0.61667201,\n",
              "        0.69106286, 0.61667201, 0.61667201, 0.61667201, 0.49351162,\n",
              "        0.59815624, 0.59901626, 0.61897857, 0.71272539, 0.59816042,\n",
              "        0.59901626, 0.62161964, 0.62779475, 0.59985789, 0.59985789,\n",
              "        0.62242326, 0.66662685, 0.59985789, 0.59985789, 0.6202935 ,\n",
              "        0.56642157, 0.64500166, 0.64660421, 0.6517712 , 0.67200368,\n",
              "        0.6517712 , 0.64660421, 0.64660421, 0.52340215, 0.61667201,\n",
              "        0.61667201, 0.61667201, 0.63317584, 0.61667201, 0.61667201,\n",
              "        0.61667201, 0.62121212, 0.59901626, 0.59901626, 0.62136254,\n",
              "        0.69931351, 0.59815624, 0.59901626, 0.6202935 , 0.54951569,\n",
              "        0.59985789, 0.59985789, 0.6202935 , 0.69931351, 0.59985789,\n",
              "        0.59985789, 0.62136254, 0.54275406, 0.64500166, 0.65018553,\n",
              "        0.64660421, 0.57061206, 0.65018553, 0.65018553, 0.64500166,\n",
              "        0.65247682, 0.61667201, 0.61667201, 0.61667201, 0.55290241,\n",
              "        0.61667201, 0.61667201, 0.61667201, 0.69931351, 0.59901626,\n",
              "        0.59816042, 0.62161964, 0.66971658, 0.59815624, 0.59901626,\n",
              "        0.62242326, 0.62779475, 0.59985789, 0.59985789, 0.6202935 ,\n",
              "        0.662268  , 0.59985789, 0.59985789, 0.62161964, 0.54673638]),\n",
              " 'split8_train_recall': array([0.52497001, 0.52545662, 0.52497001, 0.50252167, 0.5249958 ,\n",
              "        0.5249958 , 0.52450918, 0.51254219, 0.53134089, 0.53134089,\n",
              "        0.53134089, 0.50571159, 0.53134089, 0.53134089, 0.53134089,\n",
              "        0.51055871, 0.54062812, 0.54038481, 0.53608934, 0.50939375,\n",
              "        0.53994977, 0.53973225, 0.53562851, 0.50586296, 0.54739704,\n",
              "        0.54739704, 0.53514189, 0.50917623, 0.54625786, 0.54625786,\n",
              "        0.53562851, 0.50113918, 0.52450918, 0.52450918, 0.52450918,\n",
              "        0.5029825 , 0.5249958 , 0.52450918, 0.52497001, 0.50802247,\n",
              "        0.53134089, 0.53134089, 0.53134089, 0.50678462, 0.53134089,\n",
              "        0.53134089, 0.53134089, 0.49803222, 0.54062812, 0.54062812,\n",
              "        0.53655017, 0.50385258, 0.54062812, 0.54062812, 0.5353852 ,\n",
              "        0.50915044, 0.54739704, 0.54739704, 0.53560272, 0.50726675,\n",
              "        0.54739704, 0.54739704, 0.53584603, 0.50092166, 0.5249958 ,\n",
              "        0.5249958 , 0.5249958 , 0.50881855, 0.5249958 , 0.52497001,\n",
              "        0.5249958 , 0.51082781, 0.53134089, 0.53134089, 0.53134089,\n",
              "        0.50773095, 0.53134089, 0.53134089, 0.53134089, 0.49840111,\n",
              "        0.54038481, 0.54062812, 0.53514189, 0.50387837, 0.54016729,\n",
              "        0.54062812, 0.53562851, 0.5100721 , 0.54739704, 0.54739704,\n",
              "        0.53630686, 0.51063608, 0.54739704, 0.54739704, 0.5353852 ,\n",
              "        0.50729254, 0.52450918, 0.52497001, 0.52545662, 0.50838351,\n",
              "        0.52545662, 0.52497001, 0.52497001, 0.50749997, 0.53134089,\n",
              "        0.53134089, 0.53134089, 0.5103154 , 0.53134089, 0.53134089,\n",
              "        0.53134089, 0.51004631, 0.54062812, 0.54062812, 0.53584603,\n",
              "        0.50751343, 0.54038481, 0.54062812, 0.5353852 , 0.50583269,\n",
              "        0.54739704, 0.54739704, 0.5353852 , 0.50751343, 0.54739704,\n",
              "        0.54739704, 0.53584603, 0.50532028, 0.52450918, 0.5249958 ,\n",
              "        0.52497001, 0.51055535, 0.5249958 , 0.5249958 , 0.52450918,\n",
              "        0.51014946, 0.53134089, 0.53134089, 0.53134089, 0.50566675,\n",
              "        0.53134089, 0.53134089, 0.53134089, 0.50751343, 0.54062812,\n",
              "        0.54016729, 0.53562851, 0.51245361, 0.54038481, 0.54062812,\n",
              "        0.53630686, 0.5100721 , 0.54739704, 0.54739704, 0.5353852 ,\n",
              "        0.50746185, 0.54739704, 0.54739704, 0.53562851, 0.50826241]),\n",
              " 'split9_test_accuracy': array([0.66091954, 0.66091954, 0.66091954, 0.65804598, 0.66091954,\n",
              "        0.66091954, 0.66091954, 0.6408046 , 0.64655172, 0.64655172,\n",
              "        0.64655172, 0.65804598, 0.64655172, 0.64942529, 0.64655172,\n",
              "        0.65229885, 0.64367816, 0.64367816, 0.6408046 , 0.64367816,\n",
              "        0.64367816, 0.64367816, 0.6408046 , 0.65517241, 0.64367816,\n",
              "        0.64367816, 0.64367816, 0.64367816, 0.64655172, 0.64655172,\n",
              "        0.6408046 , 0.6408046 , 0.66091954, 0.66091954, 0.66091954,\n",
              "        0.6637931 , 0.66091954, 0.66091954, 0.66091954, 0.6408046 ,\n",
              "        0.64655172, 0.64655172, 0.64655172, 0.64655172, 0.64655172,\n",
              "        0.64655172, 0.64655172, 0.65517241, 0.64367816, 0.64367816,\n",
              "        0.6408046 , 0.65804598, 0.64367816, 0.64367816, 0.64367816,\n",
              "        0.65229885, 0.64367816, 0.64367816, 0.6408046 , 0.65804598,\n",
              "        0.64367816, 0.64367816, 0.6408046 , 0.66091954, 0.66091954,\n",
              "        0.66091954, 0.66091954, 0.65517241, 0.66091954, 0.66091954,\n",
              "        0.66091954, 0.66091954, 0.64655172, 0.64655172, 0.64655172,\n",
              "        0.64367816, 0.64655172, 0.64655172, 0.64655172, 0.65229885,\n",
              "        0.64367816, 0.64367816, 0.64367816, 0.65517241, 0.64367816,\n",
              "        0.64367816, 0.6408046 , 0.65229885, 0.64367816, 0.64367816,\n",
              "        0.6408046 , 0.64367816, 0.64367816, 0.64367816, 0.6408046 ,\n",
              "        0.65804598, 0.66091954, 0.66091954, 0.66091954, 0.64655172,\n",
              "        0.66091954, 0.66091954, 0.66091954, 0.65229885, 0.64655172,\n",
              "        0.64655172, 0.64367816, 0.64367816, 0.64655172, 0.64655172,\n",
              "        0.64655172, 0.65804598, 0.64367816, 0.64367816, 0.6408046 ,\n",
              "        0.64367816, 0.64367816, 0.64367816, 0.64367816, 0.65804598,\n",
              "        0.64367816, 0.64367816, 0.6408046 , 0.65804598, 0.64367816,\n",
              "        0.64367816, 0.6408046 , 0.6408046 , 0.66091954, 0.66091954,\n",
              "        0.66091954, 0.64942529, 0.66091954, 0.66091954, 0.66091954,\n",
              "        0.65804598, 0.64655172, 0.64655172, 0.64655172, 0.65804598,\n",
              "        0.64655172, 0.64655172, 0.64655172, 0.65804598, 0.64367816,\n",
              "        0.64367816, 0.64367816, 0.65804598, 0.64367816, 0.64367816,\n",
              "        0.64367816, 0.64942529, 0.64367816, 0.64367816, 0.64367816,\n",
              "        0.65804598, 0.64367816, 0.64367816, 0.64367816, 0.66091954]),\n",
              " 'split9_test_balanced_accuracy': array([0.52017544, 0.52017544, 0.52017544, 0.50811404, 0.52017544,\n",
              "        0.52017544, 0.52017544, 0.49298246, 0.51315789, 0.51315789,\n",
              "        0.51315789, 0.50416667, 0.51315789, 0.51535088, 0.51315789,\n",
              "        0.50372807, 0.52083333, 0.52083333, 0.50877193, 0.49714912,\n",
              "        0.52083333, 0.52083333, 0.50877193, 0.50592105, 0.53267544,\n",
              "        0.53267544, 0.5129386 , 0.49714912, 0.53486842, 0.53486842,\n",
              "        0.50877193, 0.49298246, 0.52017544, 0.52017544, 0.52017544,\n",
              "        0.51842105, 0.52017544, 0.52017544, 0.52017544, 0.51469298,\n",
              "        0.51315789, 0.51315789, 0.51315789, 0.52105263, 0.51315789,\n",
              "        0.51315789, 0.51315789, 0.5       , 0.52083333, 0.52083333,\n",
              "        0.50877193, 0.50614035, 0.52083333, 0.52083333, 0.5129386 ,\n",
              "        0.50372807, 0.53267544, 0.53267544, 0.50877193, 0.50811404,\n",
              "        0.53267544, 0.53267544, 0.50877193, 0.51030702, 0.52017544,\n",
              "        0.52017544, 0.52017544, 0.53157895, 0.52017544, 0.52017544,\n",
              "        0.52017544, 0.51030702, 0.51315789, 0.51315789, 0.51315789,\n",
              "        0.49714912, 0.51315789, 0.51315789, 0.51315789, 0.52741228,\n",
              "        0.52083333, 0.52083333, 0.5129386 , 0.51776316, 0.52083333,\n",
              "        0.52083333, 0.50877193, 0.50175439, 0.53267544, 0.53267544,\n",
              "        0.50877193, 0.49912281, 0.53267544, 0.53267544, 0.50877193,\n",
              "        0.50614035, 0.52017544, 0.52017544, 0.52017544, 0.49934211,\n",
              "        0.52017544, 0.52017544, 0.52017544, 0.50570175, 0.51315789,\n",
              "        0.51315789, 0.51096491, 0.49912281, 0.51315789, 0.51315789,\n",
              "        0.51315789, 0.50614035, 0.52083333, 0.52083333, 0.50877193,\n",
              "        0.49714912, 0.52083333, 0.52083333, 0.5129386 , 0.50416667,\n",
              "        0.53267544, 0.53267544, 0.50877193, 0.50614035, 0.53267544,\n",
              "        0.53267544, 0.50877193, 0.49298246, 0.52017544, 0.52017544,\n",
              "        0.52017544, 0.4995614 , 0.52017544, 0.52017544, 0.52017544,\n",
              "        0.50811404, 0.51315789, 0.51315789, 0.51315789, 0.50811404,\n",
              "        0.51315789, 0.51315789, 0.51315789, 0.50416667, 0.52083333,\n",
              "        0.52083333, 0.5129386 , 0.50811404, 0.52083333, 0.52083333,\n",
              "        0.5129386 , 0.52324561, 0.53267544, 0.53267544, 0.5129386 ,\n",
              "        0.50416667, 0.53267544, 0.53267544, 0.5129386 , 0.51030702]),\n",
              " 'split9_test_f1': array([0.45471929, 0.45471929, 0.45471929, 0.41979685, 0.45471929,\n",
              "        0.45471929, 0.45471929, 0.4052746 , 0.4587185 , 0.4587185 ,\n",
              "        0.4587185 , 0.4047862 , 0.4587185 , 0.46031423, 0.4587185 ,\n",
              "        0.41729516, 0.48301145, 0.48301145, 0.45553539, 0.41353629,\n",
              "        0.48301145, 0.48301145, 0.45553539, 0.41854637, 0.50952489,\n",
              "        0.50952489, 0.46261519, 0.41353629, 0.51145352, 0.51145352,\n",
              "        0.45553539, 0.4052746 , 0.45471929, 0.45471929, 0.45471929,\n",
              "        0.44333702, 0.45471929, 0.45471929, 0.45471929, 0.47141382,\n",
              "        0.4587185 , 0.4587185 , 0.4587185 , 0.4798712 , 0.4587185 ,\n",
              "        0.4587185 , 0.4587185 , 0.39583333, 0.48301145, 0.48301145,\n",
              "        0.45553539, 0.41242072, 0.48301145, 0.48301145, 0.46261519,\n",
              "        0.41729516, 0.50952489, 0.50952489, 0.45553539, 0.41979685,\n",
              "        0.50952489, 0.50952489, 0.45553539, 0.4210467 , 0.45471929,\n",
              "        0.45471929, 0.45471929, 0.49496904, 0.45471929, 0.45471929,\n",
              "        0.45471929, 0.4210467 , 0.4587185 , 0.4587185 , 0.4587185 ,\n",
              "        0.41353629, 0.4587185 , 0.4587185 , 0.4587185 , 0.48832857,\n",
              "        0.48301145, 0.48301145, 0.46261519, 0.45769051, 0.48301145,\n",
              "        0.48301145, 0.45553539, 0.41004553, 0.50952489, 0.50952489,\n",
              "        0.45553539, 0.42037395, 0.50952489, 0.50952489, 0.45553539,\n",
              "        0.41242072, 0.45471929, 0.45471929, 0.45471929, 0.41479021,\n",
              "        0.45471929, 0.45471929, 0.45471929, 0.42430581, 0.4587185 ,\n",
              "        0.4587185 , 0.4571256 , 0.42037395, 0.4587185 , 0.4587185 ,\n",
              "        0.4587185 , 0.41242072, 0.48301145, 0.48301145, 0.45553539,\n",
              "        0.41353629, 0.48301145, 0.48301145, 0.46261519, 0.4047862 ,\n",
              "        0.50952489, 0.50952489, 0.45553539, 0.41242072, 0.50952489,\n",
              "        0.50952489, 0.45553539, 0.4052746 , 0.45471929, 0.45471929,\n",
              "        0.45471929, 0.40885547, 0.45471929, 0.45471929, 0.45471929,\n",
              "        0.41979685, 0.4587185 , 0.4587185 , 0.4587185 , 0.41979685,\n",
              "        0.4587185 , 0.4587185 , 0.4587185 , 0.4047862 , 0.48301145,\n",
              "        0.48301145, 0.46261519, 0.41979685, 0.48301145, 0.48301145,\n",
              "        0.46261519, 0.48161172, 0.50952489, 0.50952489, 0.46261519,\n",
              "        0.4047862 , 0.50952489, 0.50952489, 0.46261519, 0.4210467 ]),\n",
              " 'split9_test_precision': array([0.61804962, 0.61804962, 0.61804962, 0.62944606, 0.61804962,\n",
              "        0.61804962, 0.61804962, 0.43706981, 0.54816054, 0.54816054,\n",
              "        0.54816054, 0.82853026, 0.54816054, 0.55856107, 0.54816054,\n",
              "        0.54273146, 0.55339078, 0.55339078, 0.52972136, 0.47692308,\n",
              "        0.55339078, 0.55339078, 0.52972136, 0.57894737, 0.56435358,\n",
              "        0.56435358, 0.5422838 , 0.47692308, 0.569967  , 0.569967  ,\n",
              "        0.52972136, 0.43706981, 0.61804962, 0.61804962, 0.61804962,\n",
              "        0.66519174, 0.61804962, 0.61804962, 0.61804962, 0.5409077 ,\n",
              "        0.54816054, 0.54816054, 0.54816054, 0.55861402, 0.54816054,\n",
              "        0.54816054, 0.54816054, 0.32758621, 0.55339078, 0.55339078,\n",
              "        0.52972136, 0.66231884, 0.55339078, 0.55339078, 0.5422838 ,\n",
              "        0.54273146, 0.56435358, 0.56435358, 0.52972136, 0.62944606,\n",
              "        0.56435358, 0.56435358, 0.52972136, 0.70494186, 0.61804962,\n",
              "        0.61804962, 0.61804962, 0.58544304, 0.61804962, 0.61804962,\n",
              "        0.61804962, 0.70494186, 0.54816054, 0.54816054, 0.54816054,\n",
              "        0.47692308, 0.54816054, 0.54816054, 0.54816054, 0.57632034,\n",
              "        0.55339078, 0.55339078, 0.5422838 , 0.58181818, 0.55339078,\n",
              "        0.55339078, 0.52972136, 0.52798834, 0.56435358, 0.56435358,\n",
              "        0.52972136, 0.49404762, 0.56435358, 0.56435358, 0.52972136,\n",
              "        0.66231884, 0.61804962, 0.61804962, 0.61804962, 0.49410029,\n",
              "        0.61804962, 0.61804962, 0.61804962, 0.55113078, 0.54816054,\n",
              "        0.54816054, 0.53858025, 0.49404762, 0.54816054, 0.54816054,\n",
              "        0.54816054, 0.66231884, 0.55339078, 0.55339078, 0.52972136,\n",
              "        0.47692308, 0.55339078, 0.55339078, 0.5422838 , 0.82853026,\n",
              "        0.56435358, 0.56435358, 0.52972136, 0.66231884, 0.56435358,\n",
              "        0.56435358, 0.52972136, 0.43706981, 0.61804962, 0.61804962,\n",
              "        0.61804962, 0.49415205, 0.61804962, 0.61804962, 0.61804962,\n",
              "        0.62944606, 0.54816054, 0.54816054, 0.54816054, 0.62944606,\n",
              "        0.54816054, 0.54816054, 0.54816054, 0.82853026, 0.55339078,\n",
              "        0.55339078, 0.5422838 , 0.62944606, 0.55339078, 0.55339078,\n",
              "        0.5422838 , 0.56666667, 0.56435358, 0.56435358, 0.5422838 ,\n",
              "        0.82853026, 0.56435358, 0.56435358, 0.5422838 , 0.70494186]),\n",
              " 'split9_test_recall': array([0.52017544, 0.52017544, 0.52017544, 0.50811404, 0.52017544,\n",
              "        0.52017544, 0.52017544, 0.49298246, 0.51315789, 0.51315789,\n",
              "        0.51315789, 0.50416667, 0.51315789, 0.51535088, 0.51315789,\n",
              "        0.50372807, 0.52083333, 0.52083333, 0.50877193, 0.49714912,\n",
              "        0.52083333, 0.52083333, 0.50877193, 0.50592105, 0.53267544,\n",
              "        0.53267544, 0.5129386 , 0.49714912, 0.53486842, 0.53486842,\n",
              "        0.50877193, 0.49298246, 0.52017544, 0.52017544, 0.52017544,\n",
              "        0.51842105, 0.52017544, 0.52017544, 0.52017544, 0.51469298,\n",
              "        0.51315789, 0.51315789, 0.51315789, 0.52105263, 0.51315789,\n",
              "        0.51315789, 0.51315789, 0.5       , 0.52083333, 0.52083333,\n",
              "        0.50877193, 0.50614035, 0.52083333, 0.52083333, 0.5129386 ,\n",
              "        0.50372807, 0.53267544, 0.53267544, 0.50877193, 0.50811404,\n",
              "        0.53267544, 0.53267544, 0.50877193, 0.51030702, 0.52017544,\n",
              "        0.52017544, 0.52017544, 0.53157895, 0.52017544, 0.52017544,\n",
              "        0.52017544, 0.51030702, 0.51315789, 0.51315789, 0.51315789,\n",
              "        0.49714912, 0.51315789, 0.51315789, 0.51315789, 0.52741228,\n",
              "        0.52083333, 0.52083333, 0.5129386 , 0.51776316, 0.52083333,\n",
              "        0.52083333, 0.50877193, 0.50175439, 0.53267544, 0.53267544,\n",
              "        0.50877193, 0.49912281, 0.53267544, 0.53267544, 0.50877193,\n",
              "        0.50614035, 0.52017544, 0.52017544, 0.52017544, 0.49934211,\n",
              "        0.52017544, 0.52017544, 0.52017544, 0.50570175, 0.51315789,\n",
              "        0.51315789, 0.51096491, 0.49912281, 0.51315789, 0.51315789,\n",
              "        0.51315789, 0.50614035, 0.52083333, 0.52083333, 0.50877193,\n",
              "        0.49714912, 0.52083333, 0.52083333, 0.5129386 , 0.50416667,\n",
              "        0.53267544, 0.53267544, 0.50877193, 0.50614035, 0.53267544,\n",
              "        0.53267544, 0.50877193, 0.49298246, 0.52017544, 0.52017544,\n",
              "        0.52017544, 0.4995614 , 0.52017544, 0.52017544, 0.52017544,\n",
              "        0.50811404, 0.51315789, 0.51315789, 0.51315789, 0.50811404,\n",
              "        0.51315789, 0.51315789, 0.51315789, 0.50416667, 0.52083333,\n",
              "        0.52083333, 0.5129386 , 0.50811404, 0.52083333, 0.52083333,\n",
              "        0.5129386 , 0.52324561, 0.53267544, 0.53267544, 0.5129386 ,\n",
              "        0.50416667, 0.53267544, 0.53267544, 0.5129386 , 0.51030702]),\n",
              " 'split9_train_accuracy': array([0.67112385, 0.67112385, 0.67112385, 0.65807068, 0.67016874,\n",
              "        0.67112385, 0.67016874, 0.65329513, 0.66730341, 0.66698504,\n",
              "        0.6663483 , 0.65647883, 0.66730341, 0.66666667, 0.66666667,\n",
              "        0.65711557, 0.66539319, 0.66539319, 0.66348297, 0.65329513,\n",
              "        0.66443808, 0.66443808, 0.66443808, 0.65743394, 0.669532  ,\n",
              "        0.669532  , 0.66348297, 0.65265839, 0.669532  , 0.669532  ,\n",
              "        0.66443808, 0.65488698, 0.67112385, 0.67112385, 0.67048711,\n",
              "        0.65966253, 0.67112385, 0.67112385, 0.67112385, 0.63642152,\n",
              "        0.66698504, 0.6663483 , 0.6663483 , 0.63642152, 0.66698504,\n",
              "        0.6663483 , 0.6663483 , 0.65456861, 0.66539319, 0.66539319,\n",
              "        0.66348297, 0.65807068, 0.66539319, 0.66539319, 0.66348297,\n",
              "        0.65870742, 0.669532  , 0.669532  , 0.66507482, 0.65934416,\n",
              "        0.669532  , 0.669532  , 0.66348297, 0.65934416, 0.67112385,\n",
              "        0.67016874, 0.67112385, 0.63801337, 0.67048711, 0.67112385,\n",
              "        0.67112385, 0.6599809 , 0.66666667, 0.6663483 , 0.6663483 ,\n",
              "        0.65393187, 0.6663483 , 0.6663483 , 0.6663483 , 0.63705826,\n",
              "        0.66539319, 0.66539319, 0.66348297, 0.64660936, 0.66539319,\n",
              "        0.66539319, 0.66443808, 0.65807068, 0.669532  , 0.669532  ,\n",
              "        0.66348297, 0.65520535, 0.669532  , 0.669532  , 0.66443808,\n",
              "        0.65838905, 0.67112385, 0.67112385, 0.67112385, 0.6567972 ,\n",
              "        0.67112385, 0.67112385, 0.67112385, 0.65807068, 0.66698504,\n",
              "        0.66698504, 0.66698504, 0.65488698, 0.6663483 , 0.66698504,\n",
              "        0.66698504, 0.65902579, 0.66539319, 0.66539319, 0.66507482,\n",
              "        0.6536135 , 0.66539319, 0.66539319, 0.66348297, 0.6567972 ,\n",
              "        0.669532  , 0.669532  , 0.66348297, 0.65902579, 0.669532  ,\n",
              "        0.669532  , 0.66443808, 0.65234002, 0.67112385, 0.67080548,\n",
              "        0.67112385, 0.6567972 , 0.67048711, 0.67048711, 0.67048711,\n",
              "        0.65743394, 0.66698504, 0.66730341, 0.6663483 , 0.65711557,\n",
              "        0.66730341, 0.66730341, 0.66730341, 0.65902579, 0.66539319,\n",
              "        0.66539319, 0.66348297, 0.65584209, 0.66539319, 0.66539319,\n",
              "        0.66380134, 0.637695  , 0.669532  , 0.669532  , 0.66380134,\n",
              "        0.65456861, 0.669532  , 0.669532  , 0.66348297, 0.65966253]),\n",
              " 'split9_train_balanced_accuracy': array([0.53854494, 0.53854494, 0.53854494, 0.51072773, 0.53716245,\n",
              "        0.53854494, 0.53716245, 0.50707987, 0.54084998, 0.54038915,\n",
              "        0.53946749, 0.50341789, 0.54063234, 0.53971068, 0.53971068,\n",
              "        0.51065108, 0.5498375 , 0.5498375 , 0.53880225, 0.50838571,\n",
              "        0.54823737, 0.54823737, 0.53953182, 0.51024135, 0.56213981,\n",
              "        0.56213981, 0.53901989, 0.50724641, 0.56192217, 0.56192217,\n",
              "        0.53953182, 0.50807819, 0.53854494, 0.53854494, 0.53762328,\n",
              "        0.51281424, 0.53854494, 0.53854494, 0.53854494, 0.5033316 ,\n",
              "        0.54038915, 0.53946749, 0.53946749, 0.50507271, 0.54038915,\n",
              "        0.53946749, 0.53946749, 0.5       , 0.5498375 , 0.5498375 ,\n",
              "        0.53880225, 0.50876898, 0.5498375 , 0.5498375 , 0.53901989,\n",
              "        0.51230231, 0.56213981, 0.56213981, 0.54045348, 0.51191814,\n",
              "        0.56213981, 0.56213981, 0.53880225, 0.51082994, 0.53854494,\n",
              "        0.53716245, 0.53854494, 0.50715922, 0.53762328, 0.53854494,\n",
              "        0.53854494, 0.51218688, 0.53992832, 0.53946749, 0.53946749,\n",
              "        0.50843681, 0.53946749, 0.53946749, 0.53946749, 0.50555909,\n",
              "        0.5498375 , 0.5498375 , 0.53901989, 0.50610801, 0.5498375 ,\n",
              "        0.5498375 , 0.53953182, 0.5105101 , 0.56213981, 0.56213981,\n",
              "        0.53880225, 0.51115068, 0.56213981, 0.56213981, 0.53953182,\n",
              "        0.50944745, 0.53854494, 0.53854494, 0.53854494, 0.51062553,\n",
              "        0.53854494, 0.53854494, 0.53854494, 0.51355704, 0.54038915,\n",
              "        0.54038915, 0.54038915, 0.51025458, 0.53946749, 0.54038915,\n",
              "        0.54038915, 0.50993383, 0.5498375 , 0.5498375 , 0.54045348,\n",
              "        0.50775834, 0.5498375 , 0.5498375 , 0.53901989, 0.50344345,\n",
              "        0.56213981, 0.56213981, 0.53880225, 0.50906328, 0.56213981,\n",
              "        0.56213981, 0.53953182, 0.50656794, 0.53854494, 0.53808411,\n",
              "        0.53854494, 0.51040789, 0.53762328, 0.53762328, 0.53762328,\n",
              "        0.50915316, 0.54038915, 0.54084998, 0.53946749, 0.50934525,\n",
              "        0.54063234, 0.54084998, 0.54063234, 0.50841036, 0.5498375 ,\n",
              "        0.5498375 , 0.53901989, 0.50532554, 0.5498375 , 0.5498375 ,\n",
              "        0.53926308, 0.50517492, 0.56213981, 0.56213981, 0.53948072,\n",
              "        0.50130583, 0.56213981, 0.56213981, 0.53880225, 0.51216133]),\n",
              " 'split9_train_f1': array([0.49055383, 0.49055383, 0.49055383, 0.42763316, 0.48817192,\n",
              "        0.49055383, 0.48817192, 0.42544584, 0.50190378, 0.50115302,\n",
              "        0.49964857, 0.40415405, 0.50135563, 0.49985027, 0.49985027,\n",
              "        0.429499  , 0.52491395, 0.52491395, 0.50162922, 0.42997738,\n",
              "        0.52238213, 0.52238213, 0.50224055, 0.42734149, 0.54629736,\n",
              "        0.54629736, 0.50216399, 0.42742826, 0.54588331, 0.54588331,\n",
              "        0.50224055, 0.4254064 , 0.49055383, 0.49055383, 0.48896698,\n",
              "        0.43145047, 0.49055383, 0.49055383, 0.49055383, 0.44615663,\n",
              "        0.50115302, 0.49964857, 0.49964857, 0.45112755, 0.50115302,\n",
              "        0.49964857, 0.49964857, 0.39561285, 0.52491395, 0.52491395,\n",
              "        0.50162922, 0.42054739, 0.52491395, 0.52491395, 0.50216399,\n",
              "        0.43176722, 0.54629736, 0.54629736, 0.50371992, 0.42899215,\n",
              "        0.54629736, 0.54629736, 0.50162922, 0.42508505, 0.49055383,\n",
              "        0.48817192, 0.49055383, 0.45444401, 0.48896698, 0.49055383,\n",
              "        0.49055383, 0.42850825, 0.50040129, 0.49964857, 0.49964857,\n",
              "        0.42877504, 0.49964857, 0.49964857, 0.49964857, 0.45147575,\n",
              "        0.52491395, 0.52491395, 0.50216399, 0.43614272, 0.52491395,\n",
              "        0.52491395, 0.50224055, 0.42685754, 0.54629736, 0.54629736,\n",
              "        0.50162922, 0.43533461, 0.54629736, 0.54629736, 0.50224055,\n",
              "        0.42228339, 0.49055383, 0.49055383, 0.49055383, 0.4301125 ,\n",
              "        0.49055383, 0.49055383, 0.49055383, 0.43746023, 0.50115302,\n",
              "        0.50115302, 0.50115302, 0.43296602, 0.49964857, 0.50115302,\n",
              "        0.50115302, 0.4225648 , 0.52491395, 0.52491395, 0.50371992,\n",
              "        0.42711461, 0.52491395, 0.52491395, 0.50216399, 0.40341901,\n",
              "        0.54629736, 0.54629736, 0.50162922, 0.41935308, 0.54629736,\n",
              "        0.54629736, 0.50224055, 0.42576851, 0.49055383, 0.48976095,\n",
              "        0.49055383, 0.42935106, 0.48896698, 0.48896698, 0.48896698,\n",
              "        0.42344163, 0.50115302, 0.50190378, 0.49964857, 0.4248666 ,\n",
              "        0.50135563, 0.50190378, 0.50135563, 0.41691226, 0.52491395,\n",
              "        0.52491395, 0.50216399, 0.41311345, 0.52491395, 0.52491395,\n",
              "        0.50236806, 0.44934737, 0.54629736, 0.54629736, 0.50290146,\n",
              "        0.40084073, 0.54629736, 0.54629736, 0.50162922, 0.42913872]),\n",
              " 'split9_train_precision': array([0.6564407 , 0.6564407 , 0.6564407 , 0.62341038, 0.65314679,\n",
              "        0.6564407 , 0.65314679, 0.56610538, 0.62795611, 0.62698514,\n",
              "        0.62502237, 0.70306008, 0.62822903, 0.62626896, 0.62626896,\n",
              "        0.60752688, 0.61438232, 0.61438232, 0.61318535, 0.5681252 ,\n",
              "        0.61212299, 0.61212299, 0.61650729, 0.61426428, 0.62154039,\n",
              "        0.62154039, 0.61304944, 0.56152704, 0.62160624, 0.62160624,\n",
              "        0.61650729, 0.58267422, 0.6564407 , 0.6564407 , 0.65425538,\n",
              "        0.64084996, 0.6564407 , 0.6564407 , 0.6564407 , 0.51181389,\n",
              "        0.62698514, 0.62502237, 0.62502237, 0.51683403, 0.62698514,\n",
              "        0.62502237, 0.62502237, 0.3272843 , 0.61438232, 0.61438232,\n",
              "        0.61318535, 0.64040698, 0.61438232, 0.61438232, 0.61304944,\n",
              "        0.62590494, 0.62154039, 0.62154039, 0.61840607, 0.64150788,\n",
              "        0.62154039, 0.62154039, 0.61318535, 0.65330287, 0.6564407 ,\n",
              "        0.65314679, 0.6564407 , 0.52347781, 0.65425538, 0.6564407 ,\n",
              "        0.6564407 , 0.65465219, 0.62600726, 0.62502237, 0.62502237,\n",
              "        0.57329198, 0.62502237, 0.62502237, 0.62502237, 0.51859644,\n",
              "        0.61438232, 0.61438232, 0.61304944, 0.53310792, 0.61438232,\n",
              "        0.61438232, 0.61650729, 0.62478976, 0.62154039, 0.62154039,\n",
              "        0.61318535, 0.58509339, 0.62154039, 0.62154039, 0.61650729,\n",
              "        0.6419536 , 0.6564407 , 0.6564407 , 0.6564407 , 0.60307893,\n",
              "        0.6564407 , 0.6564407 , 0.6564407 , 0.61133793, 0.62698514,\n",
              "        0.62698514, 0.62698514, 0.58241938, 0.62502237, 0.62698514,\n",
              "        0.62698514, 0.65565077, 0.61438232, 0.61438232, 0.61840607,\n",
              "        0.56982571, 0.61438232, 0.61438232, 0.61304944, 0.77250958,\n",
              "        0.62154039, 0.62154039, 0.61318535, 0.67146395, 0.62154039,\n",
              "        0.62154039, 0.61650729, 0.55772534, 0.6564407 , 0.65535329,\n",
              "        0.6564407 , 0.60366621, 0.65425538, 0.65425538, 0.65425538,\n",
              "        0.62029989, 0.62698514, 0.62795611, 0.62502237, 0.61277183,\n",
              "        0.62822903, 0.62795611, 0.62822903, 0.68858053, 0.61438232,\n",
              "        0.61438232, 0.61304944, 0.60628019, 0.61438232, 0.61438232,\n",
              "        0.61414034, 0.51803981, 0.62154039, 0.62154039, 0.61399903,\n",
              "        0.5775807 , 0.62154039, 0.62154039, 0.61318535, 0.6467543 ]),\n",
              " 'split9_train_recall': array([0.53854494, 0.53854494, 0.53854494, 0.51072773, 0.53716245,\n",
              "        0.53854494, 0.53716245, 0.50707987, 0.54084998, 0.54038915,\n",
              "        0.53946749, 0.50341789, 0.54063234, 0.53971068, 0.53971068,\n",
              "        0.51065108, 0.5498375 , 0.5498375 , 0.53880225, 0.50838571,\n",
              "        0.54823737, 0.54823737, 0.53953182, 0.51024135, 0.56213981,\n",
              "        0.56213981, 0.53901989, 0.50724641, 0.56192217, 0.56192217,\n",
              "        0.53953182, 0.50807819, 0.53854494, 0.53854494, 0.53762328,\n",
              "        0.51281424, 0.53854494, 0.53854494, 0.53854494, 0.5033316 ,\n",
              "        0.54038915, 0.53946749, 0.53946749, 0.50507271, 0.54038915,\n",
              "        0.53946749, 0.53946749, 0.5       , 0.5498375 , 0.5498375 ,\n",
              "        0.53880225, 0.50876898, 0.5498375 , 0.5498375 , 0.53901989,\n",
              "        0.51230231, 0.56213981, 0.56213981, 0.54045348, 0.51191814,\n",
              "        0.56213981, 0.56213981, 0.53880225, 0.51082994, 0.53854494,\n",
              "        0.53716245, 0.53854494, 0.50715922, 0.53762328, 0.53854494,\n",
              "        0.53854494, 0.51218688, 0.53992832, 0.53946749, 0.53946749,\n",
              "        0.50843681, 0.53946749, 0.53946749, 0.53946749, 0.50555909,\n",
              "        0.5498375 , 0.5498375 , 0.53901989, 0.50610801, 0.5498375 ,\n",
              "        0.5498375 , 0.53953182, 0.5105101 , 0.56213981, 0.56213981,\n",
              "        0.53880225, 0.51115068, 0.56213981, 0.56213981, 0.53953182,\n",
              "        0.50944745, 0.53854494, 0.53854494, 0.53854494, 0.51062553,\n",
              "        0.53854494, 0.53854494, 0.53854494, 0.51355704, 0.54038915,\n",
              "        0.54038915, 0.54038915, 0.51025458, 0.53946749, 0.54038915,\n",
              "        0.54038915, 0.50993383, 0.5498375 , 0.5498375 , 0.54045348,\n",
              "        0.50775834, 0.5498375 , 0.5498375 , 0.53901989, 0.50344345,\n",
              "        0.56213981, 0.56213981, 0.53880225, 0.50906328, 0.56213981,\n",
              "        0.56213981, 0.53953182, 0.50656794, 0.53854494, 0.53808411,\n",
              "        0.53854494, 0.51040789, 0.53762328, 0.53762328, 0.53762328,\n",
              "        0.50915316, 0.54038915, 0.54084998, 0.53946749, 0.50934525,\n",
              "        0.54063234, 0.54084998, 0.54063234, 0.50841036, 0.5498375 ,\n",
              "        0.5498375 , 0.53901989, 0.50532554, 0.5498375 , 0.5498375 ,\n",
              "        0.53926308, 0.50517492, 0.56213981, 0.56213981, 0.53948072,\n",
              "        0.50130583, 0.56213981, 0.56213981, 0.53880225, 0.51216133]),\n",
              " 'std_fit_time': array([4.71285289e-03, 1.63358190e-03, 1.92570732e-03, 1.06023820e-04,\n",
              "        1.46945135e-03, 2.76289073e-03, 7.32340568e-03, 1.10830759e-03,\n",
              "        3.20262594e-03, 1.73135331e-03, 2.11012747e-03, 8.35911043e-04,\n",
              "        1.33989971e-03, 1.67891496e-03, 1.30978203e-03, 2.95202202e-04,\n",
              "        2.60875326e-03, 3.46775800e-03, 1.96500477e-03, 3.57487616e-04,\n",
              "        2.14720506e-03, 3.49491139e-03, 9.15650196e-04, 9.00856493e-04,\n",
              "        2.04182899e-03, 3.40350600e-03, 2.79697903e-03, 1.23747318e-04,\n",
              "        1.51236401e-03, 1.06210829e-03, 2.61408156e-03, 1.67117991e-04,\n",
              "        3.93909795e-03, 9.59450509e-04, 2.25559873e-04, 9.62448320e-05,\n",
              "        1.43309512e-03, 3.23115712e-03, 1.11297136e-03, 1.14723185e-04,\n",
              "        8.07861018e-04, 3.24138204e-03, 1.22488233e-03, 1.28256966e-04,\n",
              "        3.90402110e-03, 1.58319601e-03, 1.40024806e-03, 4.53314069e-04,\n",
              "        3.24955871e-03, 3.12291516e-03, 1.65931235e-03, 2.81477202e-04,\n",
              "        3.52124847e-03, 2.41972790e-03, 3.08189302e-03, 2.32408929e-04,\n",
              "        4.61872745e-03, 6.35178091e-03, 1.27690121e-03, 7.40026471e-04,\n",
              "        2.65690468e-03, 2.23793316e-03, 8.26185674e-04, 4.87650174e-04,\n",
              "        1.65415229e-03, 2.88242130e-03, 1.69353963e-03, 7.02148221e-05,\n",
              "        2.46985317e-03, 9.09848977e-04, 1.82667285e-03, 9.43770063e-04,\n",
              "        2.68671674e-03, 1.24259416e-03, 1.68382228e-03, 4.66323511e-04,\n",
              "        1.49618576e-03, 2.93778867e-03, 1.24863417e-03, 6.94734721e-05,\n",
              "        1.83227871e-03, 4.42618703e-03, 3.86948250e-03, 9.99569573e-05,\n",
              "        2.36364274e-03, 1.81877157e-03, 3.09495431e-03, 1.03639437e-04,\n",
              "        2.80550129e-03, 1.74425400e-03, 1.27782317e-03, 1.08234801e-04,\n",
              "        2.52625494e-03, 1.59091485e-03, 2.27668660e-03, 1.30400151e-04,\n",
              "        3.10587301e-03, 3.57373772e-03, 3.90035687e-04, 1.34384185e-04,\n",
              "        5.86646541e-04, 8.13695157e-04, 8.90161303e-04, 1.93845808e-04,\n",
              "        2.95524733e-03, 4.91962362e-04, 2.98423747e-03, 6.76724205e-05,\n",
              "        3.15163291e-03, 1.21317140e-03, 1.46718626e-03, 1.27867163e-04,\n",
              "        8.59305266e-04, 2.79819344e-03, 2.79160435e-03, 2.04684016e-04,\n",
              "        2.70633658e-03, 2.84362581e-03, 8.09777409e-04, 5.48659949e-05,\n",
              "        1.30778013e-03, 4.80052983e-03, 1.62921635e-03, 2.59899113e-04,\n",
              "        1.45883300e-03, 1.39102170e-03, 3.16150192e-03, 1.19458225e-04,\n",
              "        9.47995533e-04, 1.34239045e-03, 8.48072678e-04, 3.38283521e-04,\n",
              "        6.55941692e-04, 1.36181752e-03, 9.33083761e-04, 4.49108998e-04,\n",
              "        4.37342509e-03, 2.72564989e-03, 1.16536156e-03, 1.28103117e-04,\n",
              "        3.10540988e-03, 5.17001112e-04, 2.93266584e-03, 2.63131780e-04,\n",
              "        2.94428899e-03, 3.91121987e-03, 6.90294537e-04, 3.81090438e-04,\n",
              "        3.41780345e-03, 1.88205133e-03, 4.55069275e-04, 2.97717510e-05,\n",
              "        1.89967059e-03, 3.35843971e-03, 1.35152023e-03, 1.28124869e-04,\n",
              "        1.83209234e-03, 2.57867825e-03, 6.48644706e-04, 1.91972752e-04]),\n",
              " 'std_score_time': array([4.67653868e-04, 1.58024431e-04, 2.16516366e-04, 3.97184676e-04,\n",
              "        1.26134659e-04, 7.67951610e-04, 5.50419052e-04, 8.96110145e-05,\n",
              "        1.28697557e-03, 4.13262702e-04, 5.49513633e-04, 1.17512609e-04,\n",
              "        1.77048399e-04, 4.92688389e-04, 4.33943963e-04, 2.68288830e-03,\n",
              "        4.00321377e-04, 6.64478416e-04, 9.35360055e-05, 9.09305432e-04,\n",
              "        6.29530565e-04, 4.84752796e-04, 1.18838403e-04, 3.89309254e-03,\n",
              "        1.86541597e-04, 3.64976441e-04, 5.23360062e-05, 3.77427681e-05,\n",
              "        1.36476959e-04, 5.84977166e-04, 2.56868297e-04, 1.74740976e-04,\n",
              "        1.07308979e-04, 3.41318639e-04, 1.96548506e-03, 4.19792580e-04,\n",
              "        2.51761626e-04, 1.13800261e-03, 3.56331769e-04, 4.89880506e-05,\n",
              "        1.83324921e-03, 1.80378631e-04, 4.31805749e-04, 1.82151060e-04,\n",
              "        4.94112533e-04, 1.43063775e-04, 1.58310288e-04, 2.45396762e-04,\n",
              "        4.19904846e-04, 7.29102984e-04, 1.62463259e-03, 2.97716373e-04,\n",
              "        9.91212786e-04, 8.86997300e-04, 3.85087519e-04, 6.72170815e-05,\n",
              "        7.98501374e-05, 1.27589953e-03, 1.25983652e-03, 2.16568335e-04,\n",
              "        1.34478116e-04, 6.24675257e-04, 3.28505335e-04, 3.34919681e-04,\n",
              "        7.59842774e-04, 1.33039967e-03, 1.46587620e-03, 4.64005434e-05,\n",
              "        2.72391347e-04, 1.46858106e-04, 3.62284263e-04, 4.77377138e-04,\n",
              "        6.83827236e-04, 2.99391890e-03, 1.69380177e-04, 2.15676020e-04,\n",
              "        1.36297594e-04, 3.22266272e-04, 3.53369603e-04, 1.78882452e-04,\n",
              "        5.01721353e-04, 6.44169693e-04, 5.10704005e-05, 1.85081780e-05,\n",
              "        6.54478978e-04, 2.50423019e-04, 4.90025296e-05, 6.46804779e-05,\n",
              "        1.10471167e-03, 1.15178296e-04, 1.93407996e-04, 6.10413474e-04,\n",
              "        9.28338391e-05, 2.94383826e-04, 2.55906326e-03, 6.36484241e-05,\n",
              "        1.31296880e-03, 1.75966699e-04, 1.12526268e-04, 3.27104139e-04,\n",
              "        6.79240965e-04, 1.28670821e-04, 1.45735132e-03, 2.96368642e-04,\n",
              "        8.14013097e-04, 6.11720529e-04, 3.04749236e-04, 1.10804944e-04,\n",
              "        8.66892708e-05, 4.29741494e-04, 9.84642782e-05, 2.67529427e-04,\n",
              "        3.70169630e-04, 3.66675888e-04, 1.86164959e-04, 4.12193235e-04,\n",
              "        5.50897722e-04, 4.48930103e-04, 2.68804201e-04, 5.18285196e-05,\n",
              "        9.47668596e-05, 1.92373309e-04, 4.10478767e-04, 8.66095838e-04,\n",
              "        1.13891794e-04, 2.17699540e-04, 6.94514753e-05, 5.61271416e-05,\n",
              "        9.19964866e-05, 6.05706708e-04, 4.61579019e-04, 2.61303888e-03,\n",
              "        2.39380489e-04, 1.13633546e-04, 8.42380313e-05, 1.15474910e-04,\n",
              "        6.93793313e-05, 3.50839465e-04, 2.67446392e-04, 2.43732025e-04,\n",
              "        4.89987178e-04, 4.50696634e-05, 2.90722537e-04, 3.37613980e-04,\n",
              "        4.62565262e-04, 5.75441397e-04, 4.73618432e-05, 5.15286478e-04,\n",
              "        6.49466861e-04, 4.17304758e-04, 7.30349630e-04, 1.80977274e-04,\n",
              "        5.53564057e-04, 3.61871497e-04, 8.04622575e-04, 1.44468313e-04,\n",
              "        3.91892198e-04, 1.66913782e-04, 1.61396375e-04, 5.27613851e-05]),\n",
              " 'std_test_accuracy': array([0.0082854 , 0.0082854 , 0.0082854 , 0.01266904, 0.00834306,\n",
              "        0.00819251, 0.0082854 , 0.0097526 , 0.01129493, 0.01079583,\n",
              "        0.01079583, 0.01202758, 0.01079583, 0.0103943 , 0.01079583,\n",
              "        0.01322487, 0.01236669, 0.01236669, 0.01257948, 0.01163892,\n",
              "        0.01203016, 0.01203016, 0.01205871, 0.00952332, 0.01841891,\n",
              "        0.01841891, 0.01154563, 0.01052563, 0.01737752, 0.01737752,\n",
              "        0.01205871, 0.01002644, 0.0082854 , 0.0082854 , 0.00814387,\n",
              "        0.00824266, 0.00819251, 0.00834306, 0.00851181, 0.01210282,\n",
              "        0.01079583, 0.01079583, 0.01079583, 0.0105861 , 0.01079583,\n",
              "        0.01079583, 0.01079583, 0.01238166, 0.01236669, 0.01236669,\n",
              "        0.01283789, 0.00588835, 0.01250627, 0.01236669, 0.01154563,\n",
              "        0.01266284, 0.01832443, 0.01832443, 0.01257948, 0.00828315,\n",
              "        0.01804385, 0.01804385, 0.01256081, 0.00414134, 0.00834306,\n",
              "        0.00834306, 0.00834306, 0.0094032 , 0.00834306, 0.0082854 ,\n",
              "        0.00824407, 0.01033383, 0.01079583, 0.01079583, 0.01054302,\n",
              "        0.00874857, 0.01079583, 0.01079583, 0.01079583, 0.01080615,\n",
              "        0.01250627, 0.01236669, 0.01208854, 0.00939763, 0.01236669,\n",
              "        0.01236669, 0.01204604, 0.00646613, 0.01841891, 0.01841891,\n",
              "        0.01283789, 0.01123279, 0.01841891, 0.01841891, 0.01257948,\n",
              "        0.01102514, 0.00819251, 0.0082854 , 0.00851181, 0.01061068,\n",
              "        0.0082854 , 0.00834306, 0.00814387, 0.00922805, 0.01054302,\n",
              "        0.01079583, 0.01124921, 0.01206932, 0.01079583, 0.01079583,\n",
              "        0.01079583, 0.00857588, 0.01236669, 0.01250627, 0.01246989,\n",
              "        0.01146627, 0.01236669, 0.01236669, 0.01153242, 0.01023446,\n",
              "        0.01841891, 0.01841891, 0.01249527, 0.00568831, 0.01841891,\n",
              "        0.01841891, 0.01256081, 0.00848024, 0.0082854 , 0.00814387,\n",
              "        0.0082854 , 0.01041462, 0.00834306, 0.00851181, 0.00886002,\n",
              "        0.01457189, 0.01079583, 0.01079583, 0.01079583, 0.01130954,\n",
              "        0.01079583, 0.01079583, 0.01079583, 0.0104393 , 0.01236669,\n",
              "        0.01236669, 0.01206913, 0.00427533, 0.01236669, 0.01236669,\n",
              "        0.01208854, 0.00908594, 0.01841891, 0.01841891, 0.01206913,\n",
              "        0.0111603 , 0.01841891, 0.01841891, 0.01236955, 0.0129902 ]),\n",
              " 'std_test_balanced_accuracy': array([0.01019425, 0.01019425, 0.01019425, 0.00730506, 0.01012474,\n",
              "        0.0099111 , 0.01019425, 0.0073896 , 0.01330162, 0.0127955 ,\n",
              "        0.0127955 , 0.0095558 , 0.0127955 , 0.01247249, 0.0127955 ,\n",
              "        0.00801195, 0.01347971, 0.01347971, 0.01420846, 0.00657788,\n",
              "        0.01373485, 0.01373485, 0.013767  , 0.00867256, 0.01721636,\n",
              "        0.01721636, 0.01289211, 0.0068662 , 0.01638936, 0.01638936,\n",
              "        0.013767  , 0.00903254, 0.01019425, 0.01019425, 0.00998687,\n",
              "        0.01071254, 0.0099111 , 0.01012474, 0.01043867, 0.00878475,\n",
              "        0.0127955 , 0.0127955 , 0.0127955 , 0.00732398, 0.0127955 ,\n",
              "        0.0127955 , 0.0127955 , 0.00751462, 0.01347971, 0.01347971,\n",
              "        0.01464811, 0.00629564, 0.01396824, 0.01347971, 0.01289211,\n",
              "        0.01117755, 0.01723112, 0.01723112, 0.01420846, 0.01029093,\n",
              "        0.01708142, 0.01708142, 0.01394287, 0.00709728, 0.01012474,\n",
              "        0.01012474, 0.01012474, 0.01062309, 0.01012474, 0.01019425,\n",
              "        0.0104076 , 0.00622818, 0.0127955 , 0.0127955 , 0.0126108 ,\n",
              "        0.0065997 , 0.0127955 , 0.0127955 , 0.0127955 , 0.0100928 ,\n",
              "        0.01396824, 0.01347971, 0.01348139, 0.00778367, 0.01347971,\n",
              "        0.01347971, 0.01339282, 0.00891956, 0.01721636, 0.01721636,\n",
              "        0.01464811, 0.00969139, 0.01721636, 0.01721636, 0.01420846,\n",
              "        0.01099014, 0.0099111 , 0.01019425, 0.01043867, 0.00609774,\n",
              "        0.01019425, 0.01012474, 0.00998687, 0.00893245, 0.0126108 ,\n",
              "        0.0127955 , 0.01314352, 0.00739948, 0.0127955 , 0.0127955 ,\n",
              "        0.0127955 , 0.0060695 , 0.01347971, 0.01396824, 0.01386129,\n",
              "        0.00975712, 0.01347971, 0.01347971, 0.01261885, 0.00719788,\n",
              "        0.01721636, 0.01721636, 0.01413479, 0.00606517, 0.01721636,\n",
              "        0.01721636, 0.01394287, 0.00857828, 0.01019425, 0.00998687,\n",
              "        0.01019425, 0.0109855 , 0.01012474, 0.01043867, 0.0111264 ,\n",
              "        0.0103601 , 0.0127955 , 0.0127955 , 0.0127955 , 0.00682174,\n",
              "        0.0127955 , 0.0127955 , 0.0127955 , 0.00748204, 0.01347971,\n",
              "        0.01347971, 0.01320739, 0.00591906, 0.01347971, 0.01347971,\n",
              "        0.01348139, 0.01130558, 0.01721636, 0.01721636, 0.01320739,\n",
              "        0.0072612 , 0.01721636, 0.01721636, 0.01372159, 0.01162132]),\n",
              " 'std_test_f1': array([0.01775674, 0.01775674, 0.01775674, 0.01847725, 0.01767435,\n",
              "        0.01751536, 0.01775674, 0.01479404, 0.02083695, 0.02029017,\n",
              "        0.02029017, 0.01811121, 0.02029017, 0.02007299, 0.02029017,\n",
              "        0.00967417, 0.0190071 , 0.0190071 , 0.02137081, 0.01391814,\n",
              "        0.01988292, 0.01988292, 0.02076425, 0.02002654, 0.02077613,\n",
              "        0.02077613, 0.01956863, 0.01548991, 0.02003728, 0.02003728,\n",
              "        0.02076425, 0.01599255, 0.01775674, 0.01775674, 0.01759976,\n",
              "        0.02408327, 0.01751536, 0.01767435, 0.01792292, 0.01791345,\n",
              "        0.02029017, 0.02029017, 0.02029017, 0.02205811, 0.02029017,\n",
              "        0.02029017, 0.02029017, 0.01783503, 0.0190071 , 0.0190071 ,\n",
              "        0.02202929, 0.01252827, 0.02000286, 0.0190071 , 0.01956863,\n",
              "        0.02120008, 0.02083377, 0.02083377, 0.02137081, 0.01864802,\n",
              "        0.02073962, 0.02073962, 0.02077206, 0.01682871, 0.01767435,\n",
              "        0.01767435, 0.01767435, 0.02549961, 0.01767435, 0.01775674,\n",
              "        0.01803704, 0.01672888, 0.02029017, 0.02029017, 0.02016893,\n",
              "        0.01565398, 0.02029017, 0.02029017, 0.02029017, 0.02787141,\n",
              "        0.02000286, 0.0190071 , 0.02021652, 0.01635412, 0.0190071 ,\n",
              "        0.0190071 , 0.02018044, 0.02049316, 0.02077613, 0.02077613,\n",
              "        0.02202929, 0.01089424, 0.02077613, 0.02077613, 0.02137081,\n",
              "        0.02006405, 0.01751536, 0.01775674, 0.01792292, 0.01133521,\n",
              "        0.01775674, 0.01767435, 0.01759976, 0.02830726, 0.02016893,\n",
              "        0.02029017, 0.02051582, 0.00930055, 0.02029017, 0.02029017,\n",
              "        0.02029017, 0.01182953, 0.0190071 , 0.02000286, 0.02068118,\n",
              "        0.01559293, 0.0190071 , 0.0190071 , 0.01895307, 0.01398735,\n",
              "        0.02077613, 0.02077613, 0.02128795, 0.01644209, 0.02077613,\n",
              "        0.02077613, 0.02077206, 0.01354066, 0.01775674, 0.01759976,\n",
              "        0.01775674, 0.0210309 , 0.01767435, 0.01792292, 0.01915338,\n",
              "        0.01001721, 0.02029017, 0.02029017, 0.02029017, 0.01584043,\n",
              "        0.02029017, 0.02029017, 0.02029017, 0.01406742, 0.0190071 ,\n",
              "        0.0190071 , 0.01959965, 0.01312831, 0.0190071 , 0.0190071 ,\n",
              "        0.02021652, 0.02677281, 0.02077613, 0.02077613, 0.01959965,\n",
              "        0.01777817, 0.02077613, 0.02077613, 0.02044329, 0.01808247]),\n",
              " 'std_test_precision': array([0.06110019, 0.06110019, 0.06110019, 0.13203281, 0.0615885 ,\n",
              "        0.06036451, 0.06110019, 0.13481323, 0.04824956, 0.04675095,\n",
              "        0.04675095, 0.16172752, 0.04675095, 0.04532658, 0.04675095,\n",
              "        0.14641336, 0.03131323, 0.03131323, 0.04580481, 0.13061003,\n",
              "        0.0312272 , 0.0312272 , 0.04476605, 0.1219072 , 0.03871588,\n",
              "        0.03871588, 0.04226877, 0.07258143, 0.03662226, 0.03662226,\n",
              "        0.04476605, 0.12387474, 0.06110019, 0.06110019, 0.05996626,\n",
              "        0.14279002, 0.06036451, 0.0615885 , 0.06266676, 0.14354884,\n",
              "        0.04675095, 0.04675095, 0.04675095, 0.08644602, 0.04675095,\n",
              "        0.04675095, 0.04675095, 0.1457217 , 0.03131323, 0.03131323,\n",
              "        0.04658731, 0.15389696, 0.03201264, 0.03131323, 0.04226877,\n",
              "        0.11184634, 0.03853355, 0.03853355, 0.04580481, 0.10720784,\n",
              "        0.03805705, 0.03805705, 0.04576108, 0.15235678, 0.0615885 ,\n",
              "        0.0615885 , 0.0615885 , 0.09933004, 0.0615885 , 0.06110019,\n",
              "        0.06067541, 0.15393661, 0.04675095, 0.04675095, 0.04549011,\n",
              "        0.15516756, 0.04675095, 0.04675095, 0.04675095, 0.18994789,\n",
              "        0.03201264, 0.03131323, 0.04380654, 0.13620768, 0.03131323,\n",
              "        0.03131323, 0.04431536, 0.08335738, 0.03871588, 0.03871588,\n",
              "        0.04658731, 0.0943134 , 0.03871588, 0.03871588, 0.04580481,\n",
              "        0.09552238, 0.06036451, 0.06110019, 0.06266676, 0.12415042,\n",
              "        0.06110019, 0.0615885 , 0.05996626, 0.11701836, 0.04549011,\n",
              "        0.04675095, 0.04820466, 0.12492756, 0.04675095, 0.04675095,\n",
              "        0.04675095, 0.12255206, 0.03131323, 0.03201264, 0.04561574,\n",
              "        0.11645857, 0.03131323, 0.03131323, 0.04224518, 0.11253135,\n",
              "        0.03871588, 0.03871588, 0.04567617, 0.12541468, 0.03871588,\n",
              "        0.03871588, 0.04576108, 0.1402776 , 0.06110019, 0.05996626,\n",
              "        0.06110019, 0.10332864, 0.0615885 , 0.06266676, 0.0631421 ,\n",
              "        0.14236836, 0.04675095, 0.04675095, 0.04675095, 0.13099575,\n",
              "        0.04675095, 0.04675095, 0.04675095, 0.16325132, 0.03131323,\n",
              "        0.03131323, 0.0437659 , 0.10491222, 0.03131323, 0.03131323,\n",
              "        0.04380654, 0.12710854, 0.03871588, 0.03871588, 0.0437659 ,\n",
              "        0.16358226, 0.03871588, 0.03871588, 0.04469316, 0.15620988]),\n",
              " 'std_test_recall': array([0.01019425, 0.01019425, 0.01019425, 0.00730506, 0.01012474,\n",
              "        0.0099111 , 0.01019425, 0.0073896 , 0.01330162, 0.0127955 ,\n",
              "        0.0127955 , 0.0095558 , 0.0127955 , 0.01247249, 0.0127955 ,\n",
              "        0.00801195, 0.01347971, 0.01347971, 0.01420846, 0.00657788,\n",
              "        0.01373485, 0.01373485, 0.013767  , 0.00867256, 0.01721636,\n",
              "        0.01721636, 0.01289211, 0.0068662 , 0.01638936, 0.01638936,\n",
              "        0.013767  , 0.00903254, 0.01019425, 0.01019425, 0.00998687,\n",
              "        0.01071254, 0.0099111 , 0.01012474, 0.01043867, 0.00878475,\n",
              "        0.0127955 , 0.0127955 , 0.0127955 , 0.00732398, 0.0127955 ,\n",
              "        0.0127955 , 0.0127955 , 0.00751462, 0.01347971, 0.01347971,\n",
              "        0.01464811, 0.00629564, 0.01396824, 0.01347971, 0.01289211,\n",
              "        0.01117755, 0.01723112, 0.01723112, 0.01420846, 0.01029093,\n",
              "        0.01708142, 0.01708142, 0.01394287, 0.00709728, 0.01012474,\n",
              "        0.01012474, 0.01012474, 0.01062309, 0.01012474, 0.01019425,\n",
              "        0.0104076 , 0.00622818, 0.0127955 , 0.0127955 , 0.0126108 ,\n",
              "        0.0065997 , 0.0127955 , 0.0127955 , 0.0127955 , 0.0100928 ,\n",
              "        0.01396824, 0.01347971, 0.01348139, 0.00778367, 0.01347971,\n",
              "        0.01347971, 0.01339282, 0.00891956, 0.01721636, 0.01721636,\n",
              "        0.01464811, 0.00969139, 0.01721636, 0.01721636, 0.01420846,\n",
              "        0.01099014, 0.0099111 , 0.01019425, 0.01043867, 0.00609774,\n",
              "        0.01019425, 0.01012474, 0.00998687, 0.00893245, 0.0126108 ,\n",
              "        0.0127955 , 0.01314352, 0.00739948, 0.0127955 , 0.0127955 ,\n",
              "        0.0127955 , 0.0060695 , 0.01347971, 0.01396824, 0.01386129,\n",
              "        0.00975712, 0.01347971, 0.01347971, 0.01261885, 0.00719788,\n",
              "        0.01721636, 0.01721636, 0.01413479, 0.00606517, 0.01721636,\n",
              "        0.01721636, 0.01394287, 0.00857828, 0.01019425, 0.00998687,\n",
              "        0.01019425, 0.0109855 , 0.01012474, 0.01043867, 0.0111264 ,\n",
              "        0.0103601 , 0.0127955 , 0.0127955 , 0.0127955 , 0.00682174,\n",
              "        0.0127955 , 0.0127955 , 0.0127955 , 0.00748204, 0.01347971,\n",
              "        0.01347971, 0.01320739, 0.00591906, 0.01347971, 0.01347971,\n",
              "        0.01348139, 0.01130558, 0.01721636, 0.01721636, 0.01320739,\n",
              "        0.0072612 , 0.01721636, 0.01721636, 0.01372159, 0.01162132]),\n",
              " 'std_train_accuracy': array([0.00217128, 0.00219752, 0.00222504, 0.00841651, 0.00189956,\n",
              "        0.00215127, 0.00209305, 0.00628671, 0.0018721 , 0.00183795,\n",
              "        0.00187214, 0.00488639, 0.00212142, 0.00180745, 0.00179025,\n",
              "        0.00660373, 0.0022874 , 0.00235247, 0.00190441, 0.00868198,\n",
              "        0.00163722, 0.0016672 , 0.00183985, 0.00729882, 0.00367429,\n",
              "        0.00367429, 0.0020662 , 0.0069124 , 0.00368199, 0.00369812,\n",
              "        0.00186458, 0.00537223, 0.00213074, 0.00219327, 0.00211415,\n",
              "        0.00718771, 0.00211704, 0.00219943, 0.0021666 , 0.00898523,\n",
              "        0.00188113, 0.00181651, 0.00186635, 0.00898359, 0.00184309,\n",
              "        0.00182829, 0.00175469, 0.00870105, 0.00226907, 0.00229367,\n",
              "        0.00226008, 0.00112762, 0.00215968, 0.00230741, 0.00208379,\n",
              "        0.00954066, 0.00368384, 0.00371077, 0.00203654, 0.00212091,\n",
              "        0.00371077, 0.00371077, 0.00181282, 0.0018583 , 0.00225828,\n",
              "        0.00211738, 0.00215996, 0.00853772, 0.00214817, 0.00207283,\n",
              "        0.002174  , 0.00606624, 0.00193194, 0.00186354, 0.00190933,\n",
              "        0.00717336, 0.00173882, 0.00177775, 0.00182764, 0.00855786,\n",
              "        0.00234412, 0.00233653, 0.0021068 , 0.00630223, 0.00234087,\n",
              "        0.00230895, 0.00189017, 0.00718949, 0.00367429, 0.00368609,\n",
              "        0.00223147, 0.00581484, 0.00371077, 0.00367231, 0.00189062,\n",
              "        0.00746179, 0.00212683, 0.00217866, 0.00197724, 0.00732557,\n",
              "        0.0020932 , 0.0022462 , 0.00219759, 0.00731425, 0.00194484,\n",
              "        0.00191815, 0.00188364, 0.00756216, 0.00192564, 0.00181341,\n",
              "        0.00188839, 0.00607372, 0.00233653, 0.00230741, 0.00225522,\n",
              "        0.00745053, 0.00232936, 0.00233653, 0.00199502, 0.00628622,\n",
              "        0.00367429, 0.00367429, 0.0020515 , 0.00514255, 0.00367231,\n",
              "        0.00367429, 0.00199639, 0.00311042, 0.0022175 , 0.00218644,\n",
              "        0.00207836, 0.00527854, 0.00194422, 0.00194421, 0.0020719 ,\n",
              "        0.00630761, 0.00187031, 0.00198738, 0.00177003, 0.00873724,\n",
              "        0.0019666 , 0.00187463, 0.00177124, 0.00448422, 0.00230895,\n",
              "        0.00223598, 0.00210317, 0.00248   , 0.00229861, 0.00233653,\n",
              "        0.00217276, 0.00773532, 0.00367429, 0.00367429, 0.00210069,\n",
              "        0.00773813, 0.00367429, 0.00367429, 0.00212874, 0.00450308]),\n",
              " 'std_train_balanced_accuracy': array([0.00379104, 0.0037741 , 0.00378987, 0.00401603, 0.00340139,\n",
              "        0.00375739, 0.00357125, 0.00253255, 0.00284427, 0.00279412,\n",
              "        0.00266727, 0.00328823, 0.00301735, 0.00265789, 0.00270269,\n",
              "        0.00548197, 0.00301266, 0.00305161, 0.00139459, 0.0026601 ,\n",
              "        0.00217332, 0.00229767, 0.00175289, 0.0025917 , 0.00529858,\n",
              "        0.00529858, 0.00157181, 0.00214481, 0.00528999, 0.00529192,\n",
              "        0.00174799, 0.00277187, 0.00373216, 0.00384308, 0.00360747,\n",
              "        0.00290928, 0.00378396, 0.0038708 , 0.00376433, 0.00351809,\n",
              "        0.00279143, 0.00267772, 0.0026868 , 0.00248589, 0.00277621,\n",
              "        0.00262656, 0.00260157, 0.00502872, 0.00292809, 0.00296173,\n",
              "        0.00167737, 0.00282011, 0.00289498, 0.00299299, 0.00180587,\n",
              "        0.00367805, 0.00530279, 0.00535189, 0.0018626 , 0.00211185,\n",
              "        0.00535189, 0.00535189, 0.00133739, 0.00451116, 0.00388958,\n",
              "        0.00360167, 0.00379537, 0.00302463, 0.00358687, 0.00367743,\n",
              "        0.00380069, 0.00313219, 0.00278864, 0.00285277, 0.00270884,\n",
              "        0.00351725, 0.00260186, 0.00259896, 0.00263814, 0.00396797,\n",
              "        0.00304092, 0.00303012, 0.00163599, 0.00203982, 0.00305206,\n",
              "        0.00298435, 0.00158197, 0.00395175, 0.00529858, 0.00530574,\n",
              "        0.00182983, 0.00171279, 0.00535189, 0.00529574, 0.00178155,\n",
              "        0.00226298, 0.00372685, 0.00383875, 0.00368098, 0.00205326,\n",
              "        0.00366046, 0.00382832, 0.00377292, 0.00452173, 0.00281757,\n",
              "        0.00283026, 0.00280612, 0.00249788, 0.00266129, 0.00273628,\n",
              "        0.00279526, 0.00213535, 0.00298299, 0.003023  , 0.00192162,\n",
              "        0.00198628, 0.00305233, 0.00303012, 0.00157324, 0.00299017,\n",
              "        0.00529858, 0.00529858, 0.00147081, 0.00356304, 0.00529574,\n",
              "        0.00529858, 0.00164779, 0.00155967, 0.00387069, 0.00372369,\n",
              "        0.00368637, 0.00384638, 0.00349207, 0.0035087 , 0.00365858,\n",
              "        0.00292283, 0.0028143 , 0.00296116, 0.00272283, 0.00188893,\n",
              "        0.00284092, 0.00283522, 0.00287181, 0.00292803, 0.00298435,\n",
              "        0.00296891, 0.00183413, 0.0037619 , 0.0030117 , 0.00303012,\n",
              "        0.0016229 , 0.00337256, 0.00529858, 0.00529858, 0.00170741,\n",
              "        0.00308512, 0.00529858, 0.00529858, 0.00154886, 0.00286078]),\n",
              " 'std_train_f1': array([0.00780279, 0.00777843, 0.00780558, 0.02133189, 0.007255  ,\n",
              "        0.00787776, 0.00741555, 0.01317694, 0.00523264, 0.00518162,\n",
              "        0.00487174, 0.01334875, 0.00542587, 0.00487936, 0.00499971,\n",
              "        0.01582128, 0.00466187, 0.00468435, 0.0027213 , 0.01523311,\n",
              "        0.00372286, 0.00394744, 0.0032664 , 0.01425116, 0.00763102,\n",
              "        0.00763102, 0.00298242, 0.01107798, 0.00761247, 0.00761098,\n",
              "        0.00324794, 0.01377283, 0.00779227, 0.0079446 , 0.00748254,\n",
              "        0.01248048, 0.0078863 , 0.00802457, 0.00777375, 0.0123434 ,\n",
              "        0.00512377, 0.00491546, 0.00488687, 0.01695163, 0.00511826,\n",
              "        0.00485058, 0.00482625, 0.02050439, 0.00451474, 0.00456333,\n",
              "        0.00275931, 0.0090308 , 0.00455687, 0.00460463, 0.00327496,\n",
              "        0.01423601, 0.00763344, 0.00770503, 0.00330936, 0.00764768,\n",
              "        0.00770503, 0.00770503, 0.00273671, 0.01329031, 0.00799202,\n",
              "        0.00746288, 0.00787132, 0.01598805, 0.00742019, 0.00770686,\n",
              "        0.00795778, 0.0152795 , 0.00505398, 0.00523701, 0.00495008,\n",
              "        0.01508188, 0.00480621, 0.00481471, 0.00484654, 0.02005809,\n",
              "        0.00467177, 0.0046621 , 0.00301667, 0.01271277, 0.00469807,\n",
              "        0.00459211, 0.00301409, 0.00481978, 0.00763102, 0.00763708,\n",
              "        0.00297263, 0.0098818 , 0.00770503, 0.00762744, 0.00325173,\n",
              "        0.01302668, 0.00773755, 0.00790175, 0.00775366, 0.01507137,\n",
              "        0.00768138, 0.0078163 , 0.00777089, 0.02290905, 0.00510476,\n",
              "        0.00519212, 0.00519224, 0.01099973, 0.00480462, 0.00504539,\n",
              "        0.00509816, 0.00857225, 0.00458157, 0.0046645 , 0.00330104,\n",
              "        0.01421357, 0.00469651, 0.0046621 , 0.0030706 , 0.01520218,\n",
              "        0.00763102, 0.00763102, 0.00282148, 0.0141085 , 0.00762744,\n",
              "        0.00763102, 0.00295131, 0.00490418, 0.00799742, 0.0077181 ,\n",
              "        0.00766483, 0.01685256, 0.007409  , 0.00742572, 0.00758572,\n",
              "        0.00820202, 0.00519913, 0.00538344, 0.005068  , 0.01629128,\n",
              "        0.00513297, 0.00522805, 0.00533424, 0.01495304, 0.00459211,\n",
              "        0.0046528 , 0.00334771, 0.01010636, 0.00465349, 0.0046621 ,\n",
              "        0.00284681, 0.01505218, 0.00763102, 0.00763102, 0.0031718 ,\n",
              "        0.0194049 , 0.00763102, 0.00763102, 0.00283767, 0.01324489]),\n",
              " 'std_train_precision': array([0.01051   , 0.01108418, 0.01125396, 0.10556057, 0.00996879,\n",
              "        0.01118356, 0.01073878, 0.0792379 , 0.00673353, 0.00675404,\n",
              "        0.00704873, 0.05988254, 0.00800799, 0.0067365 , 0.00650428,\n",
              "        0.15693235, 0.00583222, 0.00602572, 0.00751045, 0.07172573,\n",
              "        0.00412643, 0.00419859, 0.00704182, 0.06293425, 0.00781786,\n",
              "        0.00781786, 0.00780209, 0.03715238, 0.00791758, 0.00794747,\n",
              "        0.0071748 , 0.05111549, 0.0107024 , 0.01051555, 0.0107578 ,\n",
              "        0.07832856, 0.01005342, 0.01049621, 0.01064884, 0.05841909,\n",
              "        0.0068945 , 0.00667379, 0.00687498, 0.0584516 , 0.00676533,\n",
              "        0.0069864 , 0.00657412, 0.09571449, 0.00583162, 0.00586475,\n",
              "        0.00864666, 0.04115807, 0.00549006, 0.00591316, 0.00782857,\n",
              "        0.07194921, 0.00783736, 0.0079047 , 0.00766613, 0.07310324,\n",
              "        0.0079047 , 0.0079047 , 0.00715461, 0.08079275, 0.01117429,\n",
              "        0.01116235, 0.01049521, 0.0844036 , 0.01126013, 0.01017313,\n",
              "        0.01093364, 0.06168573, 0.00709997, 0.00666253, 0.00725895,\n",
              "        0.10353054, 0.00630326, 0.00669731, 0.00686503, 0.13599661,\n",
              "        0.00600884, 0.00598756, 0.00795986, 0.05746036, 0.00599308,\n",
              "        0.00592406, 0.00735072, 0.0488756 , 0.00781786, 0.00784112,\n",
              "        0.00839401, 0.04218309, 0.0079047 , 0.00781473, 0.00718946,\n",
              "        0.08370259, 0.01048437, 0.01014323, 0.00910123, 0.06886094,\n",
              "        0.01081992, 0.01102141, 0.01096192, 0.13298856, 0.0072341 ,\n",
              "        0.00705916, 0.00698232, 0.0678089 , 0.00728821, 0.00663242,\n",
              "        0.006821  , 0.06496229, 0.00600545, 0.00589828, 0.00838067,\n",
              "        0.06141242, 0.00594748, 0.00598756, 0.00761618, 0.07430395,\n",
              "        0.00781786, 0.00781786, 0.00784946, 0.05972049, 0.00781473,\n",
              "        0.00781786, 0.00767308, 0.06025133, 0.01067185, 0.01135594,\n",
              "        0.00989514, 0.07916715, 0.0097935 , 0.00951607, 0.00980867,\n",
              "        0.05467633, 0.00681509, 0.0072067 , 0.00645585, 0.07911617,\n",
              "        0.00718744, 0.00674722, 0.00620156, 0.05515947, 0.00592406,\n",
              "        0.00569757, 0.00794799, 0.08728437, 0.00588139, 0.00598756,\n",
              "        0.00828702, 0.08722624, 0.00781786, 0.00781786, 0.0079078 ,\n",
              "        0.1255436 , 0.00781786, 0.00781786, 0.00813501, 0.05193969]),\n",
              " 'std_train_recall': array([0.00379104, 0.0037741 , 0.00378987, 0.00401603, 0.00340139,\n",
              "        0.00375739, 0.00357125, 0.00253255, 0.00284427, 0.00279412,\n",
              "        0.00266727, 0.00328823, 0.00301735, 0.00265789, 0.00270269,\n",
              "        0.00548197, 0.00301266, 0.00305161, 0.00139459, 0.0026601 ,\n",
              "        0.00217332, 0.00229767, 0.00175289, 0.0025917 , 0.00529858,\n",
              "        0.00529858, 0.00157181, 0.00214481, 0.00528999, 0.00529192,\n",
              "        0.00174799, 0.00277187, 0.00373216, 0.00384308, 0.00360747,\n",
              "        0.00290928, 0.00378396, 0.0038708 , 0.00376433, 0.00351809,\n",
              "        0.00279143, 0.00267772, 0.0026868 , 0.00248589, 0.00277621,\n",
              "        0.00262656, 0.00260157, 0.00502872, 0.00292809, 0.00296173,\n",
              "        0.00167737, 0.00282011, 0.00289498, 0.00299299, 0.00180587,\n",
              "        0.00367805, 0.00530279, 0.00535189, 0.0018626 , 0.00211185,\n",
              "        0.00535189, 0.00535189, 0.00133739, 0.00451116, 0.00388958,\n",
              "        0.00360167, 0.00379537, 0.00302463, 0.00358687, 0.00367743,\n",
              "        0.00380069, 0.00313219, 0.00278864, 0.00285277, 0.00270884,\n",
              "        0.00351725, 0.00260186, 0.00259896, 0.00263814, 0.00396797,\n",
              "        0.00304092, 0.00303012, 0.00163599, 0.00203982, 0.00305206,\n",
              "        0.00298435, 0.00158197, 0.00395175, 0.00529858, 0.00530574,\n",
              "        0.00182983, 0.00171279, 0.00535189, 0.00529574, 0.00178155,\n",
              "        0.00226298, 0.00372685, 0.00383875, 0.00368098, 0.00205326,\n",
              "        0.00366046, 0.00382832, 0.00377292, 0.00452173, 0.00281757,\n",
              "        0.00283026, 0.00280612, 0.00249788, 0.00266129, 0.00273628,\n",
              "        0.00279526, 0.00213535, 0.00298299, 0.003023  , 0.00192162,\n",
              "        0.00198628, 0.00305233, 0.00303012, 0.00157324, 0.00299017,\n",
              "        0.00529858, 0.00529858, 0.00147081, 0.00356304, 0.00529574,\n",
              "        0.00529858, 0.00164779, 0.00155967, 0.00387069, 0.00372369,\n",
              "        0.00368637, 0.00384638, 0.00349207, 0.0035087 , 0.00365858,\n",
              "        0.00292283, 0.0028143 , 0.00296116, 0.00272283, 0.00188893,\n",
              "        0.00284092, 0.00283522, 0.00287181, 0.00292803, 0.00298435,\n",
              "        0.00296891, 0.00183413, 0.0037619 , 0.0030117 , 0.00303012,\n",
              "        0.0016229 , 0.00337256, 0.00529858, 0.00529858, 0.00170741,\n",
              "        0.00308512, 0.00529858, 0.00529858, 0.00154886, 0.00286078])}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dBPF3eRkLqXj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "4448c7d3-728d-4db5-87ef-5dae3625d119"
      },
      "source": [
        "print('Best score: {}'.format(grid_search.best_score_))\n",
        "print('Best parameters: {}'.format(grid_search.best_params_))\n",
        "\n",
        "best_dtc = grid_search.best_estimator_\n",
        "best_dtc"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best score: 0.5362064362643515\n",
            "Best parameters: {'C': 0.1, 'dual': False, 'max_iter': 200, 'penalty': 'l2', 'solver': 'saga', 'tol': 0.0001}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=200,\n",
              "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=None, solver='saga', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jNmMjs8SLqXu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "4eecda84-6b5e-41b7-fcc3-51eb3fcc1af7"
      },
      "source": [
        "my_model=best_dtc\n",
        "my_model.fit(train_X_imp, train_y)\n",
        "my_model.score(train_X_imp, train_y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6638005159071367"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_IeZREYLqX6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "785fe716-b7ba-4b86-f8f6-b2b50cee3b95"
      },
      "source": [
        "y_pred = my_model.predict(train_X_imp)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(train_y, y_pred))\n",
        "\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(train_y, y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Confusion Matrix:\n",
            "[[ 247  958]\n",
            " [ 215 2069]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.53      0.20      0.30      1205\n",
            "           1       0.68      0.91      0.78      2284\n",
            "\n",
            "    accuracy                           0.66      3489\n",
            "   macro avg       0.61      0.56      0.54      3489\n",
            "weighted avg       0.63      0.66      0.61      3489\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97UBgbYNLqYG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "outputId": "eff92e99-feca-413a-a985-1f995073c1fa"
      },
      "source": [
        "# The snippet below will retrieve the feature importances from the model and make them into a DataFrame.\n",
        "feature_importances = pd.DataFrame(my_model.feature_importances_,\n",
        "                                   index = train_X.columns,\n",
        "                                   columns=['importance']).sort_values('importance', ascending=False)\n",
        "feature_importances"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-86f4e5daee72>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# The snippet below will retrieve the feature importances from the model and make them into a DataFrame.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m feature_importances = pd.DataFrame(my_model.feature_importances_,\n\u001b[0m\u001b[1;32m      3\u001b[0m                                    \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_X\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                    columns=['importance']).sort_values('importance', ascending=False)\n\u001b[1;32m      5\u001b[0m \u001b[0mfeature_importances\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'LogisticRegression' object has no attribute 'feature_importances_'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "S5SUv2uHe47m",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c35feed8-6dbb-4abc-8cb7-daccbf91e472"
      },
      "source": [
        "# generate a submission file\n",
        "generateSubmission(my_model,'LogisticRegression2.csv', \"User defined logistic regression evaluated with Grid Search and SCORE\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 1 1 ... 1 1 1]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
