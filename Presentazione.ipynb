{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Presentazione.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "ZVpoC2llOkzj",
        "ez6JFVwd-3q-",
        "dZOaStpGijff",
        "oMQfMmwwP009",
        "jNkqavy1pzVJ",
        "9qHJUN2SGUp7",
        "RMKHcKDrI7aU",
        "09lLAwKAhy1l",
        "w52dGG9JeAtV",
        "xDM92Z-ed6Da",
        "Bm3V0VPGhl4g"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AndreaBertoglio/MLDM/blob/master/Presentazione.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVpoC2llOkzj",
        "colab_type": "text"
      },
      "source": [
        "# **Import**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZI9-kt4ALqSO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d26ec757-a51c-4b75-96a7-368372dfd6dc"
      },
      "source": [
        "import pip\n",
        "import sys\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import graphviz\n",
        "import matplotlib.pyplot as plt\n",
        "print(\"Random number with seed 2020\")\n",
        "# first call\n",
        "random.seed(2020)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Random number with seed 2020\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ez6JFVwd-3q-",
        "colab_type": "text"
      },
      "source": [
        "# **Pre-processing**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PK9gO4JyMwQa",
        "colab_type": "text"
      },
      "source": [
        "Per migliorare le prestazioni dei modelli utilizzati per l'apprendimento, è stato necessario effettuare una fase di pre-processing dei dati. In particolare, ci siamo occupati per la maggior parte di normalizzazione dei dati, gestione di dati mancanti, ricerca di eventuali outliers che interferissero con gli algoritmi di apprendimento, ed in misura minore di feature construction.\n",
        "\n",
        "Link al notebook: https://github.com/AndreaBertoglio/MLDM/blob/master/Pre-processing/Data_Preprocessing.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_1qPlAOMyP5",
        "colab_type": "text"
      },
      "source": [
        "## Analisi e normalizzazione"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCtZiZckBKfq",
        "colab_type": "text"
      },
      "source": [
        "In prima battuta abbiamo fatto una rapida analisi dei dati, notando fin da subito che alcune feature del dataset presentavano distribuzioni di dati anomale, ad esempio se consideriamo la feature \"density\", si possono notare valori compresi tra valori vicini a 1 e valori vicini a 1000, mentre si nota una completa assenza di dati nei valori intermedi. Questo è stato probabilmente causato dall'utilizzo di unità di misura diverse per record diversi (verosimilmente mg/l e g/l nel caso di esempio). Un discorso simile vale anche per la \"volatile acidity\", anch'essa presenta valori disomogenei probabilmente dovuti a diverse unità di misura. Di conseguenza, per prima cosa sono state uniformate le unità di misura di ogni singola feature.\n",
        "\n",
        "Come secondo fatto, si è notato che le varie feature tra loro, avevano valori molto diversi dato che misurando grandezze differenti si usano misure differenti (ad esempio per densità e pH), e quindi difficili da confrontare. Quindi è stato necessario normalizzare i dati per consentirne una migliore valutazione. Per la normalizzazione si sono visti due approcci distinti, uno che prevede la riscalatura dei valori su un range compreso tra 0 e 1, e un secondo che riscala i dati su una distribuzione normale con media 0 e varianza 1. I risultati migliori sono stati ottenuti con il secondo approccio, quindi d'ora in avanti si considererà solo quello.\n",
        "\n",
        "Anche solo con questi due accorgimenti in fase di pre-processing le prestazioni dei modelli addestrati sono migliorate sensibilmente.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DoNVVkUM6ve",
        "colab_type": "text"
      },
      "source": [
        "## Gestione valori fuori scala"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4-Ba0-tskJc",
        "colab_type": "text"
      },
      "source": [
        "Inoltre, dall'analisi dei dati è risultato che anche dopo la normalizzazione alcuni valori risultavano \"fuori scala\", ovvero risultavano molto distanti dalla maggior parte degli altri valori. Per ovviare al problema è stato deciso di rimuovere quei valori che risultavano fuori scala e gestirli come missing values.\n",
        "Il punto cruciale di questa gestione è stato stabilire la giusta soglia per cui considerare un valore fuori scala oppure no. Per far ciò abbiamo analizzato i valori su diverse soglie.\n",
        "\n",
        "Grafico 1: tabelle valori fuori scala\n",
        "![GRAFICO1](https://raw.githubusercontent.com/AndreaBertoglio/MLDM/master/Immagini%20Presentazione/Tabelle%20valori%20fuori%20scala.PNG)\n",
        "\n",
        "Grafico 2: valori fuori scala totali\n",
        "![GRAFICO2](https://raw.githubusercontent.com/AndreaBertoglio/MLDM/master/Immagini%20Presentazione/Grafico%20percentuale%20valori%20fuori%20scala%20totali.PNG)\n",
        "\n",
        "Grafico 3: valori fuori scala\n",
        "![GRAFICO3](https://raw.githubusercontent.com/AndreaBertoglio/MLDM/master/Immagini%20Presentazione/Grafico%20valori%20fuori%20scala%20per%20ogni%20feature.PNG)\n",
        "\n",
        "Grafico 4: particolare del grafico 3\n",
        "![GRAFICO4](https://raw.githubusercontent.com/AndreaBertoglio/MLDM/master/Immagini%20Presentazione/Grafico%20valori%20fuori%20scala%20per%20ogni%20feature%20zoom.PNG)\n",
        "\n",
        "Da queste analisi abbiamo ritenuto opportuno non utilizzare soglie troppo basse, poiché in questo modo avremmo potuto modificare anche dati corretti, e notando che con soglie superiori a 3.5 il numero di valori fuori scala non diminuiva significativamente, abbiamo ritenuto opportuno creare 3 dataset differenti con soglia rispettivamente a 3, 3.5 e 4. I migliori risultati si sono ottenuti comunque con la soglia di 3.5\n",
        "Come detto in precedenza i dati “fuori scala” sono stati cancellati e considerati missing values.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjnDTLyANAgz",
        "colab_type": "text"
      },
      "source": [
        "## Missing values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edALQXrgv0q-",
        "colab_type": "text"
      },
      "source": [
        "Per la gestione dei missing values è stato dapprima utilizzato una semplice strategia di gestione sostituendo ogni valore mancante con la media degli altri valori, ed in seguito altre strategie abbastanza semplici, come il valore più frequente. Tuttavia, tali strategie risultavano poco efficaci in quanto generavano dati non veritieri o non rilevanti, quindi siamo passati ad un approccio leggermente più complesso con un imputer iterativo, che modella ogni feature con missing values in funzione di altre feature. Il processo avviene in modo iterativo: ad ogni passo, una colonna delle feature è designata come output y e le altre colonne sono trattate come input X. Viene creato un regressore sulla base di (X, y) per le y conosciute. Poi, il regressore viene usato per predire i missing values di y. Questo viene fatto per ogni caratteristica in modo iterativo, e poi è ripetuto più volte per avere una maggiore accuratezza."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFff0gwgND3p",
        "colab_type": "text"
      },
      "source": [
        "## Outliers detection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_w7thVdiv7Hx",
        "colab_type": "text"
      },
      "source": [
        "Successivamente abbiamo provato a evidenziare quei dati che risultavano particolarmente diversi degli altri della stessa classe, ad esempio un elemento di classe “good”, ma che era molto distante dagli altri elementi della stessa classe e più simile a quelli di classe “bad”.\n",
        "Per fare ciò abbiamo implementato un algoritmo per il riconoscimento di questi elementi, gli outliers. Tale algoritmo è basato sulla distanza di ogni elemento dagli altri elementi della stessa classe, in particolare si calcola la media delle distanze di ogni elemento da tutti gli altri. La distanza utilizzata è stata la distanza euclidea, e per la decisione della soglia è stato fatto un ragionamento simile a quello per la gestione dei valori fuori scala.\n",
        "\n",
        "Grafico 5: Tabella outliers detection\n",
        "![GRAFICO5](https://raw.githubusercontent.com/AndreaBertoglio/MLDM/master/Immagini%20Presentazione/Tabella%20outliers.PNG)\n",
        "\n",
        "\n",
        "Grafico 6: Outliers detection\n",
        "![GRAFICO6](https://raw.githubusercontent.com/AndreaBertoglio/MLDM/master/Immagini%20Presentazione/Grafico%20outliers%20a%20confronto%201%25.png)\n",
        "\n",
        "Nella valutazione per la scelta della soglia abbiamo anche considerato l’ordine di esecuzione delle varie fasi di pre-processing. Ovviamente per prima avviene la normalizzazione dei dati e la gestione dei missing values, ma l’identificazione di outliers e la gestione dei valori fuori scala può essere fatta in ordine qualsiasi.\n",
        "Dai risultati ottenuti si può notare come la gestione di valori fuori scala riduca notevolmente gli outliers identificati, di conseguenza abbiamo deciso di effettuare prima la gestione dei fuori scala e successivamente quella degli outliers.\n",
        "Per la decisione sulla soglia si nota come con soglie inferiori a 20 tutti i dati sono considerati outliers, mentre la riduzione più consistente si ha fino a un valore di 50. Quindi, non volendo eliminare una parte troppo consistente del dataset, abbiamo deciso di eliminare fino ad un massimo dell'1% dei dati e anche in questo caso abbiamo creato diversi dataset con soglie di 40, 50 e 60.\n",
        "Una volta identificati gli elementi outliers, la loro nostra decisione è stata semplicemente quella di non considerare tali elementi nei nostri algoritmi di apprendimento e quindi di eliminarli dal dataset.\n",
        "Alla luce di ciò le prestazioni migliori sono state ottenute con la soglia pari a 40, ovvero considerando outliers tutti gli elementi con distanza media dagli altri superiore a 40.\n",
        "Con queste operazioni aggiuntive le prestazioni dei modelli hanno subito un leggero miglioramento, ovviamente meno significativo per i modelli che per loro natura sono meno sensibili agli outliers (ad esempio alberi di decisione e random forest), e più marcati in modelli più sensibili come le Support Vector Machine.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L88_hEOZNQnQ",
        "colab_type": "text"
      },
      "source": [
        "## Feature creation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0zOkcM7f7bP",
        "colab_type": "text"
      },
      "source": [
        "### Clustering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14ZPgL8jxMY9",
        "colab_type": "text"
      },
      "source": [
        "Infine, abbiamo provato un approccio di feature creation, in particolare utilizzando un semplice algoritmo di clustering e utilizzando la suddivisione in cluster come feature aggiuntiva. L’algoritmo scelto è stato il K-means, con 10 cluster.\n",
        "Tuttavia, non abbiamo riscontrato significativi miglioramenti, ad eccezzione che in alberi di decisione e random forest, motivo per cui non è stato approfondito su altri metodi."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHsVRMAPNUUU",
        "colab_type": "text"
      },
      "source": [
        "#### Link al notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPvSnbLr19u-",
        "colab_type": "text"
      },
      "source": [
        "LINK AL NOTEBOOK USATO PER LA FEATURE CONSTRUCTION\n",
        "\n",
        "https://github.com/AndreaBertoglio/MLDM/blob/master/Pre-processing/ClusterASFeature.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-oK208nogBvm",
        "colab_type": "text"
      },
      "source": [
        "### Feature construction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ft0Mwx--22kT",
        "colab_type": "text"
      },
      "source": [
        "In alternativa abbiamo tentato un approccio di feature construction basato sull’analisi delle feature disponibili, notando come “fixed acidity” sia fortemente correlata con “citric acid” e “density”. Di conseguenza abbiamo pensato di utilizzare due nuove feature che fossero combinazione delle precedenti:\n",
        "\n",
        "“citric acidity” = “fixed acidity” * “citric acid”\n",
        "\n",
        "“density acidity” = “fixed acidity” * “density”\n",
        "\n",
        "Anche questo approccio però si è rivelato poco efficace e quindi non è stato approffondito e utilizzato."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O83J6KLmNXHT",
        "colab_type": "text"
      },
      "source": [
        "#### Codice"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xOCv6Qsx2v43",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def featureConstruction(dataset):\n",
        "  \n",
        "  citricacid = dataset['fixed.acidity'] * dataset['citric.acid']\n",
        "  citric_acidity = pd.DataFrame(citricacid, columns=['citric_accidity'])\n",
        "\n",
        "  density_acidity = dataset['fixed.acidity'] * dataset['density']\n",
        "  density_acidity = pd.DataFrame(density_acidity, columns=['density_acidity'])\n",
        "\n",
        "  datafinal = dataset.join(citric_acidity).join(density_acidity)\n",
        "\n",
        "  return datafinal"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZOaStpGijff",
        "colab_type": "text"
      },
      "source": [
        "# **Caricamento dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zIDnaVnAiQIK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainFilePath='https://raw.githubusercontent.com/AndreaBertoglio/MLDM/master/Pre-processing/Data%20Set%20elaborati/SetSoloOutlierPerFeature_soglia3%2C5.csv'\n",
        "train = pd.read_csv(trainFilePath)\n",
        "\n",
        "#train[\"Quality\"] = np.where(train[\"Quality\"].str.contains(\"Good\"), 1, 0)\n",
        "train[\"Quality\"] = np.where(train[\"Quality\"].astype(str).str.contains(\"1.0\"), 1, 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXtbtmDait-Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "testFilePath='https://raw.githubusercontent.com/serivan/mldmlab/master/Datasets/Kaggle2020/test.csv'\n",
        "test = pd.read_csv(testFilePath)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_VJjds_jQ3Q",
        "colab_type": "text"
      },
      "source": [
        "## Divisione dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJIUVDySLqTY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import IterativeImputer\n",
        "\n",
        "train_y = train.Quality\n",
        "\n",
        "predictor_cols = ['fixed.acidity','volatile.acidity','citric.acid','residual.sugar','chlorides','free.sulfur.dioxide','total.sulfur.dioxide','density','pH','sulphates','alcohol']\n",
        "\n",
        "train_x = train[predictor_cols]\n",
        "\n",
        "#imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
        "imp = IterativeImputer(missing_values=np.nan, max_iter=100, initial_strategy='mean')\n",
        "imp = imp.fit(train_x)\n",
        "\n",
        "# Impute our data, then train\n",
        "train_X_imp = imp.transform(train_x)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "#Uso l'80% dei dati per train e il restante 20% per test\n",
        "xTrain, xTest, yTrain, yTest = train_test_split(train_X_imp, train_y, train_size = 0.8, random_state = 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMQfMmwwP009",
        "colab_type": "text"
      },
      "source": [
        "# **Grid search e K-fold cross validation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6RULLgjP7Fs",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Nel machine learning,l'**ottimizzazione degli iperparametri** è il problema di scegliere un insieme di iperparametri ottimali per un algoritmo di apprendimento. Un iperparametro è un parametro il cui valore viene utilizzato per controllare il processo di apprendimento. \\\\\n",
        "Lo stesso modello di machine learning può richiedere vincoli, pesi o velocità di apprendimento diversi per generalizzare modelli di dati diversi. Queste misure sono chiamate iperparametri e devono essere regolate in modo che il modello possa risolvere in modo ottimale il problema dell'apprendimento automatico. L'ottimizzazione degli iperparametri trova una tupla di iperparametri che produce un modello ottimale che riduce al minimo una funzione di perdita predefinita sui dati indipendenti. La funzione obiettivo accetta una tupla di iperparametri e restituisce la \"loss\" associata. La *cross-validation* viene spesso utilizzata per stimare questa prestazione di generalizzazione."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ophqyNjsms_-",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Il modo tradizionale di eseguire l'ottimizzazione degli iperparametri è la **Grid search**, o sweep di parametri, che è semplicemente una ricerca esaustiva attraverso un sottoinsieme specificato manualmente dello spazio iperparametrico di un algoritmo di apprendimento. Un algoritmo di ricerca sulla griglia deve essere guidato da una metrica sulle prestazioni, in genere misurata mediante cross validation sul training set o valutazione su un validation set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eiDxIXp9P-2N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Pseudocodice standard Grid\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "# parameters = {'iperparametro1':['lista valori da testare,...'],\n",
        "#             'iperparametro2':['lista valori da testare,...'],\n",
        "#              ...\n",
        "#              }\n",
        "#model = CreazioneModello()\n",
        "\n",
        "#grid = GridSearchCV(model, parameters)\n",
        "#grid.fit(xTrain, yTrain)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPcCIpzQn8rP",
        "colab_type": "text"
      },
      "source": [
        "La **Cross validation** è una tecnica di convalida di modelli simili per valutare come i risultati di un'analisi si generalizzerà su un set di dati indipendente. Viene utilizzato principalmente in ambienti in cui l'obiettivo è la previsione e si desidera stimare la precisione con cui un modello predittivo lavorerà nella pratica. In un problema di previsione, a un modello viene solitamente assegnato un set di dati noti su cui viene eseguito il training (training set) e un set di dati sconosciuti (visualizzati per la prima volta) rispetto al quale viene testato il modello (chiamato validation set). L'obiettivo della cross validation è quello di testare la capacità del modello di prevedere nuovi dati che non sono stati utilizzati per la stima, al fine di evitare problemi come l'overfitting o il bias di selezione e per dare un'idea di come il modello si generalizzerà a un dataset indipendente (cioè, un dataset sconosciuto, ad esempio da un problema reale).\\\\\n",
        "Un ciclo di cross validation prevede il partizionamento di un campione di dati in sottoinsiemi complementari, l'esecuzione dell'analisi su un sottoinsieme e la convalida dell'analisi sull'altro sottoinsieme. Per ridurre la variabilità, nella maggior parte dei metodi vengono eseguiti più cicli di cross validation utilizzando partizioni diverse e i risultati della convalida vengono combinati (ad esempio, tramite una media) nei cicli per fornire una stima delle prestazioni predittive del modello.\n",
        "\n",
        "In sintesi, la cross validation combina  misure di fitness nella previsione per ottenere una stima più accurata delle prestazioni di previsione del modello. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JwePyUXXP812",
        "colab_type": "text"
      },
      "source": [
        "Nella **K-fold cross validation**, il campione originale viene suddiviso in modo casuale in k sottocampioni di uguale dimensione. Dei k sottocampioni, un singolo sottocampione viene conservato come validation set per testare il modello e i restanti k-1 sottocampioni vengono utilizzati come training set. Il processo di cross validation viene quindi ripetuto k volte, con ciascuno dei k sottocampioni utilizzato esattamente una volta come validation set. I k risultati possono quindi essere mediati per produrre una singola stima. Il vantaggio di questo metodo è che tutte le osservazioni vengono utilizzate sia per il train che per la validation e ogni osservazione viene utilizzata per la convalida esattamente una volta. La 10-fold cross validation è la più utilizzata, ma in generale k rimane un parametro variabile."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygv5kAJaQA2_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2bf4630b-03c8-4ea4-9c9c-81d2abe59678"
      },
      "source": [
        "# Codice standard k-fold\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "cross_validation = StratifiedKFold(n_splits=10)\n",
        "cross_validation.get_n_splits(xTrain, yTrain)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44c41QF1vNOQ",
        "colab_type": "text"
      },
      "source": [
        "Come accennato in precedenta la K-fold cross validation viene spesso utilizzata insieme alla Grid Search, in modo da ottenere il modello con le prestazioni migliori tra tutte le possibili combinazioni di iperparametri impostati manualmente."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PTPZi2KVvyAo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Pseudocodice Grid search + k-fold\n",
        "# grid_search= GridSearchCV(modello, param_grid=parameter_grid, cv=cross_validation, scoring=SCORING,return_train_score=True, refit='f1')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5mRdXu0wGI1",
        "colab_type": "text"
      },
      "source": [
        "Notiamo come sia possibile settare altri parametri per ottimizzare il lavoro della grid search, ad esempio utilizzando come metrica di classificazione la f1 measure."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "seWxT1TRGRpO",
        "colab_type": "text"
      },
      "source": [
        "# **Metodi**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jNkqavy1pzVJ"
      },
      "source": [
        "\n",
        "## *Decision Tree Classifier*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Bj4UDG71pzVK"
      },
      "source": [
        "Gli alberi di decisione sono un metodo di classificazione di records, i quali contengono vari attributi, tra cui il più importante è la classe. Quest'ultimo è, infatti, quello che deve essere predetto dall'algoritmo, sulla base dei valori assunti dagli altri attributi. Nel nostro caso la classe è rappresentata da un valore binario: \"Good\" e \"Bad\".\n",
        "Gli alberi di decisione sono formati da vari nodi, ognuno dei quali rappresenta un test su uno specifico attributo. Le alternative che si diramano dai nodi corrispondono ai possibili valori che l'attributo può assumere. Questa suddivisione può essere binaria, quindi diramazione in due soli rami, oppure multy-way, quindi una suddivisione in k percorsi, con k>2. La scelta sullo specifico modo di splittare viene effettuata in base al tipo di attributi che formano i nostri records: una divisione binaria si presta bene per quelli attributi che assumono un valore booleano, un valore binario oppure un valore numerico, scegliendo, in quest'ultimo caso, un valore preciso in cui suddividere i due insiemi. La suddivisione multi-way è adatta nei casi in cui un valore può assumere tanti valori differenti, creando così una ramificazione per ogni possibile valore assunto dall'attributo.\n",
        "L'altezza massima che può assumere l'albero è pari al numero di attributi differenti contenuti nei record, effettuando, in tal modo, un controllo per ogni singolo attributo. I nodi foglia rappresentano la classe, cioè il valore obiettivo che vogliamo predire. \n",
        "\n",
        "La scelta della suddivisione dei nodi è basata su alcune misure di similarità: Gini Index, Entropia oppure errore di classificazione. In queste l'obiettivo che si vuole ottenere è quello di avere la maggior omogeneità possibile, riducendo sempre più l'impurità. Questo viene ottenuto quando all'interno di un nodo la maggior parte delle istanze, se non tutte, appartengono alla stessa classe, evitando così situazioni in cui si ha alta eterogeneità di classi all'interno del medesimo nodo.\n",
        "\n",
        "La classificazione di una nuova istanza avviene partendo dal nodo radice, il primo realizzato, man mano scendendo seguendo il percorso designato in base ai valori dei vari attributi del record che stiamo considerando. Pertanto sia la classificazione dei records che la spiegazione della stessa risultano molto semplici e intuitive, formate, per l'appunto, da una sequenza di confronti all'interno dei nodi, che portano l'istanza ad arrivare ad un determinato nodo foglia contrassegnato con una specifica classe."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vhjljlIQpzVL",
        "colab": {}
      },
      "source": [
        "# Codice standard\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "dtc = DecisionTreeClassifier()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DCZBc-zgpzVN"
      },
      "source": [
        "Abbiamo usato i Decision Trees principalmente all'interno del contesto della Grid Search, modificando i parametri maggiormente significativi e quelli che ci portavano ad ottenere i risultati migliori:\n",
        "\n",
        "*   **Criterion:** Indica la funzione che vione usata per misurare la qualità dello split. I criteri possibili sono \"Gini\" ed \"Entropy\". Provando ad usarli entrambi abbiamo notato che ottenevamo un public score migliore sfruttando il \"Gini\".\n",
        "*   **Splitter:** Rappresenta la strategia usata per decidere lo split da effettuare ad ogni nodo. Le possibilità sono \"best\" e \"random\". Nel nostro progetto abbaimo sfruttato praticamente sempre best, scegliendo così sempre lo split che portava i migliori risultati in base al criterio scelto.\n",
        "*   **Max Depth:** Indica la profondità massima raggiungibile dall'albero. Questo serve ad impedire overfitting, fermando la creazione dell'albero prima dell'analisi completa di ogni attributo. Se non viene indicato nulla, la ricerca continua finchè le foglie non sono pure (cioè contengono solo elementi della stessa classe) o finchè tutte le foglie non contengono meno di \"*Min samples split*\" elementi. Effettuando varie prove abbiamo constatato che i risultati migliori si ottenevano con una profondità che si aggira tra i 5 e i 7 nodi.\n",
        "*   **Min Samples Split:** Rappresenta il numero minimo di istanze richieste in un nodo per poterlo splittare: se il numero è un intero allora viene preso quel numero come il minimo, se invece è un reale *Min samples split* assume un valore pari allp'intero superiore del prodotto di tale reale per il numero totale di istanze. Nelle nostra prove abbiamo notato che spesso il valore di default di 2 assegnato a questo parametro è troppo basso e porta ad overfitting. Quindi abbiamo alzato questo limite portandolo ad un valore tra 10 e 30, ottenendo migliori risultati.\n",
        "*   **Min Samples Leaf:** Indica il numero minimo numero di istanze che servono per poter essere considerato un nodo foglia. Uno split può essere effettuato solo se tutti i nodi in cui il nodo padre viene diviso contengono almeno \"*Min samples leaf* istanze. Come prima serve principalmente ad evitare che si crei overfitting, pertanto lo abbiamo usato in alternativa a \"*Min samples split*\", ottenendo risultati migliori agendo su quest'ultimo.\n",
        "*   **Max Features:** Il numero di attributi da considerare quando cerco il best split. Noi abbiamo utilizzato spesso un numero reale per questo parametro, per indicare la frazione di attributi da considerare, ottenendo i migliori risultati con un valore compreso tra 0.5 e 0.7.\n",
        "\n",
        "Abbiamo provato a modificare la percentuale di istanze assegnate al training set e quelle assegnate al testset, ricavando le risposte migliori da parte del classificatore quando assegnavamo al training set una percentuale intorno al 70/80%. \n",
        "\n",
        "Inoltre abbiamo sfruttato due diversi tipi di Imputer, \"Simple\" e \"Iterative\", notando subito miglioramenti dal punto di vista dello score sfruttando quest'ultimo. Il \"Simple Imputer\" stima i valori di una feature considerando solo i non-missing value legati a quella feature, mentre l'\"Iterative Imputer\" stima ogni feature usando tutte le altre, modellando ogni attributo contenente missing values come una funzione degli altri attributi in una schedulazione round-robin.\n",
        "\n",
        "Il risultato migliore l'abbiamo ottenuto sfruttando il 75% del dataset per il train e il restante 25% per il test.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tJgHqi_epzVN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "b580e63f-856f-420c-8518-120e85a90bd1"
      },
      "source": [
        "# Best risultato\n",
        "\n",
        "#User defined decision tree evaluated with Grid Search \n",
        "\n",
        "parameter_grid = {\n",
        "    'criterion':['gini','entropy'], \n",
        "    'splitter': ['best'], \n",
        "    'max_depth': [5,7], \n",
        "    'min_samples_split':[10,20,30,0.1], \n",
        "    'max_features':[0.2,0.4,0.7] \n",
        "    }\n",
        "\n",
        "cross_validation = StratifiedKFold(n_splits=10)\n",
        "cross_validation.get_n_splits(train_X_imp, train_y)\n",
        "#Create the scoring dictionary\n",
        "SCORING = {'accuracy': 'accuracy',\n",
        "'balanced_accuracy': 'balanced_accuracy',\n",
        "'precision': 'precision_macro',\n",
        "'recall': 'recall_macro',\n",
        "'f1': 'f1_macro'}\n",
        "\n",
        "grid_search = GridSearchCV(dtc, param_grid=parameter_grid, cv=cross_validation, scoring=SCORING,return_train_score=True, refit='f1')\n",
        "\n",
        "grid_search.fit(xTrain, yTrain)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=StratifiedKFold(n_splits=10, random_state=None, shuffle=False),\n",
              "             error_score=nan,\n",
              "             estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None,\n",
              "                                              criterion='gini', max_depth=None,\n",
              "                                              max_features=None,\n",
              "                                              max_leaf_nodes=None,\n",
              "                                              min_impurity_decrease=0.0,\n",
              "                                              min_impurity_split=None,\n",
              "                                              min_samples_leaf=1,\n",
              "                                              min_samples_split=2,\n",
              "                                              min_weight_fraction_leaf=0.0,\n",
              "                                              pres...\n",
              "             param_grid={'criterion': ['gini', 'entropy'], 'max_depth': [5, 7],\n",
              "                         'max_features': [0.2, 0.4, 0.7],\n",
              "                         'min_samples_split': [10, 20, 30, 0.1],\n",
              "                         'splitter': ['best']},\n",
              "             pre_dispatch='2*n_jobs', refit='f1', return_train_score=True,\n",
              "             scoring={'accuracy': 'accuracy',\n",
              "                      'balanced_accuracy': 'balanced_accuracy',\n",
              "                      'f1': 'f1_macro', 'precision': 'precision_macro',\n",
              "                      'recall': 'recall_macro'},\n",
              "             verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oxFp17mshdQa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "0fed4f3f-3513-4eb7-e125-47cbd03f6751"
      },
      "source": [
        "print('Best score: {}'.format(grid_search.best_score_))\n",
        "print('Best parameters: {}'.format(grid_search.best_params_))\n",
        "\n",
        "best_dtc = grid_search.best_estimator_\n",
        "my_model=best_dtc\n",
        "my_model.fit(xTrain, yTrain)\n",
        "my_model.score(xTest, yTest)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best score: 0.7303925345308111\n",
            "Best parameters: {'criterion': 'entropy', 'max_depth': 5, 'max_features': 0.7, 'min_samples_split': 20, 'splitter': 'best'}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7521489971346705"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3P3szY-phLWI",
        "colab_type": "text"
      },
      "source": [
        "Public Score: **0.74785**\n",
        "\n",
        "Private Score: **0.76319**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7RK9qSfdpzVR"
      },
      "source": [
        "Il Decision Tree Classifier si è rivelato un metodo abbastanza facile da usare e da capire, riuscendo ad individuare subito i parametri che impattavano di più sulla soluzione. I risultati ottenuti, però, non sono ottimi, infatti sono di molto inferiori a quelli dei metodi d'insieme, ma tra i metodi non d'insieme risulta uno dei migliori."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qHJUN2SGUp7",
        "colab_type": "text"
      },
      "source": [
        "## *Linear Regression*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fX_BLWDyGfh4",
        "colab_type": "text"
      },
      "source": [
        "La *Linear regression* è un semplice approccio al \"supervised learning\", prevede che la dipendenza tra Y e X1,X2,…,Xp sia lineare. \\\\\n",
        "Il metodo infatti approssima l'andamento dei dati con una retta, cercando di minimizzare l'errore."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7TeGkoR8HCmU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Codice standard\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "lin_reg =LinearRegression()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-gnj_sGHLoj",
        "colab_type": "text"
      },
      "source": [
        "Il problema della LinearRegression è che non può essere utilizzata per la classificazione binaria, per questo motivo abbiamo deciso di utilizzare il RidgeClassifier: questo classificatore prima converte i target values in {-1, 1} e poi tratta il problema con la regressione lineare. \\\\\n",
        "\n",
        "Sfruttiamo la grid search per valutare l'algoritmo con vari tipi di **solver**: \\\\\n",
        "* ‘auto’ chooses the solver automatically based on the type of data. \\\\\n",
        "* svd’ uses a Singular Value Decomposition of X to compute the Ridge  coefficients. More stable for singular matrices than ‘cholesky’. \\\\\n",
        "* ‘cholesky’ uses the standard scipy.linalg.solve function to obtain a closed-form solution. \\\\\n",
        "* ‘sparse_cg’ uses the conjugate gradient solver as found in scipy.sparse.linalg.cg. As an iterative algorithm, this solver is more appropriate than ‘cholesky’ for large-scale data (possibility to set tol and max_iter). \\\\\n",
        "* ‘lsqr’ uses the dedicated regularized least-squares routine scipy.sparse.linalg.lsqr. It is the fastest and uses an iterative procedure."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sqi8ozlSHN9M",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "cc072dbe-4ffd-4ae2-92e0-5ebf36e72c38"
      },
      "source": [
        "# Best risultato\n",
        "from sklearn.linear_model import RidgeClassifier\n",
        "\n",
        "lin_reg =RidgeClassifier()\n",
        "parameter_grid={'solver':['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg']}\n",
        "cross_validation = StratifiedKFold(n_splits=10)\n",
        "cross_validation.get_n_splits(xTrain, yTrain)\n",
        "SCORING = {'accuracy': 'accuracy',\n",
        "'balanced_accuracy': 'balanced_accuracy',\n",
        "'precision': 'precision_macro',\n",
        "'recall': 'recall_macro',\n",
        "'f1': 'f1_macro'}\n",
        "grid_search= GridSearchCV(lin_reg, param_grid=parameter_grid, cv=cross_validation, scoring=SCORING,return_train_score=True, refit='f1')\n",
        "grid_search.fit(xTrain, yTrain)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=StratifiedKFold(n_splits=10, random_state=None, shuffle=False),\n",
              "             error_score=nan,\n",
              "             estimator=RidgeClassifier(alpha=1.0, class_weight=None,\n",
              "                                       copy_X=True, fit_intercept=True,\n",
              "                                       max_iter=None, normalize=False,\n",
              "                                       random_state=None, solver='auto',\n",
              "                                       tol=0.001),\n",
              "             iid='deprecated', n_jobs=None,\n",
              "             param_grid={'solver': ['auto', 'svd', 'cholesky', 'lsqr',\n",
              "                                    'sparse_cg']},\n",
              "             pre_dispatch='2*n_jobs', refit='f1', return_train_score=True,\n",
              "             scoring={'accuracy': 'accuracy',\n",
              "                      'balanced_accuracy': 'balanced_accuracy',\n",
              "                      'f1': 'f1_macro', 'precision': 'precision_macro',\n",
              "                      'recall': 'recall_macro'},\n",
              "             verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jNmMjs8SLqXu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "eeb6c3f4-fc81-47ac-a918-4f6efb10fb82"
      },
      "source": [
        "# Score locale\n",
        "print('Best score: {}'.format(grid_search.best_score_))\n",
        "print('Best parameters: {}'.format(grid_search.best_params_))\n",
        "\n",
        "best_dtc = grid_search.best_estimator_\n",
        "my_model=best_dtc\n",
        "my_model.fit(xTrain, yTrain)\n",
        "my_model.score(xTest, yTest)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best score: 0.7068422566464572\n",
            "Best parameters: {'solver': 'auto'}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7449856733524355"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOSAOGoYHQG9",
        "colab_type": "text"
      },
      "source": [
        "**Public score**: 0.73065 \\\\\n",
        "**Private score**: 0.74846 \\\\\n",
        "Questo metodo ha conseguito risultati scadenti in quanto non si adatta nel miglior modo al nostro problema, limitando la classificazione a un modello lineare."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "RMKHcKDrI7aU"
      },
      "source": [
        "## *Logistic Regression*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "x_V-mTaNI7az"
      },
      "source": [
        "In statistica e in econometria la *Logistic regression* è un modello di regressione nonlineare utilizzato quando la variabile dipendente è di tipo dicotomico. \\\\\n",
        " L'obiettivo del modello è di stabilire la probabilità con cui un'osservazione può generare uno o l'altro valore della variabile dipendente; può inoltre essere utilizzato per classificare le osservazioni, in base alla caratteristiche di queste, in due categorie"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EbeqtGVJI7a2",
        "colab": {}
      },
      "source": [
        "# Codice standard\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "log_reg = LogisticRegression()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "iWpFVW8-I7bJ"
      },
      "source": [
        "Abbiamo deciso di sfruttare questo metodo in quanto, rispetto alla *Linear regression* è più adatto a una classificazione binaria. \\\\\n",
        "É stato possibile sfruttare una parameter grid abbastanza fitta in quanto il metodo ha il pregio di essere relativamente veloce, grazie a ciò è stato possibile testare varie combinazioni di parametri quali: \\\\\n",
        "\n",
        "* 'penalty' per specificare la norma utilizzata nella penalizzazione,\n",
        "* 'dual' per il tipo di formulazione (dual o primal),\n",
        "* 'C' per il peso della regolarizzazione,\n",
        "* 'solver' per il tipo di risolutore, \n",
        "* 'max_iter' per il numero massimo di iterazioni concesse (in modo da evitare l'overfitting).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "g9a_J9t7I7bM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "cde118b5-513a-4484-ce92-f69cb8b7f043"
      },
      "source": [
        "# Best risultato\n",
        "log_reg = LogisticRegression()\n",
        "parameter_grid = {'penalty': ['l1', 'l2'],\n",
        "                  'dual':[True,False],\n",
        "                  'tol': [1e-4,1e-2],\n",
        "                  'C': [0.01,1,3,10,100],\n",
        "                  'solver' : ['saga','lbfgs'],\n",
        "                  'max_iter': [50,100,200]}\n",
        "\n",
        "cross_validation = StratifiedKFold(n_splits=10)\n",
        "cross_validation.get_n_splits(xTrain, yTrain)\n",
        "\n",
        "SCORING = {'accuracy': 'accuracy',\n",
        "'balanced_accuracy': 'balanced_accuracy',\n",
        "'precision': 'precision_macro',\n",
        "'recall': 'recall_macro',\n",
        "'f1': 'f1_macro'}\n",
        "grid_search= GridSearchCV(log_reg, param_grid=parameter_grid, cv=cross_validation, scoring=SCORING,return_train_score=True, refit='f1')\n",
        "grid_search.fit(xTrain, yTrain)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver saga supports only dual=False, got dual=True\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only dual=False, got dual=True\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver saga supports only dual=False, got dual=True\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only dual=False, got dual=True\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver saga supports only dual=False, got dual=True\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only dual=False, got dual=True\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver saga supports only dual=False, got dual=True\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only dual=False, got dual=True\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver saga supports only dual=False, got dual=True\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only dual=False, got dual=True\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=StratifiedKFold(n_splits=10, random_state=None, shuffle=False),\n",
              "             error_score=nan,\n",
              "             estimator=LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
              "                                          fit_intercept=True,\n",
              "                                          intercept_scaling=1, l1_ratio=None,\n",
              "                                          max_iter=100, multi_class='auto',\n",
              "                                          n_jobs=None, penalty='l2',\n",
              "                                          random_state=None, solver='lbfgs',\n",
              "                                          tol=0.0001, verbose=0,\n",
              "                                          warm_start=False),\n",
              "             iid='deprec...\n",
              "             param_grid={'C': [0.01, 1, 3, 10, 100], 'dual': [True, False],\n",
              "                         'max_iter': [50, 100, 200], 'penalty': ['l1', 'l2'],\n",
              "                         'solver': ['saga', 'lbfgs'], 'tol': [0.0001, 0.01]},\n",
              "             pre_dispatch='2*n_jobs', refit='f1', return_train_score=True,\n",
              "             scoring={'accuracy': 'accuracy',\n",
              "                      'balanced_accuracy': 'balanced_accuracy',\n",
              "                      'f1': 'f1_macro', 'precision': 'precision_macro',\n",
              "                      'recall': 'recall_macro'},\n",
              "             verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HNpL6WdTRtSb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "8ac5d177-7a4a-4db6-ec8a-91b0f399ece3"
      },
      "source": [
        "# Score locale\n",
        "print('Best score: {}'.format(grid_search.best_score_))\n",
        "print('Best parameters: {}'.format(grid_search.best_params_))\n",
        "\n",
        "best_dtc = grid_search.best_estimator_\n",
        "my_model=best_dtc\n",
        "my_model.fit(xTrain, yTrain)\n",
        "my_model.score(xTest, yTest)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best score: 0.7064348542087665\n",
            "Best parameters: {'C': 100, 'dual': False, 'max_iter': 200, 'penalty': 'l2', 'solver': 'lbfgs', 'tol': 0.0001}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7363896848137536"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LZXnP32_I7bX"
      },
      "source": [
        "**Public score**: 0.72147 \\\\\n",
        "**Private score**: 0.71633 \\\\\n",
        "Anche questo metodo non è performante in quanto non si adatta nel modo migliore al problema, approssimandolo in modo poco accurato."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "09lLAwKAhy1l"
      },
      "source": [
        "## *Neural network*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2k413hDpnQz",
        "colab_type": "text"
      },
      "source": [
        "Le reti neurali artificiali (ANN), solitamente chiamate semplicemente reti neurali (NN), sono sistemi informatici vagamente ispirati alle reti neurali biologiche che costituiscono il cervello degli animali.\n",
        "Una ANN si basa su una raccolta di unità o nodi collegati chiamati neuroni, che modellano vagamente i neuroni in un cervello biologico. Ogni connessione, come le sinapsi in un cervello, può trasmettere un segnale ad altri neuroni. Un neurone che riceve un segnale lo elabora e può inoltrarlo ai neuroni ad esso collegati. Il \"segnale\" in una connessione è un numero reale e l'uscita di ogni neurone è calcolata da una funzione non lineare dipendente della somma dei suoi ingressi. Neuroni e connessioni hanno tipicamente un peso che si adatta man mano che l'apprendimento procede. Il peso aumenta o diminuisce la potenza del segnale a una connessione. I neuroni possono avere una soglia tale che un segnale viene inviato solo se il segnale aggregato supera quella soglia. In genere, i neuroni vengono aggregati in strati. Livelli diversi possono eseguire trasformazioni diverse sui loro input. I segnali viaggiano dal primo livello (input layer), all'ultimo livello (output layer), possibilmente dopo aver attraversato più livelli intermedi (hidden layer)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uEHkbN3qkY95",
        "colab_type": "text"
      },
      "source": [
        "### Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JzL4S-NPkgxN",
        "colab_type": "text"
      },
      "source": [
        "Keras è un'API per reti neurali di alto livello, scritta in Python e può essere eseguita su TensorFlow, CNTK o Theano. È stato sviluppato con l'obiettivo di consentire una rapida sperimentazione. I vantaggi dell'utilizzo di Keras derivano dal fatto che si concentra sull'essere user-friendly, modulare ed estensibile. \\\\\n",
        "Sfruttandolo è possibile costruire reti neurali in modo semplice ed intuitivo, definendo il numero di neuroni da aggiungere per ogni livello, ognuno con una determinata funzione di attivazione."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7YZS-5FWrUV-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "34bba4ce-362e-4e5a-ffd5-2e540f4ae1e4"
      },
      "source": [
        "import tensorflow as tf\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Dense(15,kernel_initializer='uniform', activation='relu', input_shape=(11,)))\n",
        "model.add(Dense(8, activation='relu'))\n",
        "model.add(Dense(3, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='SGD',\n",
        "              metrics=['accuracy',tf.keras.metrics.AUC()])\n",
        "                   \n",
        "training_phase =model.fit(xTrain, yTrain,epochs=25, batch_size=1,  validation_split=0.2, verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7f9979531598> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: smac attempted to use a functionality that requires module emcee, but it couldn't be loaded. Please install emcee and retry.\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7f9979531598> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: smac attempted to use a functionality that requires module emcee, but it couldn't be loaded. Please install emcee and retry.\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "2226/2232 [============================>.] - ETA: 0s - loss: 0.6464 - accuracy: 0.6604 - auc_1: 0.4871"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7f9978ba8e18> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: smac attempted to use a functionality that requires module emcee, but it couldn't be loaded. Please install emcee and retry.\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7f9978ba8e18> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: smac attempted to use a functionality that requires module emcee, but it couldn't be loaded. Please install emcee and retry.\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "2232/2232 [==============================] - 4s 2ms/step - loss: 0.6464 - accuracy: 0.6604 - auc_1: 0.4871 - val_loss: 0.6532 - val_accuracy: 0.6404 - val_auc_1: 0.5000\n",
            "Epoch 2/25\n",
            "2232/2232 [==============================] - 3s 1ms/step - loss: 0.6407 - accuracy: 0.6622 - auc_1: 0.4680 - val_loss: 0.6545 - val_accuracy: 0.6404 - val_auc_1: 0.5000\n",
            "Epoch 3/25\n",
            "2232/2232 [==============================] - 3s 1ms/step - loss: 0.6404 - accuracy: 0.6622 - auc_1: 0.4877 - val_loss: 0.6545 - val_accuracy: 0.6404 - val_auc_1: 0.5000\n",
            "Epoch 4/25\n",
            "2232/2232 [==============================] - 3s 1ms/step - loss: 0.6402 - accuracy: 0.6622 - auc_1: 0.4938 - val_loss: 0.6532 - val_accuracy: 0.6404 - val_auc_1: 0.5000\n",
            "Epoch 5/25\n",
            "2232/2232 [==============================] - 3s 1ms/step - loss: 0.6403 - accuracy: 0.6622 - auc_1: 0.4940 - val_loss: 0.6532 - val_accuracy: 0.6404 - val_auc_1: 0.5000\n",
            "Epoch 6/25\n",
            "2232/2232 [==============================] - 3s 1ms/step - loss: 0.6405 - accuracy: 0.6622 - auc_1: 0.4835 - val_loss: 0.6559 - val_accuracy: 0.6404 - val_auc_1: 0.5000\n",
            "Epoch 7/25\n",
            "2232/2232 [==============================] - 3s 1ms/step - loss: 0.6405 - accuracy: 0.6622 - auc_1: 0.4866 - val_loss: 0.6552 - val_accuracy: 0.6404 - val_auc_1: 0.5000\n",
            "Epoch 8/25\n",
            "2232/2232 [==============================] - 3s 1ms/step - loss: 0.6399 - accuracy: 0.6622 - auc_1: 0.5001 - val_loss: 0.6574 - val_accuracy: 0.6404 - val_auc_1: 0.5000\n",
            "Epoch 9/25\n",
            "2232/2232 [==============================] - 3s 1ms/step - loss: 0.6404 - accuracy: 0.6622 - auc_1: 0.4951 - val_loss: 0.6541 - val_accuracy: 0.6404 - val_auc_1: 0.5000\n",
            "Epoch 10/25\n",
            "2232/2232 [==============================] - 3s 1ms/step - loss: 0.6403 - accuracy: 0.6622 - auc_1: 0.4874 - val_loss: 0.6539 - val_accuracy: 0.6404 - val_auc_1: 0.5000\n",
            "Epoch 11/25\n",
            "2232/2232 [==============================] - 3s 1ms/step - loss: 0.6403 - accuracy: 0.6622 - auc_1: 0.4892 - val_loss: 0.6564 - val_accuracy: 0.6404 - val_auc_1: 0.5000\n",
            "Epoch 12/25\n",
            "2232/2232 [==============================] - 3s 1ms/step - loss: 0.6399 - accuracy: 0.6622 - auc_1: 0.5055 - val_loss: 0.6532 - val_accuracy: 0.6404 - val_auc_1: 0.5000\n",
            "Epoch 13/25\n",
            "2232/2232 [==============================] - 3s 1ms/step - loss: 0.6405 - accuracy: 0.6622 - auc_1: 0.4880 - val_loss: 0.6552 - val_accuracy: 0.6404 - val_auc_1: 0.5000\n",
            "Epoch 14/25\n",
            "2232/2232 [==============================] - 3s 1ms/step - loss: 0.6405 - accuracy: 0.6622 - auc_1: 0.4871 - val_loss: 0.6541 - val_accuracy: 0.6404 - val_auc_1: 0.5000\n",
            "Epoch 15/25\n",
            "2232/2232 [==============================] - 3s 1ms/step - loss: 0.6403 - accuracy: 0.6622 - auc_1: 0.4885 - val_loss: 0.6561 - val_accuracy: 0.6404 - val_auc_1: 0.5000\n",
            "Epoch 16/25\n",
            "2232/2232 [==============================] - 3s 1ms/step - loss: 0.6400 - accuracy: 0.6622 - auc_1: 0.4969 - val_loss: 0.6589 - val_accuracy: 0.6404 - val_auc_1: 0.5000\n",
            "Epoch 17/25\n",
            "2232/2232 [==============================] - 3s 1ms/step - loss: 0.6403 - accuracy: 0.6622 - auc_1: 0.4990 - val_loss: 0.6533 - val_accuracy: 0.6404 - val_auc_1: 0.5000\n",
            "Epoch 18/25\n",
            "2232/2232 [==============================] - 3s 1ms/step - loss: 0.6404 - accuracy: 0.6622 - auc_1: 0.4852 - val_loss: 0.6534 - val_accuracy: 0.6404 - val_auc_1: 0.5000\n",
            "Epoch 19/25\n",
            "2232/2232 [==============================] - 3s 1ms/step - loss: 0.6404 - accuracy: 0.6622 - auc_1: 0.4885 - val_loss: 0.6543 - val_accuracy: 0.6404 - val_auc_1: 0.5000\n",
            "Epoch 20/25\n",
            "2232/2232 [==============================] - 3s 1ms/step - loss: 0.6395 - accuracy: 0.6622 - auc_1: 0.5090 - val_loss: 0.6536 - val_accuracy: 0.6404 - val_auc_1: 0.5000\n",
            "Epoch 21/25\n",
            "2232/2232 [==============================] - 3s 1ms/step - loss: 0.6409 - accuracy: 0.6622 - auc_1: 0.4785 - val_loss: 0.6538 - val_accuracy: 0.6404 - val_auc_1: 0.5000\n",
            "Epoch 22/25\n",
            "2232/2232 [==============================] - 3s 1ms/step - loss: 0.6405 - accuracy: 0.6622 - auc_1: 0.4743 - val_loss: 0.6543 - val_accuracy: 0.6404 - val_auc_1: 0.5000\n",
            "Epoch 23/25\n",
            "2232/2232 [==============================] - 3s 1ms/step - loss: 0.6403 - accuracy: 0.6622 - auc_1: 0.4941 - val_loss: 0.6533 - val_accuracy: 0.6404 - val_auc_1: 0.5000\n",
            "Epoch 24/25\n",
            "2232/2232 [==============================] - 3s 1ms/step - loss: 0.6405 - accuracy: 0.6622 - auc_1: 0.4815 - val_loss: 0.6539 - val_accuracy: 0.6404 - val_auc_1: 0.5000\n",
            "Epoch 25/25\n",
            "2232/2232 [==============================] - 3s 1ms/step - loss: 0.6402 - accuracy: 0.6622 - auc_1: 0.4947 - val_loss: 0.6544 - val_accuracy: 0.6404 - val_auc_1: 0.5000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hK7UbFtu888V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "outputId": "a7dcf861-daa5-4c5b-c9d1-4e70655655b4"
      },
      "source": [
        "    import matplotlib.pyplot as pyplot\n",
        "    \n",
        "    # plot training history\n",
        "    print(\"loss\")\n",
        "    pyplot.plot(training_phase.history['loss'], label='train')\n",
        "    pyplot.plot(training_phase.history['val_loss'], label='validation')\n",
        "    pyplot.legend()\n",
        "    pyplot.show()\n",
        "    \n",
        "    print(\"Accuracy\")\n",
        "    pyplot.plot(training_phase.history['accuracy'], label='train')\n",
        "    pyplot.plot(training_phase.history['val_accuracy'], label='validation')\n",
        "    pyplot.legend()\n",
        "    pyplot.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loss\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Accuracy\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rpfUUI-frwI-"
      },
      "source": [
        "**Public score**: 0.76564 \\\\\n",
        "**Private score**: 0.79656 \\\\\n",
        "Sfruttando solamente Keras è difficile capire come poter migliorare il modello, oltre che molto lento in quanto ogni volta è necessario modificare manualmente la composizione dei layer della rete neurale. \\\\\n",
        "Per evitare questo problema e sfruttare a pieno le potenzialità delle reti neurali è possibile utilizzare la grid search per costruire dinamicamente più reti neurali differenti."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f67QCwMAkccV",
        "colab_type": "text"
      },
      "source": [
        "### Neural Network con Grid search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "AN0fXSIehy1o"
      },
      "source": [
        "Tramite la grid search è possibile costruire più reti neurali di dimensioni differenti in modo da individuare il modello migliore per il dataset a disposizione, per farlo è stato necessario definire una funzione per la costruzione di un modello che richiede in input il numero di neuroni per ogni livello e il tipo di funzione di attivazione."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cXYgDmsFn-pW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def createModel(hidden_layers=0, input_neurons=8, hidden_neurons=8, activation_fun='relu', _optimizer='SGD'):\n",
        "  # Initialize the constructor\n",
        "  model = Sequential()\n",
        "\n",
        "  # Add an input layer \n",
        "  model.add(Dense(input_neurons, activation=activation_fun, input_shape=(11,)))\n",
        "\n",
        "  # Add hidden layers\n",
        "  for i in range(hidden_layers): \n",
        "    model.add(Dense(hidden_neurons, activation=activation_fun))\n",
        "\n",
        "  # Add an output layer \n",
        "  model.add(Dense(1, activation='sigmoid'))   \n",
        "  \n",
        "  #Compile model\n",
        "  model.compile(loss='binary_crossentropy',\n",
        "              optimizer=_optimizer,\n",
        "              metrics=['accuracy',tf.keras.metrics.AUC()])\n",
        "\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMy4HBZ5iu2M",
        "colab_type": "text"
      },
      "source": [
        "`model = KerasClassifier(build_fn=createModel(args), verbose=1)`\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4Z8KFKa0hy1u"
      },
      "source": [
        "Grazie alla funzione definita è possibile sfruttare la grid search per chiamarla con parametri sempre differenti:\n",
        "* epochs: numero di epoche per il train della rete\n",
        "* batch_size: ogni quanto fare back propagation [maggiore o uguale a 1]\n",
        "* input_neurons: quanti neuroni inserire nello strato di input [maggiore o uguale a 1]\n",
        "* hidden_layers: numero di livelli nascosti (interni) [intero non negativo]\n",
        "* hidden_neurons: quanti neuroni inserire negli strati interni [maggiore o uguale a 1]\n",
        "* _optimizer: algoritmo di ottimizzazione usato ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']\n",
        "* activation_fun: funzione di attivazione dei neuroni ['softmax', 'softplus', 'softsign', 'relu', 'tanh', 'sigmoid', 'hard_sigmoid', 'linear']"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6dKGhBGunOsS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.wrappers.scikit_learn import KerasClassifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "V1KLwEehhy1v",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "abf3be08-f2c6-4bc2-ea23-c54836b482be"
      },
      "source": [
        "parameter_grid = {'epochs': [15],\n",
        "                  'batch_size': [1],\n",
        "                  'input_neurons': [24],\n",
        "                  'hidden_neurons': [8],\n",
        "                  'hidden_layers': [1],\n",
        "                  '_optimizer': ['Nadam'],\n",
        "                  'activation_fun': ['tanh']}\n",
        "\n",
        "\n",
        "\n",
        "#Create the scoring dictionary\n",
        "SCORING = {'f1': 'f1_macro',\n",
        "          'accuracy': 'accuracy',\n",
        "          'balanced_accuracy': 'balanced_accuracy',\n",
        "          'precision': 'precision_macro',\n",
        "          'recall': 'recall_macro'}\n",
        "\n",
        "cross_validation = StratifiedKFold(n_splits=3)\n",
        "cross_validation.get_n_splits(xTrain, yTrain)\n",
        "\n",
        "model = KerasClassifier(build_fn=createModel, verbose=1)\n",
        "\n",
        "#grid_search = GridSearchCV(model, param_grid=parameter_grid, cv=cross_validation, )\n",
        "grid_search = GridSearchCV(model, param_grid=parameter_grid, cv=cross_validation, scoring=SCORING, return_train_score=True, refit='accuracy')\n",
        "\n",
        "grid_search.fit(xTrain, yTrain)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7f9975f63b70> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: smac attempted to use a functionality that requires module emcee, but it couldn't be loaded. Please install emcee and retry.\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7f9975f63b70> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: smac attempted to use a functionality that requires module emcee, but it couldn't be loaded. Please install emcee and retry.\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "1860/1860 [==============================] - 2s 1ms/step - loss: 0.6404 - accuracy: 0.6500 - auc_2: 0.5603\n",
            "Epoch 2/15\n",
            "1860/1860 [==============================] - 2s 1ms/step - loss: 0.6239 - accuracy: 0.6694 - auc_2: 0.6155\n",
            "Epoch 3/15\n",
            "1860/1860 [==============================] - 2s 1ms/step - loss: 0.6195 - accuracy: 0.6699 - auc_2: 0.6244\n",
            "Epoch 4/15\n",
            "1860/1860 [==============================] - 2s 1ms/step - loss: 0.6091 - accuracy: 0.6672 - auc_2: 0.6527\n",
            "Epoch 5/15\n",
            "1860/1860 [==============================] - 2s 1ms/step - loss: 0.6001 - accuracy: 0.6742 - auc_2: 0.6762\n",
            "Epoch 6/15\n",
            "1860/1860 [==============================] - 2s 1ms/step - loss: 0.6021 - accuracy: 0.6591 - auc_2: 0.6660\n",
            "Epoch 7/15\n",
            "1860/1860 [==============================] - 2s 1ms/step - loss: 0.5998 - accuracy: 0.6613 - auc_2: 0.6701\n",
            "Epoch 8/15\n",
            "1860/1860 [==============================] - 2s 1ms/step - loss: 0.5959 - accuracy: 0.6683 - auc_2: 0.6780\n",
            "Epoch 9/15\n",
            "1860/1860 [==============================] - 2s 1ms/step - loss: 0.6073 - accuracy: 0.6753 - auc_2: 0.6577\n",
            "Epoch 10/15\n",
            "1860/1860 [==============================] - 2s 1ms/step - loss: 0.6080 - accuracy: 0.6790 - auc_2: 0.6532\n",
            "Epoch 11/15\n",
            "1860/1860 [==============================] - 2s 1ms/step - loss: 0.5995 - accuracy: 0.6774 - auc_2: 0.6706\n",
            "Epoch 12/15\n",
            "1860/1860 [==============================] - 2s 1ms/step - loss: 0.6013 - accuracy: 0.6747 - auc_2: 0.6636\n",
            "Epoch 13/15\n",
            "1860/1860 [==============================] - 3s 1ms/step - loss: 0.5997 - accuracy: 0.6806 - auc_2: 0.6694\n",
            "Epoch 14/15\n",
            "1860/1860 [==============================] - 2s 1ms/step - loss: 0.6113 - accuracy: 0.6726 - auc_2: 0.6463\n",
            "Epoch 15/15\n",
            "1860/1860 [==============================] - 2s 1ms/step - loss: 0.5997 - accuracy: 0.6790 - auc_2: 0.6708\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/wrappers/scikit_learn.py:241: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
            "Instructions for updating:\n",
            "Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7f9975d30730> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: smac attempted to use a functionality that requires module emcee, but it couldn't be loaded. Please install emcee and retry.\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7f9975d30730> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: smac attempted to use a functionality that requires module emcee, but it couldn't be loaded. Please install emcee and retry.\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "931/931 [==============================] - 1s 683us/step\n",
            "1860/1860 [==============================] - 1s 651us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7f9975d01598> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: smac attempted to use a functionality that requires module emcee, but it couldn't be loaded. Please install emcee and retry.\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7f9975d01598> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: smac attempted to use a functionality that requires module emcee, but it couldn't be loaded. Please install emcee and retry.\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "1861/1861 [==============================] - 3s 1ms/step - loss: 0.6452 - accuracy: 0.6491 - auc_3: 0.5582\n",
            "Epoch 2/15\n",
            "1861/1861 [==============================] - 2s 1ms/step - loss: 0.6214 - accuracy: 0.6588 - auc_3: 0.6214\n",
            "Epoch 3/15\n",
            "1861/1861 [==============================] - 2s 1ms/step - loss: 0.6116 - accuracy: 0.6550 - auc_3: 0.6409\n",
            "Epoch 4/15\n",
            "1861/1861 [==============================] - 2s 1ms/step - loss: 0.5964 - accuracy: 0.6625 - auc_3: 0.6793\n",
            "Epoch 5/15\n",
            "1861/1861 [==============================] - 2s 1ms/step - loss: 0.5868 - accuracy: 0.6658 - auc_3: 0.6939\n",
            "Epoch 6/15\n",
            "1861/1861 [==============================] - 2s 1ms/step - loss: 0.5817 - accuracy: 0.6717 - auc_3: 0.7050\n",
            "Epoch 7/15\n",
            "1861/1861 [==============================] - 2s 1ms/step - loss: 0.5782 - accuracy: 0.6867 - auc_3: 0.7098\n",
            "Epoch 8/15\n",
            "1861/1861 [==============================] - 2s 1ms/step - loss: 0.5831 - accuracy: 0.6706 - auc_3: 0.7011\n",
            "Epoch 9/15\n",
            "1861/1861 [==============================] - 2s 1ms/step - loss: 0.5816 - accuracy: 0.6819 - auc_3: 0.7061\n",
            "Epoch 10/15\n",
            "1861/1861 [==============================] - 2s 1ms/step - loss: 0.5770 - accuracy: 0.6819 - auc_3: 0.7124\n",
            "Epoch 11/15\n",
            "1861/1861 [==============================] - 2s 1ms/step - loss: 0.5708 - accuracy: 0.6873 - auc_3: 0.7210\n",
            "Epoch 12/15\n",
            "1861/1861 [==============================] - 2s 1ms/step - loss: 0.5627 - accuracy: 0.6846 - auc_3: 0.7313\n",
            "Epoch 13/15\n",
            "1861/1861 [==============================] - 2s 1ms/step - loss: 0.5648 - accuracy: 0.6900 - auc_3: 0.7280\n",
            "Epoch 14/15\n",
            "1861/1861 [==============================] - 3s 1ms/step - loss: 0.5710 - accuracy: 0.6835 - auc_3: 0.7159\n",
            "Epoch 15/15\n",
            "1861/1861 [==============================] - 3s 2ms/step - loss: 0.5770 - accuracy: 0.6830 - auc_3: 0.7122\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7f9975b18b70> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: smac attempted to use a functionality that requires module emcee, but it couldn't be loaded. Please install emcee and retry.\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7f9975b18b70> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: smac attempted to use a functionality that requires module emcee, but it couldn't be loaded. Please install emcee and retry.\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "930/930 [==============================] - 1s 822us/step\n",
            "1861/1861 [==============================] - 2s 914us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7f9975a43598> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: smac attempted to use a functionality that requires module emcee, but it couldn't be loaded. Please install emcee and retry.\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7f9975a43598> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: smac attempted to use a functionality that requires module emcee, but it couldn't be loaded. Please install emcee and retry.\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "1861/1861 [==============================] - 3s 2ms/step - loss: 0.6057 - accuracy: 0.6647 - auc_4: 0.6575\n",
            "Epoch 2/15\n",
            "1861/1861 [==============================] - 3s 2ms/step - loss: 0.5906 - accuracy: 0.6615 - auc_4: 0.6901\n",
            "Epoch 3/15\n",
            "1861/1861 [==============================] - 2s 1ms/step - loss: 0.5882 - accuracy: 0.6728 - auc_4: 0.6959\n",
            "Epoch 4/15\n",
            "1861/1861 [==============================] - 2s 1ms/step - loss: 0.5893 - accuracy: 0.6582 - auc_4: 0.6906\n",
            "Epoch 5/15\n",
            "1861/1861 [==============================] - 2s 1ms/step - loss: 0.5811 - accuracy: 0.6717 - auc_4: 0.7027\n",
            "Epoch 6/15\n",
            "1861/1861 [==============================] - 2s 1ms/step - loss: 0.5840 - accuracy: 0.6507 - auc_4: 0.6976\n",
            "Epoch 7/15\n",
            "1861/1861 [==============================] - 2s 1ms/step - loss: 0.5764 - accuracy: 0.6765 - auc_4: 0.7125\n",
            "Epoch 8/15\n",
            "1861/1861 [==============================] - 2s 1ms/step - loss: 0.5764 - accuracy: 0.6695 - auc_4: 0.7097\n",
            "Epoch 9/15\n",
            "1861/1861 [==============================] - 3s 1ms/step - loss: 0.5704 - accuracy: 0.6733 - auc_4: 0.7153\n",
            "Epoch 10/15\n",
            "1861/1861 [==============================] - 2s 1ms/step - loss: 0.5718 - accuracy: 0.6905 - auc_4: 0.7178\n",
            "Epoch 11/15\n",
            "1861/1861 [==============================] - 2s 1ms/step - loss: 0.5680 - accuracy: 0.6733 - auc_4: 0.7210\n",
            "Epoch 12/15\n",
            "1861/1861 [==============================] - 2s 1ms/step - loss: 0.5751 - accuracy: 0.6733 - auc_4: 0.7108\n",
            "Epoch 13/15\n",
            "1861/1861 [==============================] - 2s 1ms/step - loss: 0.5623 - accuracy: 0.6797 - auc_4: 0.7271\n",
            "Epoch 14/15\n",
            "1861/1861 [==============================] - 2s 1ms/step - loss: 0.5711 - accuracy: 0.6744 - auc_4: 0.7171\n",
            "Epoch 15/15\n",
            "1861/1861 [==============================] - 2s 1ms/step - loss: 0.5694 - accuracy: 0.6685 - auc_4: 0.7165\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7f99758476a8> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: smac attempted to use a functionality that requires module emcee, but it couldn't be loaded. Please install emcee and retry.\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7f99758476a8> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: smac attempted to use a functionality that requires module emcee, but it couldn't be loaded. Please install emcee and retry.\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "930/930 [==============================] - 1s 647us/step\n",
            "1861/1861 [==============================] - 1s 644us/step\n",
            "Epoch 1/15\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7f9975810488> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: smac attempted to use a functionality that requires module emcee, but it couldn't be loaded. Please install emcee and retry.\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7f9975810488> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: smac attempted to use a functionality that requires module emcee, but it couldn't be loaded. Please install emcee and retry.\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "2791/2791 [==============================] - 3s 1ms/step - loss: 0.6333 - accuracy: 0.6578 - auc_5: 0.5804\n",
            "Epoch 2/15\n",
            "2791/2791 [==============================] - 4s 1ms/step - loss: 0.6132 - accuracy: 0.6525 - auc_5: 0.6433\n",
            "Epoch 3/15\n",
            "2791/2791 [==============================] - 4s 1ms/step - loss: 0.6126 - accuracy: 0.6718 - auc_5: 0.6413\n",
            "Epoch 4/15\n",
            "2791/2791 [==============================] - 4s 1ms/step - loss: 0.5942 - accuracy: 0.6682 - auc_5: 0.6847\n",
            "Epoch 5/15\n",
            "2791/2791 [==============================] - 3s 1ms/step - loss: 0.5833 - accuracy: 0.6804 - auc_5: 0.7007\n",
            "Epoch 6/15\n",
            "2791/2791 [==============================] - 4s 1ms/step - loss: 0.5785 - accuracy: 0.6783 - auc_5: 0.7099\n",
            "Epoch 7/15\n",
            "2791/2791 [==============================] - 4s 1ms/step - loss: 0.5768 - accuracy: 0.6904 - auc_5: 0.7124\n",
            "Epoch 8/15\n",
            "2791/2791 [==============================] - 4s 1ms/step - loss: 0.5772 - accuracy: 0.6711 - auc_5: 0.7081\n",
            "Epoch 9/15\n",
            "2791/2791 [==============================] - 4s 1ms/step - loss: 0.5688 - accuracy: 0.6818 - auc_5: 0.7203\n",
            "Epoch 10/15\n",
            "2791/2791 [==============================] - 3s 1ms/step - loss: 0.5722 - accuracy: 0.6858 - auc_5: 0.7174\n",
            "Epoch 11/15\n",
            "2791/2791 [==============================] - 3s 1ms/step - loss: 0.5714 - accuracy: 0.6822 - auc_5: 0.7202\n",
            "Epoch 12/15\n",
            "2791/2791 [==============================] - 4s 1ms/step - loss: 0.5732 - accuracy: 0.6711 - auc_5: 0.7114\n",
            "Epoch 13/15\n",
            "2791/2791 [==============================] - 4s 1ms/step - loss: 0.5633 - accuracy: 0.6915 - auc_5: 0.7320\n",
            "Epoch 14/15\n",
            "2791/2791 [==============================] - 4s 1ms/step - loss: 0.5697 - accuracy: 0.6876 - auc_5: 0.7237\n",
            "Epoch 15/15\n",
            "2791/2791 [==============================] - 3s 1ms/step - loss: 0.5662 - accuracy: 0.6922 - auc_5: 0.7298\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=StratifiedKFold(n_splits=3, random_state=None, shuffle=False),\n",
              "             error_score=nan,\n",
              "             estimator=<tensorflow.python.keras.wrappers.scikit_learn.KerasClassifier object at 0x7f9975fc8668>,\n",
              "             iid='deprecated', n_jobs=None,\n",
              "             param_grid={'_optimizer': ['Nadam'], 'activation_fun': ['tanh'],\n",
              "                         'batch_size': [1], 'epochs': [15],\n",
              "                         'hidden_layers': [1], 'hidden_neurons': [8],\n",
              "                         'input_neurons': [24]},\n",
              "             pre_dispatch='2*n_jobs', refit='accuracy', return_train_score=True,\n",
              "             scoring={'accuracy': 'accuracy',\n",
              "                      'balanced_accuracy': 'balanced_accuracy',\n",
              "                      'f1': 'f1_macro', 'precision': 'precision_macro',\n",
              "                      'recall': 'recall_macro'},\n",
              "             verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pceBd1Exhy1y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        },
        "outputId": "07185d54-22bc-42cc-cb88-4901b8c56af1"
      },
      "source": [
        "# Score locale\n",
        "print('Best score: {}'.format(grid_search.best_score_))\n",
        "print('Best parameters: {}'.format(grid_search.best_params_))\n",
        "\n",
        "best_dtc = grid_search.best_estimator_\n",
        "my_model=best_dtc\n",
        "my_model.fit(xTrain, yTrain)\n",
        "my_model.score(xTest, yTest)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7f99764e3268> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: smac attempted to use a functionality that requires module emcee, but it couldn't be loaded. Please install emcee and retry.\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Best score: 0.6807629673261495\n",
            "Best parameters: {'_optimizer': 'Nadam', 'activation_fun': 'tanh', 'batch_size': 1, 'epochs': 15, 'hidden_layers': 1, 'hidden_neurons': 8, 'input_neurons': 24}\n",
            "Epoch 1/15\n",
            "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7f99764e3268> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: smac attempted to use a functionality that requires module emcee, but it couldn't be loaded. Please install emcee and retry.\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "2791/2791 [==============================] - 4s 1ms/step - loss: 0.6269 - accuracy: 0.6614 - auc_6: 0.6062\n",
            "Epoch 2/15\n",
            "2791/2791 [==============================] - 4s 1ms/step - loss: 0.6079 - accuracy: 0.6675 - auc_6: 0.6543\n",
            "Epoch 3/15\n",
            "2791/2791 [==============================] - 3s 1ms/step - loss: 0.6016 - accuracy: 0.6657 - auc_6: 0.6624\n",
            "Epoch 4/15\n",
            "2791/2791 [==============================] - 3s 1ms/step - loss: 0.5884 - accuracy: 0.6707 - auc_6: 0.6907\n",
            "Epoch 5/15\n",
            "2791/2791 [==============================] - 4s 1ms/step - loss: 0.5865 - accuracy: 0.6725 - auc_6: 0.6919\n",
            "Epoch 6/15\n",
            "2791/2791 [==============================] - 4s 1ms/step - loss: 0.5840 - accuracy: 0.6793 - auc_6: 0.6988\n",
            "Epoch 7/15\n",
            "2791/2791 [==============================] - 4s 1ms/step - loss: 0.5907 - accuracy: 0.6693 - auc_6: 0.6901\n",
            "Epoch 8/15\n",
            "2791/2791 [==============================] - 4s 1ms/step - loss: 0.5762 - accuracy: 0.6761 - auc_6: 0.7108\n",
            "Epoch 9/15\n",
            "2791/2791 [==============================] - 4s 1ms/step - loss: 0.5749 - accuracy: 0.6757 - auc_6: 0.7084\n",
            "Epoch 10/15\n",
            "2791/2791 [==============================] - 4s 1ms/step - loss: 0.5705 - accuracy: 0.6718 - auc_6: 0.7139\n",
            "Epoch 11/15\n",
            "2791/2791 [==============================] - 4s 1ms/step - loss: 0.5635 - accuracy: 0.6714 - auc_6: 0.7233\n",
            "Epoch 12/15\n",
            "2791/2791 [==============================] - 4s 2ms/step - loss: 0.5680 - accuracy: 0.6718 - auc_6: 0.7207\n",
            "Epoch 13/15\n",
            "2791/2791 [==============================] - 5s 2ms/step - loss: 0.5685 - accuracy: 0.6732 - auc_6: 0.7202\n",
            "Epoch 14/15\n",
            "2791/2791 [==============================] - 4s 1ms/step - loss: 0.5579 - accuracy: 0.6912 - auc_6: 0.7355\n",
            "Epoch 15/15\n",
            "2791/2791 [==============================] - 4s 1ms/step - loss: 0.5589 - accuracy: 0.6793 - auc_6: 0.7328\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7f99759489d8> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: smac attempted to use a functionality that requires module emcee, but it couldn't be loaded. Please install emcee and retry.\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7f99759489d8> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: smac attempted to use a functionality that requires module emcee, but it couldn't be loaded. Please install emcee and retry.\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "698/698 [==============================] - 1s 928us/step - loss: 0.6119 - accuracy: 0.6089 - auc_6: 0.6978\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6088825464248657"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "y-MWClMZhy11"
      },
      "source": [
        "**Public score**: 0.77177 \\\\\n",
        "**Private score**: 0.78796 \\\\\n",
        "Nonostante un leggero miglioramento delle prestazioni sfruttando la grid search, il modello riporta ancora dei punteggi non ottimali e di conseguenza non è stato particolarmente approfondito. \\\\\n",
        "I parametri migliori risulteranno utili per l'implementazione all'interno degli *ensemble methods*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "w52dGG9JeAtV"
      },
      "source": [
        "## Support Vector Classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "E0hh1lcTeAtW"
      },
      "source": [
        "Nelle Support Vector Machines l'idea è quella di trovare un iperpiano che ci permetta di separare le istanze in due classi differenti. È un metodo di classificazione binaria, attraverso una separazione netta tra le istanze. La dimensione dell'iperpiano è uguale al numero di attributi legati alle istanze meno 1. Questo metodo funziona bene nei casi in cui i dati sono linearmente separabili, ma può essere esteso anche ai casi non linearmente separabili attraverlo le Kernel Functions.\n",
        "L'iperpiano che viene creato deve far sì che la distanza tra il piano e le due classi sia la maggiore possibile, quindi l'obiettivo è massimizzare il margine (max distanza tra il piano e il punto più vicino da entrambi i lati).\n",
        "I *Support Vectors* sono i punti che giaciono il più vicino all'iperpiano, pertanto sono quelli più difficili da classificare e quelli che determinano principalmente l'inclinazione dell'iperpiano stesso.\n",
        "È inoltre possibile introdurre una tolleranza, cioè un numero massimo di istanze che possono essere classificate in modo scorretto: questo viene fatto con un parametro chiamato C. Minore è questo parametro, maggiori sono gli errori concessi.\n",
        "\n",
        "Nel caso in cui le istanze non sono linearmente separabili si possono sfruttare le kernel functions, che sono delle misure di similarità che rappresentano un prodotto scalare in uno spazio di Hilbert. Ci sono vari tipi di funzioni Kernel che si possono utilizzare: lineare, polinomiale, esponenziale, gaussiana, ecc..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RcFyUM6BeAtX",
        "colab": {}
      },
      "source": [
        "# Codice standard\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "svc = SVC()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "iqWq5AKYeAta"
      },
      "source": [
        "### Il nostro utilizzo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "r2MRPi-JeAta"
      },
      "source": [
        "Abbiamo sfruttato le Support Vector Machines insieme al metodo della Grid Search, modificando i parametri più significativi, che influenzavano maggiormente la soluzione, cercando di ottenere il miglior risultato possibile. I parametri più importanti su cui abbiamo agito sono:\n",
        "\n",
        "*   **C:** parametro di regolarizzazione. La forza della regolarizzazione è inversamente proporzionale a C. Deve essere strettamente positiva. Di default vale 1, ma abbiamo riscontrato risultati migliori con valori più alti.\n",
        "*   **kernel:** specifica il tipo di kernel da utilizzare nell'algoritmo. Deve essere uno tra \"linear\", \"poly\", \"rbf\", \"sigmoid\", \"precomputed\". Se non viene fornito nessuno, verrà utilizzato \"rbf\". Abbiamo provato vari tipi di kernel, ma quello che ci ha permesso di ottenere i risultati migliori è stato \"rbf\", mentre \"poly\" ci ha fatto conteguire punteggi scarsi.\n",
        "*   **degree:** grado della funzione kernel polinomiale ('poly'). Ignorato da tutti gli altri kernel. Pertanto l'abbiamo utilizzato solo nei casi in cui abbiamo sfruttato la kernel polinomiale. In tal caso i risultati migliori, seppur sempre scarsi, li abbiamo ottenuto con un degree pari a 4/5.\n",
        "*   **gamma:** indica il coefficiente Kernel per \"rbf\", \"poly\" e \"sigmoid\" Kernel coefficient for ‘rbf’, ‘poly’ and ‘sigmoid’:\n",
        "\n",
        " -se gamma='scale'viene passato (default) allora usa 1 / (n_features * X.var ()) come valore di gamma\n",
        "\n",
        " -se \"auto\", utilizza 1 / n_features.\n",
        "*   **max_iter :** Limite rigido sulle iterazioni all'interno del risolutore. Di default vale -1, per indicare nessun limite. Imponendo un limite abbiamo ottenuto un risolutore più veloce, con risultati però inferiori"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "IQ8cT-Y9eAtb"
      },
      "source": [
        "### Best Result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lu28iAFXeAtc"
      },
      "source": [
        "Abbiamo ottenuto il nostro miglior risultato con il Support Vector Classifier \n",
        "sfruttando la kernel di tipo \"rbf\" (Radial Basis Function)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zrZqvlksZ2W_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "42d58891-9b35-4d72-cc5a-9d338b6bce83"
      },
      "source": [
        "svc = SVC(C = 1.2, gamma =  0.9, kernel= 'rbf')\n",
        "\n",
        "svc.fit(xTrain, yTrain)\n",
        "\n",
        "print('Score: {}'.format(svc.score(xTest, yTest)))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Score: 0.7263610315186246\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDqy8qEBr5iK",
        "colab_type": "text"
      },
      "source": [
        "Public Score: **0.74785**\n",
        "\n",
        "Private Score: **0.74969**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Q0vFrrCVeAtf"
      },
      "source": [
        "Il Support Vector Classifier si è rivelato un metodo non adatto alla classificazione di questi dati, in quanto quest'ultimi non erano linearmente separabili. Lo sfruttamento di una kernel function ha migliorato leggermente la situazione, senza, però, riuscire mai ad ottere risultati degni di nota."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xDM92Z-ed6Da"
      },
      "source": [
        "\n",
        "## *Ensemble Method*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8BuAfaGv4GtC",
        "colab_type": "text"
      },
      "source": [
        "### Voting Classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zEqgK-YwY_kA",
        "colab_type": "text"
      },
      "source": [
        "#### Il nostro utilizzo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "EqrnGpTPd6De"
      },
      "source": [
        "Abbiamo usato il Voting Classifier tentando di sfruttare insieme i metodi che avevano mostrato i risultati migliori, modificando i parametri dei singoli classificatori al fine di ottenere il punteggio migliore complessivo.\n",
        "\n",
        "Una delle scelte principali riguardava il fatto se sfruttare il voting *hard* o quello *soft*. Provando il metodo con gli stessi identici parametri, cambiando solo la modalità di voting, abbiamo notato che in molti casi il voting hard risultava migliore dal punto di vista dello score pubblico, pertanto nelle prove successive abbiamo deciso di sfruttare principalemente quello. Rianalizzando i dati alla luce dello score privato abbiamo, invece, notato che il punteggio del voting soft è risultato spesso migliore di quello hard.\n",
        "\n",
        "I metodi che abbiamo provato maggiormente sono *Random Forest, Bagging Classifier, Decision Tree, AdaBoost, Support Vector Classifier, la Logistic Regression e il Linear Classifier*, ottenendo il risultato migliore (il nostro best Private Score) con Logistic, Random Forest, SVC e Decision Tree.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DbWTx9Ssd6Df"
      },
      "source": [
        "I metodi d'insieme sono algoritmi di classificazione che predicono il valore della classe dei record considerando le predizioni di vari classificatori differenti. Il vantaggio principale ottenuto da questo approccio è che spesso migliora le performance predittive, mentre lo svantaggio più evidente risulta invece che il risultato ottenuto è difficile da analizzare. \n",
        "Gli \"Ensemble Methods\" funzionano bene quando i metodi utilizzati al suo interno sono indipendenti l'uno dall'altro, ottenendo così una riduzione consistente dell'errore di predizione, poichè più sono i metodi indipendenti utilizzati, minore è la possibilità che la maggior parte di essi dia un risultato errato, anche se la precisione dei singoli metodi non è altissima.\n",
        "\n",
        "Ci sono due modalità di voto possibile: *hard* e *soft*.\n",
        "Il primo consiste nel fatto di assegnare la stessa importanza a tutti i classificatori considerati, avendo, pertanto, una vera e propria classificazione basata su un voto di maggioranza. Il secondo, invece, consiste nell'assegnare un'importanza maggiore ad alcuni classificatori, il voto dei quali ha un valore differente in base al peso assegnato al metodo stesso."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0khdeTzh4UdQ",
        "colab_type": "text"
      },
      "source": [
        "Nel Voting Classifier di possono inserire classificatori differenti a proprio piacere, adattandoli con i parametri che si ritengono migliori. È necessario importare i vari classificatori che si vogliono utilizzare, definirli e infine inserirli all'interno del voting classifier, decidendo poi la modalità di voting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UIiEKnF_d6Dg",
        "colab": {}
      },
      "source": [
        "# Codice standard\n",
        "\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "\n",
        "ada_clf = AdaBoostClassifier(n_estimators=300,random_state=42,learning_rate=0.1) \n",
        "dec_clf = DecisionTreeClassifier(random_state=42, max_depth=8, min_samples_leaf=10,min_samples_split=20) \n",
        "rnd_clf = RandomForestClassifier(n_estimators=300, random_state=42, min_samples_leaf=10) \n",
        "bag_clf = BaggingClassifier( DecisionTreeClassifier(random_state=42), n_estimators=300, bootstrap=True, oob_score=True, random_state=42) \n",
        "\n",
        "\n",
        "voting_clf = VotingClassifier(\n",
        "    estimators=[('ada', ada_clf),('bc', bag_clf), ('rf', rnd_clf),('dec', dec_clf)],\n",
        "    voting='hard')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2oZTfwWa_3MV",
        "colab_type": "text"
      },
      "source": [
        "##### Voting Soft"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IXC5XLGGqNdk",
        "colab": {}
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "\n",
        "try:\n",
        "    import xgboost\n",
        "except ImportError as ex:\n",
        "    print(\"Error: the xgboost library is not installed.\")\n",
        "    xgboost = None\n",
        "if xgboost is not None:  # not shown in the book\n",
        "    xgb_clf = xgboost.XGBClassifier(random_state=42, max_depth=7) \n",
        "    \n",
        "ada_clf = AdaBoostClassifier(n_estimators=300,random_state=42,learning_rate=0.1) \n",
        "dec_clf = DecisionTreeClassifier(random_state=42, max_depth=8, min_samples_leaf=10,min_samples_split=20) \n",
        "rnd_clf = RandomForestClassifier(n_estimators=300, random_state=42, min_samples_leaf=10) \n",
        "bag_clf = BaggingClassifier( DecisionTreeClassifier(random_state=42), n_estimators=300, bootstrap=True, oob_score=True, random_state=42) \n",
        "\n",
        "voting_clf = VotingClassifier( estimators=[('bc', bag_clf), ('dec',dec_clf), ('xgb', xgb_clf), ('ada', ada_clf),('rf', rnd_clf)], \n",
        "                              voting='soft')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LvzqM93wqNdq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        },
        "outputId": "21d3dad3-dfe1-45c4-fcb2-44203a3ccd83"
      },
      "source": [
        "voting_clf.fit(xTrain,yTrain)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VotingClassifier(estimators=[('bc',\n",
              "                              BaggingClassifier(base_estimator=DecisionTreeClassifier(ccp_alpha=0.0,\n",
              "                                                                                      class_weight=None,\n",
              "                                                                                      criterion='gini',\n",
              "                                                                                      max_depth=None,\n",
              "                                                                                      max_features=None,\n",
              "                                                                                      max_leaf_nodes=None,\n",
              "                                                                                      min_impurity_decrease=0.0,\n",
              "                                                                                      min_impurity_split=None,\n",
              "                                                                                      min_samples_leaf=1,\n",
              "                                                                                      min_samples_split=2,\n",
              "                                                                                      min_weight_fraction_leaf=0.0,\n",
              "                                                                                      presort='deprecated',\n",
              "                                                                                      random_state=42,\n",
              "                                                                                      sp...\n",
              "                                                     criterion='gini',\n",
              "                                                     max_depth=None,\n",
              "                                                     max_features='auto',\n",
              "                                                     max_leaf_nodes=None,\n",
              "                                                     max_samples=None,\n",
              "                                                     min_impurity_decrease=0.0,\n",
              "                                                     min_impurity_split=None,\n",
              "                                                     min_samples_leaf=10,\n",
              "                                                     min_samples_split=2,\n",
              "                                                     min_weight_fraction_leaf=0.0,\n",
              "                                                     n_estimators=300,\n",
              "                                                     n_jobs=None,\n",
              "                                                     oob_score=False,\n",
              "                                                     random_state=42, verbose=0,\n",
              "                                                     warm_start=False))],\n",
              "                 flatten_transform=True, n_jobs=None, voting='soft',\n",
              "                 weights=None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WvjXg2IWqNdt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "157efed1-0a47-4bf3-8f23-258e4c2c6f01"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "for clf in (ada_clf,bag_clf,dec_clf,rnd_clf,xgb_clf,voting_clf):\n",
        "    clf.fit(xTrain, yTrain) \n",
        "    ypred = clf.predict(xTest)\n",
        "    print(clf.__class__.__name__, accuracy_score(yTest, ypred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "AdaBoostClassifier 0.7535816618911175\n",
            "BaggingClassifier 0.826647564469914\n",
            "DecisionTreeClassifier 0.7320916905444126\n",
            "RandomForestClassifier 0.7793696275071633\n",
            "XGBClassifier 0.8108882521489972\n",
            "VotingClassifier 0.8080229226361032\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0DBJFmA-qNdy"
      },
      "source": [
        "Public Score: **0.79942**\n",
        "\n",
        "Private Score: **0.82085**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOLnxdVZAqu8",
        "colab_type": "text"
      },
      "source": [
        "##### Voting Hard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Fs-aJgOlqWOh",
        "colab": {}
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "\n",
        "try:\n",
        "    import xgboost\n",
        "except ImportError as ex:\n",
        "    print(\"Error: the xgboost library is not installed.\")\n",
        "    xgboost = None\n",
        "if xgboost is not None:  # not shown in the book\n",
        "    xgb_clf = xgboost.XGBClassifier(random_state=42, max_depth=7) \n",
        "    \n",
        "ada_clf = AdaBoostClassifier(n_estimators=300,random_state=42,learning_rate=0.1) \n",
        "dec_clf = DecisionTreeClassifier(random_state=42, max_depth=8, min_samples_leaf=10,min_samples_split=20) \n",
        "rnd_clf = RandomForestClassifier(n_estimators=300, random_state=42, min_samples_leaf=10) \n",
        "bag_clf = BaggingClassifier( DecisionTreeClassifier(random_state=42), n_estimators=300, bootstrap=True, oob_score=True, random_state=42) \n",
        "\n",
        "voting_clf = VotingClassifier( estimators=[('bc', bag_clf), ('dec',dec_clf), ('xgb', xgb_clf), ('ada', ada_clf),('rf', rnd_clf)], \n",
        "                              voting='hard')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WOyHN6_9qWOn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        },
        "outputId": "caced98a-a24d-4932-ebef-4cc1e02232f9"
      },
      "source": [
        "voting_clf.fit(xTrain, yTrain)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VotingClassifier(estimators=[('bc',\n",
              "                              BaggingClassifier(base_estimator=DecisionTreeClassifier(ccp_alpha=0.0,\n",
              "                                                                                      class_weight=None,\n",
              "                                                                                      criterion='gini',\n",
              "                                                                                      max_depth=None,\n",
              "                                                                                      max_features=None,\n",
              "                                                                                      max_leaf_nodes=None,\n",
              "                                                                                      min_impurity_decrease=0.0,\n",
              "                                                                                      min_impurity_split=None,\n",
              "                                                                                      min_samples_leaf=1,\n",
              "                                                                                      min_samples_split=2,\n",
              "                                                                                      min_weight_fraction_leaf=0.0,\n",
              "                                                                                      presort='deprecated',\n",
              "                                                                                      random_state=42,\n",
              "                                                                                      sp...\n",
              "                                                     criterion='gini',\n",
              "                                                     max_depth=None,\n",
              "                                                     max_features='auto',\n",
              "                                                     max_leaf_nodes=None,\n",
              "                                                     max_samples=None,\n",
              "                                                     min_impurity_decrease=0.0,\n",
              "                                                     min_impurity_split=None,\n",
              "                                                     min_samples_leaf=10,\n",
              "                                                     min_samples_split=2,\n",
              "                                                     min_weight_fraction_leaf=0.0,\n",
              "                                                     n_estimators=300,\n",
              "                                                     n_jobs=None,\n",
              "                                                     oob_score=False,\n",
              "                                                     random_state=42, verbose=0,\n",
              "                                                     warm_start=False))],\n",
              "                 flatten_transform=True, n_jobs=None, voting='hard',\n",
              "                 weights=None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7YhZv4L5qWOr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "a69cb63d-73b1-4391-8835-f826024100d3"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "for clf in (ada_clf,bag_clf,dec_clf,rnd_clf,xgb_clf,voting_clf):\n",
        "    clf.fit(xTrain, yTrain) \n",
        "    ypred = clf.predict(xTest)\n",
        "    print(clf.__class__.__name__, accuracy_score(yTest, ypred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "AdaBoostClassifier 0.7535816618911175\n",
            "BaggingClassifier 0.826647564469914\n",
            "DecisionTreeClassifier 0.7320916905444126\n",
            "RandomForestClassifier 0.7793696275071633\n",
            "XGBClassifier 0.8108882521489972\n",
            "VotingClassifier 0.8051575931232091\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mQimmIjQqWOu"
      },
      "source": [
        "Public Score: **0.80736**\n",
        "\n",
        "Private Score: **0.80802**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4DodPIpfBPRb",
        "colab_type": "text"
      },
      "source": [
        "#### Best Result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "eJ_HVfU9qChD",
        "colab": {}
      },
      "source": [
        "#User Voting Classifier\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "log_clf = LogisticRegression(solver=\"lbfgs\", random_state=42)\n",
        "rnd_clf = RandomForestClassifier(n_estimators=100, random_state=42) \n",
        "svm_clf = SVC(gamma=\"scale\", random_state=42) \n",
        "dec_clf = DecisionTreeClassifier(splitter='best',random_state=42)\n",
        "\n",
        "voting_clf = VotingClassifier(\n",
        "    estimators=[('log', log_clf),('svm', svm_clf), ('rf', rnd_clf),('dec', dec_clf)],\n",
        "    voting='hard')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KgjZbv9bjcps",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        },
        "outputId": "48fa5c7a-d5b6-40a9-a250-569eb3319ab4"
      },
      "source": [
        "voting_clf.fit(xTrain, yTrain)\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "for clf in (log_clf,rnd_clf,dec_clf,svm_clf,voting_clf):\n",
        "    clf.fit(xTrain, yTrain) \n",
        "    y_pred = clf.predict(xTest)\n",
        "    print(clf.__class__.__name__, accuracy_score(yTest, y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "LogisticRegression 0.6862464183381088\n",
            "RandomForestClassifier 0.829512893982808\n",
            "DecisionTreeClassifier 0.7736389684813754\n",
            "SVC 0.6532951289398281\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "VotingClassifier 0.8094555873925502\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "breORsmrjTe3",
        "colab_type": "text"
      },
      "source": [
        "Public Score: **0.81375**\n",
        "\n",
        "Private Score: **0.83435** *(OUR BEST PRIVATE SCORE)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2QZPYql_d6D7"
      },
      "source": [
        "Il Voting Classifier si è rivelato un metodo robusto, che ci ha permesso di ottenere ottimi risultati. Facile da utilizzare, modificando i metodi utilizzati e i parametri a loro assegnati. Lavora bene anche con tanti metodi, senza eccessivi rallentamenti nel calcolo del punteggio."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-xnZDD3FuIp",
        "colab_type": "text"
      },
      "source": [
        "### Bagging Classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZGPOX43JnQK",
        "colab_type": "text"
      },
      "source": [
        "Nel Bagging vengono generati training sets scegliendo casualmente elementi dal dataset originale. Ci sono due modalità possibili: *con reinserimento* e *senza reinserimento*. Nel primo caso gli elementi che vengono pescati dal dataset originale vengono reinseriti in tale set, pertanto c'è la possibilità di ripescarli. In questo modo posso creare training sets della stessa dimensione di quello originale, ma che conterrano elementi duplicati.\n",
        "Nel secondo caso, invece, gli elementi, una volta estratti, non vengono reinseriti nel dataset, pertanto non posso creare training sets della stessa dimensione di quello originale, poichè altrimenti sarebbero tutti formati dagli stessi elementi. Quindi vengono creati training sets di dimensione inferiore a quello originale e sfrutto questo metodo se ho un dataset di grandi dimensioni, in modo tale da poter avere training sets di dimensioni adeguate seppur inferiori a quello originale.\n",
        "In seguito la classificazione avviene considerando tutti i training sets creati, effettuando la classificazione per ognuno di essi e, infine, scegliendo la classe che compare più spesso (se tutti i modelli hanno lo stesso peso) o quella la cui somma dei voti presenta un peso maggiore (in caso di modelli con pesi differenti)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HSCouI_dSXIq",
        "colab": {}
      },
      "source": [
        "# Codice standard\n",
        "\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "bag_clf = BaggingClassifier(\n",
        "    DecisionTreeClassifier(random_state=42),\n",
        "    n_estimators=400, max_samples=1.0, bootstrap=True, oob_score=True, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75Qv8UZHZPgB",
        "colab_type": "text"
      },
      "source": [
        "#### Il nostro utilizzo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5e8oSxNTSXI3"
      },
      "source": [
        "Abbiamo sfruttato il Bagging Classifier modificando i parametri assegnati sia a questo metodo che al Decision Tree o alla Random Forest che gli veniva passata. I parametri più importanti su cui abbiamo agito sono:\n",
        "\n",
        "*   **n_estimators:** il numero di stimatori base nell'insieme. Di base sono 100, ma abbiamo notato che aumentandoli fino ad un valore di 300/400 le performance aumentavano, seppur con un aumento del tempo necessario all'esecuzione, il quale rimaneva non eccessivo.\n",
        "*   **max_samples:** il numero di istanze da utilizzare al massimo per allenare ogni stimatore base. Abbiamo provato a modificare questo valore, usando solo una porzione di istanze, ottenendo il miglior risultato sfruttando il 95% di esse.\n",
        "*   **bootstrap:** indica se le istanze sono pescate con o senza rimpiazzo. Effettuando varie prove abbiamo notato risultati migliori sfruttando il reinserimento.\n",
        "*   **oob_score:** indica se utilizzare o meno le istanze che non sono state prese in cosiderazione nel training set al fine di stimare il generalization error. Avendo spesso utilizzato il pescaggio con reinserimento, sfruttare le istanze non viste per effettuare la stima portava dei vantaggi per quanto riguarda il punteggio ottenuto.\n",
        "*   **random_state:** serve a controllare il ricampionamento randomico del dataset originale. In questo modo viene generato un seed per ogni istanza nell'insieme. Passando un valore intero si può ottenere la riproducibilità dell'output ottenuto con specifici parametri.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCiB8lI9X4dc",
        "colab_type": "text"
      },
      "source": [
        "##### Bootstrap True"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kEhsxfOMqdMc",
        "colab": {}
      },
      "source": [
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "bag_clf = BaggingClassifier( \n",
        "    DecisionTreeClassifier(random_state=42), \n",
        "    n_estimators=200, max_samples=2500, bootstrap=True, oob_score=True, bootstrap_features=True, random_state=42) \n",
        "\n",
        "bag_clf.fit(xTrain,yTrain)\n",
        "y_pred = bag_clf.predict(xTest)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "m1mSzboMlEiq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7f99e38f-0692-48f1-bebb-a469e79b5d47"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "print(accuracy_score(yTest, y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.832378223495702\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "k3A6yNXKqdMn"
      },
      "source": [
        "Public Score: **0.81948**\n",
        "\n",
        "Private Score: **0.82699**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Js-jQQpwYUM5"
      },
      "source": [
        "##### Bootstrap False"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7agNjrxNqgXP",
        "colab": {}
      },
      "source": [
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "bag_clf = BaggingClassifier( \n",
        "    DecisionTreeClassifier(random_state=42), \n",
        "    n_estimators=200, max_samples=2500, bootstrap=False, bootstrap_features=True, random_state=42) \n",
        "\n",
        "bag_clf.fit(xTrain,yTrain)\n",
        "y_pred = bag_clf.predict(xTest)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0TGkRv-6lDLX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d71dcdeb-fdf7-478c-ccde-a18f07d66b6c"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "print(accuracy_score(yTest, y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8151862464183381\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SCnZ56epqgXZ"
      },
      "source": [
        "Public Score: **0.81375**\n",
        "\n",
        "Private Score: **0.81595**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hPQt5SjpSoYa"
      },
      "source": [
        "#### Best Result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hC1WS7d3qkBA",
        "colab": {}
      },
      "source": [
        "#User defined Bagging Ensembles with RandomForest  \n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "bag_clf = BaggingClassifier( \n",
        "    RandomForestClassifier(random_state=42), \n",
        "    n_estimators=300, max_samples=0.95, bootstrap=True, oob_score=True, random_state=42\n",
        ") \n",
        "\n",
        "bag_clf.fit(xTrain,yTrain)\n",
        "y_pred = bag_clf.predict(xTest)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I1GxmC30T6C9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3366bab5-4b2e-4c29-b878-73546c2a0bd3"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "print(accuracy_score(yTest, y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8194842406876791\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tzo7xgNRk1MT",
        "colab_type": "text"
      },
      "source": [
        "Public Score: **0.79942**\n",
        "\n",
        "Private Score: **0.83435** *(OUR BEST PRIVATE SCORE)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "W3PZp2eqSoYe"
      },
      "source": [
        "Il Bagging Classifier si è rivelato un ottimo metodo robusto, che ci ha permesso di ottenere ottimi risultati. Infatti la maggior parte dei nostri best score sono legati al Bagging Classifier, tra cui il nostro miglior score privato."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qFEfrihMZ2Wx"
      },
      "source": [
        "### Random Forests"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pmUPtKSmZ2Wy"
      },
      "source": [
        "Le Random Forests sono una combinazione di predittori ad albero. La loro efficacia dipende dalla forza di ogni singolo albero che contengono e dalla correlazione tra questi. \n",
        "Viene preso il dataset originale e suddiviso in i sottoset, ad ogni iterazione uso uno di quei set come test set e il resto come training sets. In questo modo creo i alberi differenti, tutti della grandezza massima possibile senza alcun taglio. La classificazione viene poi effettuata seguendo un voto di maggioranza tra i vari alberi di cui è composta la foresta.\n",
        "Tramite le Random Forests riusciamo a capire quali variabili siano più importanti. Inoltre il metodo garantisce una buona precisione e funziona bene su dataset di grandi dimensioni."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "feiGwbkqqr30",
        "colab": {}
      },
      "source": [
        "# Codice standard\n",
        "train = pd.read_csv('https://raw.githubusercontent.com/AndreaBertoglio/MLDM/master/Pre-processing/Data%20Set%20elaborati/TrainCluster2.csv')\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "rdf = RandomForestClassifier()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OzRVNawXqr36",
        "colab": {}
      },
      "source": [
        "# pull data into target (y) and predictors (X)\n",
        "#La y è la classe, cioè la Quality\n",
        "train_y = train.Quality\n",
        "#seleziono colonne di interesse, non metto l'ID perchè non mi interessa\n",
        "predictor_cols = ['fixed.acidity','volatile.acidity','citric.acid','residual.sugar','chlorides','free.sulfur.dioxide','total.sulfur.dioxide','density','pH','sulphates','alcohol']\n",
        "\n",
        "# La x sono gli attributi\n",
        "train_X = train[predictor_cols]\n",
        "\n",
        "# Sostituisce i missing values con la media e lo applica alle x\n",
        "\n",
        "#imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
        "imp = IterativeImputer(missing_values=np.nan, max_iter=30)\n",
        "imp = imp.fit(train_X)\n",
        "\n",
        "\n",
        "# Impute our data, then train\n",
        "train_X_imp = imp.transform(train_X)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgAodc3qLqU2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Uso l'80% dei dati per train e il restante 20% per test\n",
        "TrainX, TestX, TrainY, TestY = train_test_split(train_X_imp, train_y,  random_state = 0) #train_size = 0.8"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "EYI9PWCCZ2W3"
      },
      "source": [
        "#### Il nostro utilizzo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "v7Kowd6GZ2W3"
      },
      "source": [
        "Abbiamo sfruttato le Random Forests insieme al metodo della Grid Search, modificando i parametri più significativi, che influenzavano maggiormente la soluzione, cercando di ottenere il miglior risultato possibile. I parametri più importanti su cui abbiamo agito sono:\n",
        "\n",
        "*   **n_estimators:** il numero di alberi all'interno della foresta. Di base sono 100, ma abbiamo notato che aumentandoli fino ad un valore di 300/400 le performance aumentavano, seppur con un aumento del tempo necessario all'esecuzione, il quale rimaneva non eccessivo.\n",
        "*   **max_depth:** la profondità massima raggiungibile dagli alberi all'interno della foresta. Se non viene specificato nulla gli alberi crescono il più possibile. Limitando questo valore con un numero intorno al 7 abbiamo notato un miglioramento dei risultati.\n",
        "*   **min_samples_split:** il numero minimo di istanze che devono essere presenti in un nodo per poterlo splittare. Di default è due, ma aumentando questo valore siamo riusciti ad evitare un overfitting eccessivo.\n",
        "*   **bootstrap:** indica se le istanze sono pescate con o senza rimpiazzo. Effettuando varie prove abbiamo notato risultati migliori sfruttando il reinserimento.\n",
        "*   **oob_score:** indica se utilizzare o meno le istanze che non sono state prese in cosiderazione nel training set al fine di stimare il generalization error. Avendo spesso utilizzato il pescaggio con reinserimento, sfruttare le istanze non viste per effettuare la stima portava dei vantaggi per quanto riguarda il punteggio ottenuto.\n",
        "*   **random_state:** serve a controllare il ricampionamento randomico del dataset originale. In questo modo viene generato un seed per ogni istanza nell'insieme. Passando un valore intero si può ottenere la riproducibilità dell'output ottenuto con specifici parametri.\n",
        "*   **max_samples:** il numero di istanze da utilizzare al massimo per allenare ogni stimatore base. Abbiamo provato a modificare questo valore, usando solo una porzione di istanze, ottenendo il miglior risultato sfruttando il 95% di esse.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9iCmI4JiZ2W_"
      },
      "source": [
        "#### Best Result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03YmcTPvhpwj",
        "colab_type": "text"
      },
      "source": [
        "Abbiamo ottenuto il nostro miglior risultato con le Random Forests sfruttando il clustering e con i dati da noi pre-processati"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fuYT5tL1q5U4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "outputId": "7ed0bf9a-d857-489a-dc63-679681999150"
      },
      "source": [
        "rdf = RandomForestClassifier()\n",
        "\n",
        "parameter_grid = {'n_estimators': [100, 200, 300],\n",
        "                  'max_leaf_nodes': [5,7,9],\n",
        "                  'random_state': [42]\n",
        "                  }\n",
        "\n",
        "cross_validation = StratifiedKFold(n_splits=10)\n",
        "cross_validation.get_n_splits(train_X_imp, train_y)\n",
        "#Create the scoring dictionary\n",
        "SCORING = {'accuracy': 'accuracy',\n",
        "'balanced_accuracy': 'balanced_accuracy',\n",
        "'precision': 'precision_macro',\n",
        "'recall': 'recall_macro',\n",
        "'f1': 'f1_macro'}\n",
        "\n",
        "\n",
        "grid_search = GridSearchCV(rdf, param_grid=parameter_grid, cv=cross_validation, scoring=SCORING,return_train_score=True, refit='f1')\n",
        "\n",
        "grid_search.fit(TrainX, TrainY)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=StratifiedKFold(n_splits=10, random_state=None, shuffle=False),\n",
              "             error_score=nan,\n",
              "             estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,\n",
              "                                              class_weight=None,\n",
              "                                              criterion='gini', max_depth=None,\n",
              "                                              max_features='auto',\n",
              "                                              max_leaf_nodes=None,\n",
              "                                              max_samples=None,\n",
              "                                              min_impurity_decrease=0.0,\n",
              "                                              min_impurity_split=None,\n",
              "                                              min_samples_leaf=1,\n",
              "                                              min_samples_split=2...\n",
              "                                              random_state=None, verbose=0,\n",
              "                                              warm_start=False),\n",
              "             iid='deprecated', n_jobs=None,\n",
              "             param_grid={'max_leaf_nodes': [5, 7, 9],\n",
              "                         'n_estimators': [100, 200, 300],\n",
              "                         'random_state': [42]},\n",
              "             pre_dispatch='2*n_jobs', refit='f1', return_train_score=True,\n",
              "             scoring={'accuracy': 'accuracy',\n",
              "                      'balanced_accuracy': 'balanced_accuracy',\n",
              "                      'f1': 'f1_macro', 'precision': 'precision_macro',\n",
              "                      'recall': 'recall_macro'},\n",
              "             verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1iEGKe59uvlB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "577c8818-9d57-4d35-b2e7-ea218eca2ad2"
      },
      "source": [
        "# Score locale\n",
        "print('Best score: {}'.format(grid_search.best_score_))\n",
        "print('Best parameters: {}'.format(grid_search.best_params_))\n",
        "\n",
        "best_dtc = grid_search.best_estimator_\n",
        "my_model=best_dtc\n",
        "my_model.fit(TrainX, TrainY)\n",
        "my_model.score(TestX, TestY)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best score: 0.7127262212911027\n",
            "Best parameters: {'max_leaf_nodes': 9, 'n_estimators': 100, 'random_state': 42}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7422680412371134"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMEvB-9PpdwG",
        "colab_type": "text"
      },
      "source": [
        "Public Score: **0.82234**\n",
        "\n",
        "Private Score: **0.82208** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "z5XIZHDvZ2XD"
      },
      "source": [
        "Le Random Forests si sono rivelate un buon metodo che ci ha permesso di ottenere risultati sopra la media, specialmente se abbinate al preprocessing e al clustering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Di3rNLryEIPB"
      },
      "source": [
        "### Boosting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JHzyovX1EIPI"
      },
      "source": [
        "Il Boosting è un tipo di classificazione che, come le Random Forests, sfrutta degli alberi di decisione, insieme ai concetti di votazione e media. Infatti vengono creati vari modelli, ai quali viene assegnato un peso in base alle loro performance sul test set. \n",
        "Viene creato un modello iniziale e fatta la classificazione con quello. Si valutano poi quante e quali sono le istanze classificate in modo errato, assegnando ad esse un'importanza maggiore. Viene poi creato un nuovo modello, il quale tiene conto dei pesi assegnati alle istanze, e che, pertanto, tenderà a stimare correttamente gli esempi che prima sono stati misclassificati. Questa operazione si esegue ripetute volte. Infine la classificazione finale viene data in base ai voti da parte di tutti i modelli che ho creato, i quali sono pesati in base al peso di ognuno di essi."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "URWJxaQmJbrN",
        "colab_type": "text"
      },
      "source": [
        "#### XGBoost"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKcX-YZ8Jd4m",
        "colab_type": "text"
      },
      "source": [
        "Uno degli algoritmi di Boosting che abbiamo osservato e utilizzato è XGBoost (e**X**treme **G**radient **Boost**ing), il quale è basto sul Gradient Boosting, cioè un tipo di boosting in cui gli errori sono minimizzati dall'algoritmo di discesa del gradiente.\n",
        "\n",
        "Esso si prefigge di minimizzare una funzione obiettivo regolarizzata che combina una funzione di perdita (basata sulla differenza tra i valori osservati e previsti) e un termine di penalità per la complessità del modello (in altre parole, la funzione dell’albero di regressione, il pruning).\n",
        "\n",
        "L’addestramento procede in modo iterativo, aggiungendo nuovi alberi che predicono i residui, i quali vengono poi combinati con gli alberi precedenti per eseguire la previsione finale."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_6_Rs4WNEIPJ",
        "colab": {}
      },
      "source": [
        "# Codice standard\n",
        "\n",
        "try:\n",
        "    import xgboost\n",
        "except ImportError as ex:\n",
        "    print(\"Error: the xgboost library is not installed.\")\n",
        "    xgboost = None\n",
        "\n",
        "xgb = xgboost.XGBClassifier()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-5aE3arcEIPO"
      },
      "source": [
        "##### Il nostro utilizzo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SdUZu_J8EIPP"
      },
      "source": [
        "Come in molti altri casi, abbiamo sfruttato XGBoost insieme al metodo della Grid Search, modificando i parametri più significativi, che influenzavano maggiormente la soluzione, cercando di ottenere il miglior risultato possibile. I parametri più importanti su cui abbiamo agito sono:\n",
        "\n",
        "*   **n_estimators:** il numero di fasi di potenziamento da eseguire. L'aumento del gradiente è abbastanza robusto per l'adattamento eccessivo, quindi un numero elevato di solito si traduce in prestazioni migliori. Di base sono 100, ma abbiamo notato che aumentandoli fino ad un valore di 150/200 le performance aumentavano, seppur con un aumento del tempo necessario all'esecuzione, il quale rimaneva non eccessivo.\n",
        "*   **max_depth:** la profondità massima raggiungibile dagli alberi. Se non viene specificato nulla gli alberi crescono il più possibile. Limitando questo valore con un numero intorno al 7 abbiamo notato un miglioramento dei risultati.\n",
        "*   **min_samples_split:** il numero minimo di istanze che devono essere presenti in un nodo per poterlo splittare. Di default è due, ma aumentando questo valore siamo riusciti ad evitare un overfitting eccessivo.\n",
        "*   **learning rate:** il tasso di apprendimento riduce il contributo di ogni albero ad un fattore pari al valore che gli viene passato. Abbiamo visto che utilizzando un *learning rate* inferiore ad 1 (solitamente intorno a 0.1), riuscivamo ad ottenere classificazioni più precise e quindi punteggi migliori.\n",
        "*   **loss:** funzione di perdita da ottimizzare. 'ls' si riferisce alla regressione dei minimi quadrati. 'lad' (la deviazione minima assoluta) è una funzione di perdita altamente robusta basata esclusivamente sulle informazioni sull'ordine delle variabili di input. \"huber\" è una combinazione dei due. 'quantile' consente la regressione quantile (utilizzare alphaper specificare il quantile). Tra queste quella che ci ha portato punteggi migliori è stata *ls*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BOrxSnR6EIPP"
      },
      "source": [
        "##### Best Result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZjuPychOEIPQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "outputId": "873f0b91-039a-4405-b0e3-b02de74ea04b"
      },
      "source": [
        "#User defined XGBoost\n",
        "try:\n",
        "    import xgboost\n",
        "except ImportError as ex:\n",
        "    print(\"Error: the xgboost library is not installed.\")\n",
        "    xgboost = None\n",
        "\n",
        "xgb = xgboost.XGBClassifier()\n",
        "\n",
        "parameter_grid = {\n",
        "    'n_estimators': [100,150,200], \n",
        "    'max_depth':[6,7,8], \n",
        "    'min_samples_split': [5], \n",
        "    'learning_rate': [0.1,0.2], \n",
        "    'loss': ['ls']\n",
        "    }\n",
        "    \n",
        "cross_validation = StratifiedKFold(n_splits=12)\n",
        "cross_validation.get_n_splits(train_X_imp, train_y)\n",
        "#Create the scoring dictionary\n",
        "SCORING = {'accuracy': 'accuracy',\n",
        "'balanced_accuracy': 'balanced_accuracy',\n",
        "'precision': 'precision_macro',\n",
        "'recall': 'recall_macro',\n",
        "'f1': 'f1_macro'}\n",
        "\n",
        "grid_search = GridSearchCV(xgb, param_grid=parameter_grid, cv=cross_validation, scoring=SCORING,return_train_score=True, refit='f1')\n",
        "\n",
        "grid_search.fit(xTrain, yTrain)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=StratifiedKFold(n_splits=12, random_state=None, shuffle=False),\n",
              "             error_score=nan,\n",
              "             estimator=XGBClassifier(base_score=0.5, booster='gbtree',\n",
              "                                     colsample_bylevel=1, colsample_bynode=1,\n",
              "                                     colsample_bytree=1, gamma=0,\n",
              "                                     learning_rate=0.1, max_delta_step=0,\n",
              "                                     max_depth=3, min_child_weight=1,\n",
              "                                     missing=None, n_estimators=100, n_jobs=1,\n",
              "                                     nthread=None, objective='binary...\n",
              "             iid='deprecated', n_jobs=None,\n",
              "             param_grid={'learning_rate': [0.1, 0.2], 'loss': ['ls'],\n",
              "                         'max_depth': [6, 7, 8], 'min_samples_split': [5],\n",
              "                         'n_estimators': [100, 150, 200]},\n",
              "             pre_dispatch='2*n_jobs', refit='f1', return_train_score=True,\n",
              "             scoring={'accuracy': 'accuracy',\n",
              "                      'balanced_accuracy': 'balanced_accuracy',\n",
              "                      'f1': 'f1_macro', 'precision': 'precision_macro',\n",
              "                      'recall': 'recall_macro'},\n",
              "             verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1fi-vRcLqXW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "26f4a531-418a-4a04-b48a-dcc4c1a98929"
      },
      "source": [
        "grid_search.cv_results_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'mean_fit_time': array([0.30628908, 0.44566709, 0.58313106, 0.36986023, 0.53999446,\n",
              "        0.71109843, 0.44272047, 0.63794321, 0.84305042, 0.30162404,\n",
              "        0.44510049, 0.59921928, 0.38447269, 0.55673357, 0.73489141,\n",
              "        0.43137733, 0.63619016, 0.82382741]),\n",
              " 'mean_score_time': array([0.00568887, 0.00663728, 0.00729396, 0.00593758, 0.0069291 ,\n",
              "        0.00794244, 0.00624647, 0.0075832 , 0.00845782, 0.00584592,\n",
              "        0.00667735, 0.00759317, 0.00635165, 0.00742416, 0.0081139 ,\n",
              "        0.00637652, 0.00739054, 0.00850934]),\n",
              " 'mean_test_accuracy': array([0.79756179, 0.80578782, 0.80542555, 0.80400418, 0.81116188,\n",
              "        0.81331551, 0.81475075, 0.81689822, 0.81617983, 0.80328271,\n",
              "        0.80614702, 0.80471024, 0.80220512, 0.80829294, 0.80578474,\n",
              "        0.81367625, 0.81581909, 0.81223793]),\n",
              " 'mean_test_balanced_accuracy': array([0.76150422, 0.7689708 , 0.77068648, 0.7686427 , 0.77831387,\n",
              "        0.78222393, 0.78256775, 0.78520893, 0.78516786, 0.77083816,\n",
              "        0.77427251, 0.7724113 , 0.76826847, 0.77540949, 0.77350976,\n",
              "        0.78075015, 0.78313624, 0.77966083]),\n",
              " 'mean_test_f1': array([0.76803069, 0.77683517, 0.77738419, 0.77570213, 0.78453635,\n",
              "        0.78784233, 0.78897005, 0.79152753, 0.79109206, 0.77648297,\n",
              "        0.77984973, 0.77815817, 0.77439883, 0.78152781, 0.77910686,\n",
              "        0.78746496, 0.78979588, 0.78591516]),\n",
              " 'mean_test_precision': array([0.77894517, 0.78967669, 0.78820724, 0.78687011, 0.794303  ,\n",
              "        0.79624971, 0.79872816, 0.80118721, 0.79989198, 0.78464319,\n",
              "        0.78793418, 0.78657009, 0.78379936, 0.79097897, 0.78794044,\n",
              "        0.79747718, 0.80028328, 0.79563546]),\n",
              " 'mean_test_recall': array([0.76150422, 0.7689708 , 0.77068648, 0.7686427 , 0.77831387,\n",
              "        0.78222393, 0.78256775, 0.78520893, 0.78516786, 0.77083816,\n",
              "        0.77427251, 0.7724113 , 0.76826847, 0.77540949, 0.77350976,\n",
              "        0.78075015, 0.78313624, 0.77966083]),\n",
              " 'mean_train_accuracy': array([0.94654862, 0.97319275, 0.98706864, 0.97726462, 0.99188943,\n",
              "        0.99814334, 0.99215006, 0.99859932, 0.99967428, 0.99003251,\n",
              "        0.99872962, 0.99970684, 0.99889253, 0.99986969, 1.        ,\n",
              "        0.99980456, 1.        , 1.        ]),\n",
              " 'mean_train_balanced_accuracy': array([0.93392916, 0.9662631 , 0.98338749, 0.97123129, 0.98942719,\n",
              "        0.99744672, 0.99015079, 0.99806734, 0.99952403, 0.98712441,\n",
              "        0.99830332, 0.99957159, 0.99845014, 0.99980952, 1.        ,\n",
              "        0.99971439, 1.        , 1.        ]),\n",
              " 'mean_train_f1': array([0.93979653, 0.96998015, 0.98556876, 0.97456275, 0.99096197,\n",
              "        0.99793559, 0.99126139, 0.99844271, 0.99963813, 0.98888547,\n",
              "        0.99858791, 0.9996743 , 0.99876912, 0.99985521, 1.        ,\n",
              "        0.99978287, 1.        , 1.        ]),\n",
              " 'mean_train_precision': array([0.94663801, 0.97406321, 0.987872  , 0.97817594, 0.99255529,\n",
              "        0.99843065, 0.99240162, 0.99882219, 0.99975257, 0.99072776,\n",
              "        0.99887537, 0.99977732, 0.9990908 , 0.99990103, 1.        ,\n",
              "        0.99985154, 1.        , 1.        ]),\n",
              " 'mean_train_recall': array([0.93392916, 0.9662631 , 0.98338749, 0.97123129, 0.98942719,\n",
              "        0.99744672, 0.99015079, 0.99806734, 0.99952403, 0.98712441,\n",
              "        0.99830332, 0.99957159, 0.99845014, 0.99980952, 1.        ,\n",
              "        0.99971439, 1.        , 1.        ]),\n",
              " 'param_learning_rate': masked_array(data=[0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.2, 0.2,\n",
              "                    0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2],\n",
              "              mask=[False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False],\n",
              "        fill_value='?',\n",
              "             dtype=object),\n",
              " 'param_loss': masked_array(data=['ls', 'ls', 'ls', 'ls', 'ls', 'ls', 'ls', 'ls', 'ls',\n",
              "                    'ls', 'ls', 'ls', 'ls', 'ls', 'ls', 'ls', 'ls', 'ls'],\n",
              "              mask=[False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False],\n",
              "        fill_value='?',\n",
              "             dtype=object),\n",
              " 'param_max_depth': masked_array(data=[6, 6, 6, 7, 7, 7, 8, 8, 8, 6, 6, 6, 7, 7, 7, 8, 8, 8],\n",
              "              mask=[False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False],\n",
              "        fill_value='?',\n",
              "             dtype=object),\n",
              " 'param_min_samples_split': masked_array(data=[5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5],\n",
              "              mask=[False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False],\n",
              "        fill_value='?',\n",
              "             dtype=object),\n",
              " 'param_n_estimators': masked_array(data=[100, 150, 200, 100, 150, 200, 100, 150, 200, 100, 150,\n",
              "                    200, 100, 150, 200, 100, 150, 200],\n",
              "              mask=[False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False],\n",
              "        fill_value='?',\n",
              "             dtype=object),\n",
              " 'params': [{'learning_rate': 0.1,\n",
              "   'loss': 'ls',\n",
              "   'max_depth': 6,\n",
              "   'min_samples_split': 5,\n",
              "   'n_estimators': 100},\n",
              "  {'learning_rate': 0.1,\n",
              "   'loss': 'ls',\n",
              "   'max_depth': 6,\n",
              "   'min_samples_split': 5,\n",
              "   'n_estimators': 150},\n",
              "  {'learning_rate': 0.1,\n",
              "   'loss': 'ls',\n",
              "   'max_depth': 6,\n",
              "   'min_samples_split': 5,\n",
              "   'n_estimators': 200},\n",
              "  {'learning_rate': 0.1,\n",
              "   'loss': 'ls',\n",
              "   'max_depth': 7,\n",
              "   'min_samples_split': 5,\n",
              "   'n_estimators': 100},\n",
              "  {'learning_rate': 0.1,\n",
              "   'loss': 'ls',\n",
              "   'max_depth': 7,\n",
              "   'min_samples_split': 5,\n",
              "   'n_estimators': 150},\n",
              "  {'learning_rate': 0.1,\n",
              "   'loss': 'ls',\n",
              "   'max_depth': 7,\n",
              "   'min_samples_split': 5,\n",
              "   'n_estimators': 200},\n",
              "  {'learning_rate': 0.1,\n",
              "   'loss': 'ls',\n",
              "   'max_depth': 8,\n",
              "   'min_samples_split': 5,\n",
              "   'n_estimators': 100},\n",
              "  {'learning_rate': 0.1,\n",
              "   'loss': 'ls',\n",
              "   'max_depth': 8,\n",
              "   'min_samples_split': 5,\n",
              "   'n_estimators': 150},\n",
              "  {'learning_rate': 0.1,\n",
              "   'loss': 'ls',\n",
              "   'max_depth': 8,\n",
              "   'min_samples_split': 5,\n",
              "   'n_estimators': 200},\n",
              "  {'learning_rate': 0.2,\n",
              "   'loss': 'ls',\n",
              "   'max_depth': 6,\n",
              "   'min_samples_split': 5,\n",
              "   'n_estimators': 100},\n",
              "  {'learning_rate': 0.2,\n",
              "   'loss': 'ls',\n",
              "   'max_depth': 6,\n",
              "   'min_samples_split': 5,\n",
              "   'n_estimators': 150},\n",
              "  {'learning_rate': 0.2,\n",
              "   'loss': 'ls',\n",
              "   'max_depth': 6,\n",
              "   'min_samples_split': 5,\n",
              "   'n_estimators': 200},\n",
              "  {'learning_rate': 0.2,\n",
              "   'loss': 'ls',\n",
              "   'max_depth': 7,\n",
              "   'min_samples_split': 5,\n",
              "   'n_estimators': 100},\n",
              "  {'learning_rate': 0.2,\n",
              "   'loss': 'ls',\n",
              "   'max_depth': 7,\n",
              "   'min_samples_split': 5,\n",
              "   'n_estimators': 150},\n",
              "  {'learning_rate': 0.2,\n",
              "   'loss': 'ls',\n",
              "   'max_depth': 7,\n",
              "   'min_samples_split': 5,\n",
              "   'n_estimators': 200},\n",
              "  {'learning_rate': 0.2,\n",
              "   'loss': 'ls',\n",
              "   'max_depth': 8,\n",
              "   'min_samples_split': 5,\n",
              "   'n_estimators': 100},\n",
              "  {'learning_rate': 0.2,\n",
              "   'loss': 'ls',\n",
              "   'max_depth': 8,\n",
              "   'min_samples_split': 5,\n",
              "   'n_estimators': 150},\n",
              "  {'learning_rate': 0.2,\n",
              "   'loss': 'ls',\n",
              "   'max_depth': 8,\n",
              "   'min_samples_split': 5,\n",
              "   'n_estimators': 200}],\n",
              " 'rank_test_accuracy': array([18, 11, 13, 15,  8,  6,  4,  1,  2, 16, 10, 14, 17,  9, 12,  5,  3,\n",
              "         7], dtype=int32),\n",
              " 'rank_test_balanced_accuracy': array([18, 15, 14, 16,  8,  5,  4,  1,  2, 13, 10, 12, 17,  9, 11,  6,  3,\n",
              "         7], dtype=int32),\n",
              " 'rank_test_f1': array([18, 14, 13, 16,  8,  5,  4,  1,  2, 15, 10, 12, 17,  9, 11,  6,  3,\n",
              "         7], dtype=int32),\n",
              " 'rank_test_precision': array([18, 10, 11, 14,  8,  6,  4,  1,  3, 16, 13, 15, 17,  9, 12,  5,  2,\n",
              "         7], dtype=int32),\n",
              " 'rank_test_recall': array([18, 15, 14, 16,  8,  5,  4,  1,  2, 13, 10, 12, 17,  9, 11,  6,  3,\n",
              "         7], dtype=int32),\n",
              " 'split0_test_accuracy': array([0.76824034, 0.77682403, 0.76824034, 0.75965665, 0.78540773,\n",
              "        0.77682403, 0.78540773, 0.79399142, 0.77682403, 0.78111588,\n",
              "        0.77682403, 0.78111588, 0.78111588, 0.78540773, 0.78969957,\n",
              "        0.78969957, 0.79828326, 0.78969957]),\n",
              " 'split0_test_balanced_accuracy': array([0.72512255, 0.73762255, 0.7310866 , 0.71560458, 0.74117647,\n",
              "        0.7316585 , 0.7441585 , 0.75069444, 0.73762255, 0.74089052,\n",
              "        0.73762255, 0.74387255, 0.73492647, 0.74117647, 0.74742647,\n",
              "        0.7504085 , 0.75694444, 0.74742647]),\n",
              " 'split0_test_f1': array([0.73266191, 0.74438819, 0.73638116, 0.7227605 , 0.75064212,\n",
              "        0.74066781, 0.75246473, 0.76061644, 0.74438819, 0.74841742,\n",
              "        0.74438819, 0.75017345, 0.74469868, 0.75064212, 0.75653083,\n",
              "        0.7582834 , 0.76646834, 0.75653083]),\n",
              " 'split0_test_precision': array([0.74558824, 0.75490798, 0.74400449, 0.73520499, 0.76782798,\n",
              "        0.75725821, 0.76635472, 0.77839775, 0.75490798, 0.76056027,\n",
              "        0.75490798, 0.75952008, 0.76332418, 0.76782798, 0.77229815,\n",
              "        0.77085543, 0.78277288, 0.77229815]),\n",
              " 'split0_test_recall': array([0.72512255, 0.73762255, 0.7310866 , 0.71560458, 0.74117647,\n",
              "        0.7316585 , 0.7441585 , 0.75069444, 0.73762255, 0.74089052,\n",
              "        0.73762255, 0.74387255, 0.73492647, 0.74117647, 0.74742647,\n",
              "        0.7504085 , 0.75694444, 0.74742647]),\n",
              " 'split0_train_accuracy': array([0.94722439, 0.97615324, 0.98866302, 0.97498045, 0.9910086 ,\n",
              "        0.99804535, 0.99374511, 0.99804535, 0.99960907, 0.9910086 ,\n",
              "        0.99882721, 0.99960907, 0.99882721, 0.99960907, 1.        ,\n",
              "        1.        , 1.        , 1.        ]),\n",
              " 'split0_train_balanced_accuracy': array([0.93410508, 0.96870928, 0.98534895, 0.96781801, 0.98822884,\n",
              "        0.9974172 , 0.99168016, 0.9974172 , 0.99942857, 0.98822884,\n",
              "        0.99828571, 0.99942857, 0.99828571, 0.99942857, 1.        ,\n",
              "        1.        , 1.        , 1.        ]),\n",
              " 'split0_train_f1': array([0.94048131, 0.97324779, 0.98735398, 0.97195639, 0.98997602,\n",
              "        0.99782692, 0.99303263, 0.99782692, 0.99956562, 0.98997602,\n",
              "        0.99869615, 0.99956562, 0.99869615, 0.99956562, 1.        ,\n",
              "        1.        , 1.        , 1.        ]),\n",
              " 'split0_train_precision': array([0.94798709, 0.97829217, 0.98945202, 0.97651591, 0.99179316,\n",
              "        0.99824037, 0.99442656, 0.99824037, 0.99970309, 0.99179316,\n",
              "        0.99911032, 0.99970309, 0.99911032, 0.99970309, 1.        ,\n",
              "        1.        , 1.        , 1.        ]),\n",
              " 'split0_train_recall': array([0.93410508, 0.96870928, 0.98534895, 0.96781801, 0.98822884,\n",
              "        0.9974172 , 0.99168016, 0.9974172 , 0.99942857, 0.98822884,\n",
              "        0.99828571, 0.99942857, 0.99828571, 0.99942857, 1.        ,\n",
              "        1.        , 1.        , 1.        ]),\n",
              " 'split10_test_accuracy': array([0.78448276, 0.81034483, 0.79310345, 0.81465517, 0.82327586,\n",
              "        0.82327586, 0.81896552, 0.82327586, 0.81896552, 0.81465517,\n",
              "        0.80172414, 0.79741379, 0.79310345, 0.80172414, 0.7887931 ,\n",
              "        0.81896552, 0.81034483, 0.79741379]),\n",
              " 'split10_test_balanced_accuracy': array([0.74476711, 0.77355837, 0.75130305, 0.77988748, 0.78642343,\n",
              "        0.78948457, 0.78009432, 0.78336229, 0.78009432, 0.78294862,\n",
              "        0.77008356, 0.76681559, 0.75742533, 0.76702242, 0.75415736,\n",
              "        0.78315546, 0.77661951, 0.76375445]),\n",
              " 'split10_test_f1': array([0.75196716, 0.7817311 , 0.76014474, 0.78745073, 0.79587545,\n",
              "        0.79733674, 0.79012665, 0.79435737, 0.79012665, 0.7889258 ,\n",
              "        0.77496626, 0.77084253, 0.76356688, 0.77341826, 0.75947358,\n",
              "        0.79165241, 0.78326964, 0.76929099]),\n",
              " 'split10_test_precision': array([0.76304792, 0.79398951, 0.77476255, 0.79832451, 0.81043759,\n",
              "        0.80855379, 0.80624152, 0.81261409, 0.80624152, 0.796875  ,\n",
              "        0.78125269, 0.77582963, 0.77219841, 0.78234625, 0.76666667,\n",
              "        0.80430337, 0.7924941 , 0.77673611]),\n",
              " 'split10_test_recall': array([0.74476711, 0.77355837, 0.75130305, 0.77988748, 0.78642343,\n",
              "        0.78948457, 0.78009432, 0.78336229, 0.78009432, 0.78294862,\n",
              "        0.77008356, 0.76681559, 0.75742533, 0.76702242, 0.75415736,\n",
              "        0.78315546, 0.77661951, 0.76375445]),\n",
              " 'split10_train_accuracy': array([0.94607268, 0.97186401, 0.985932  , 0.97264556, 0.99179367,\n",
              "        0.99882767, 0.99218445, 0.99960922, 0.99960922, 0.99023056,\n",
              "        1.        , 1.        , 0.99882767, 1.        , 1.        ,\n",
              "        0.99960922, 1.        , 1.        ]),\n",
              " 'split10_train_balanced_accuracy': array([0.93354882, 0.96410418, 0.98109418, 0.96524573, 0.98910845,\n",
              "        0.99828767, 0.9902266 , 0.99942922, 0.99942922, 0.98737272,\n",
              "        1.        , 1.        , 0.99828767, 1.        , 1.        ,\n",
              "        0.99942922, 1.        , 1.        ]),\n",
              " 'split10_train_f1': array([0.93929615, 0.96845717, 0.9842736 , 0.96935099, 0.99085462,\n",
              "        0.99869713, 0.99130221, 0.99956595, 0.99956595, 0.98911264,\n",
              "        1.        , 1.        , 0.99869713, 1.        , 1.        ,\n",
              "        0.99956595, 1.        , 1.        ]),\n",
              " 'split10_train_precision': array([0.945963  , 0.97328344, 0.98769153, 0.97387527, 0.99267071,\n",
              "        0.99911032, 0.99240422, 0.99970309, 0.99970309, 0.99092234,\n",
              "        1.        , 1.        , 0.99911032, 1.        , 1.        ,\n",
              "        0.99970309, 1.        , 1.        ]),\n",
              " 'split10_train_recall': array([0.93354882, 0.96410418, 0.98109418, 0.96524573, 0.98910845,\n",
              "        0.99828767, 0.9902266 , 0.99942922, 0.99942922, 0.98737272,\n",
              "        1.        , 1.        , 0.99828767, 1.        , 1.        ,\n",
              "        0.99942922, 1.        , 1.        ]),\n",
              " 'split11_test_accuracy': array([0.84913793, 0.8362069 , 0.81896552, 0.83189655, 0.82758621,\n",
              "        0.82327586, 0.84482759, 0.84482759, 0.8362069 , 0.82327586,\n",
              "        0.81896552, 0.81896552, 0.81034483, 0.81034483, 0.80603448,\n",
              "        0.82327586, 0.82758621, 0.83189655]),\n",
              " 'split11_test_balanced_accuracy': array([0.83052039, 0.80541077, 0.7862166 , 0.79908166, 0.79581368,\n",
              "        0.79254571, 0.818069  , 0.818069  , 0.80847191, 0.79560685,\n",
              "        0.79233888, 0.7862166 , 0.78274179, 0.77968065, 0.77641267,\n",
              "        0.79254571, 0.79581368, 0.8021428 ]),\n",
              " 'split11_test_f1': array([0.83153177, 0.81282378, 0.79312102, 0.80722275, 0.8029724 ,\n",
              "        0.7987432 , 0.82388664, 0.82388664, 0.81410256, 0.80009667,\n",
              "        0.79589443, 0.79312102, 0.78617512, 0.78475034, 0.78059391,\n",
              "        0.7987432 , 0.8029724 , 0.80856061]),\n",
              " 'split11_test_precision': array([0.83258408, 0.82293763, 0.80264194, 0.81878307, 0.81278978,\n",
              "        0.80694444, 0.83122254, 0.83122254, 0.82122857, 0.80559357,\n",
              "        0.80008493, 0.80264194, 0.79023355, 0.79124666, 0.78575094,\n",
              "        0.80694444, 0.81278978, 0.81701389]),\n",
              " 'split11_test_recall': array([0.83052039, 0.80541077, 0.7862166 , 0.79908166, 0.79581368,\n",
              "        0.79254571, 0.818069  , 0.818069  , 0.80847191, 0.79560685,\n",
              "        0.79233888, 0.7862166 , 0.78274179, 0.77968065, 0.77641267,\n",
              "        0.79254571, 0.79581368, 0.8021428 ]),\n",
              " 'split11_train_accuracy': array([0.94646346, 0.97577179, 0.98788589, 0.97928878, 0.99062134,\n",
              "        0.99804611, 0.99101211, 0.99960922, 0.99960922, 0.99335678,\n",
              "        0.99882767, 0.99960922, 0.99960922, 1.        , 1.        ,\n",
              "        1.        , 1.        , 1.        ]),\n",
              " 'split11_train_balanced_accuracy': array([0.93357223, 0.96816981, 0.9853165 , 0.97303311, 0.98766981,\n",
              "        0.99714612, 0.98878796, 0.99942922, 0.99942922, 0.99139155,\n",
              "        0.99828767, 0.99942922, 0.99942922, 1.        , 1.        ,\n",
              "        1.        , 1.        , 1.        ]),\n",
              " 'split11_train_f1': array([0.93968275, 0.97282245, 0.98651469, 0.97681425, 0.98954522,\n",
              "        0.99782736, 0.98999477, 0.99956595, 0.99956595, 0.99260483,\n",
              "        0.99869713, 0.99956595, 0.99956595, 1.        , 1.        ,\n",
              "        1.        , 1.        , 1.        ]),\n",
              " 'split11_train_precision': array([0.94683443, 0.97800943, 0.98774612, 0.98094188, 0.99150165,\n",
              "        0.99851896, 0.99123496, 0.99970309, 0.99970309, 0.99385159,\n",
              "        0.99911032, 0.99970309, 0.99970309, 1.        , 1.        ,\n",
              "        1.        , 1.        , 1.        ]),\n",
              " 'split11_train_recall': array([0.93357223, 0.96816981, 0.9853165 , 0.97303311, 0.98766981,\n",
              "        0.99714612, 0.98878796, 0.99942922, 0.99942922, 0.99139155,\n",
              "        0.99828767, 0.99942922, 0.99942922, 1.        , 1.        ,\n",
              "        1.        , 1.        , 1.        ]),\n",
              " 'split1_test_accuracy': array([0.79399142, 0.8111588 , 0.8111588 , 0.80686695, 0.81545064,\n",
              "        0.82403433, 0.82403433, 0.82832618, 0.82832618, 0.80686695,\n",
              "        0.8111588 , 0.81545064, 0.80686695, 0.80686695, 0.8111588 ,\n",
              "        0.80686695, 0.81545064, 0.81974249]),\n",
              " 'split1_test_balanced_accuracy': array([0.75069444, 0.77271242, 0.77271242, 0.7754085 , 0.78492647,\n",
              "        0.79742647, 0.78848039, 0.79473039, 0.79174837, 0.7754085 ,\n",
              "        0.78464052, 0.78492647, 0.7754085 , 0.76944444, 0.77271242,\n",
              "        0.77242647, 0.78194444, 0.78819444]),\n",
              " 'split1_test_f1': array([0.76061644, 0.78216896, 0.78216896, 0.78105619, 0.79078703,\n",
              "        0.80182554, 0.79774734, 0.80337553, 0.80197178, 0.78105619,\n",
              "        0.78800662, 0.79078703, 0.78105619, 0.77801537, 0.78216896,\n",
              "        0.77956481, 0.78936193, 0.79496312]),\n",
              " 'split1_test_precision': array([0.77839775, 0.79750446, 0.79750446, 0.78861301, 0.79858733,\n",
              "        0.80721519, 0.81203606, 0.81617003, 0.81827094, 0.78861301,\n",
              "        0.79198793, 0.79858733, 0.78861301, 0.79144574, 0.79750446,\n",
              "        0.7899061 , 0.80003478, 0.80430469]),\n",
              " 'split1_test_recall': array([0.75069444, 0.77271242, 0.77271242, 0.7754085 , 0.78492647,\n",
              "        0.79742647, 0.78848039, 0.79473039, 0.79174837, 0.7754085 ,\n",
              "        0.78464052, 0.78492647, 0.7754085 , 0.76944444, 0.77271242,\n",
              "        0.77242647, 0.78194444, 0.78819444]),\n",
              " 'split1_train_accuracy': array([0.9421423 , 0.96520719, 0.98475371, 0.9781079 , 0.99061767,\n",
              "        0.99726349, 0.99296325, 0.99882721, 0.99960907, 0.99139953,\n",
              "        0.99921814, 0.99960907, 0.99843628, 0.99960907, 1.        ,\n",
              "        0.99960907, 1.        , 1.        ]),\n",
              " 'split1_train_balanced_accuracy': array([0.92969425, 0.95819608, 0.97990901, 0.97238944, 0.98820609,\n",
              "        0.99627434, 0.99136033, 0.99856005, 0.99942857, 0.98907461,\n",
              "        0.99885714, 0.99942857, 0.99771429, 0.99942857, 1.        ,\n",
              "        0.99942857, 1.        , 1.        ]),\n",
              " 'split1_train_f1': array([0.9349228 , 0.96110206, 0.98295483, 0.97551793, 0.98954895,\n",
              "        0.996956  , 0.99217041, 0.99869687, 0.99956562, 0.99041987,\n",
              "        0.99913101, 0.99956562, 0.99826105, 0.99956562, 1.        ,\n",
              "        0.99956562, 1.        , 1.        ]),\n",
              " 'split1_train_precision': array([0.94091853, 0.96422114, 0.98621958, 0.97888283, 0.99093308,\n",
              "        0.99764804, 0.99299536, 0.99883409, 0.99970309, 0.99180645,\n",
              "        0.99940653, 0.99970309, 0.99881446, 0.99970309, 1.        ,\n",
              "        0.99970309, 1.        , 1.        ]),\n",
              " 'split1_train_recall': array([0.92969425, 0.95819608, 0.97990901, 0.97238944, 0.98820609,\n",
              "        0.99627434, 0.99136033, 0.99856005, 0.99942857, 0.98907461,\n",
              "        0.99885714, 0.99942857, 0.99771429, 0.99942857, 1.        ,\n",
              "        0.99942857, 1.        , 1.        ]),\n",
              " 'split2_test_accuracy': array([0.80686695, 0.8111588 , 0.80257511, 0.8111588 , 0.81545064,\n",
              "        0.80257511, 0.80257511, 0.80257511, 0.8111588 , 0.78540773,\n",
              "        0.79828326, 0.79399142, 0.80686695, 0.80686695, 0.79828326,\n",
              "        0.80686695, 0.80686695, 0.81545064]),\n",
              " 'split2_test_balanced_accuracy': array([0.77242647, 0.77569444, 0.76319444, 0.77867647, 0.78492647,\n",
              "        0.77214052, 0.77214052, 0.77512255, 0.77867647, 0.75906863,\n",
              "        0.77185458, 0.76560458, 0.77839052, 0.78137255, 0.77185458,\n",
              "        0.7754085 , 0.7754085 , 0.78194444]),\n",
              " 'split2_test_f1': array([0.77956481, 0.78371308, 0.77226755, 0.78519946, 0.79078703,\n",
              "        0.7769314 , 0.7769314 , 0.77837055, 0.78519946, 0.76060332,\n",
              "        0.77426669, 0.76873449, 0.78249144, 0.78387236, 0.77426669,\n",
              "        0.78105619, 0.78105619, 0.78936193]),\n",
              " 'split2_test_precision': array([0.7899061 , 0.79574934, 0.78712121, 0.79425466, 0.79858733,\n",
              "        0.78310386, 0.78310386, 0.78222427, 0.79425466, 0.76228288,\n",
              "        0.77701465, 0.77246061, 0.78755274, 0.78671329, 0.77701465,\n",
              "        0.78861301, 0.78861301, 0.80003478]),\n",
              " 'split2_test_recall': array([0.77242647, 0.77569444, 0.76319444, 0.77867647, 0.78492647,\n",
              "        0.77214052, 0.77214052, 0.77512255, 0.77867647, 0.75906863,\n",
              "        0.77185458, 0.76560458, 0.77839052, 0.78137255, 0.77185458,\n",
              "        0.7754085 , 0.7754085 , 0.78194444]),\n",
              " 'split2_train_accuracy': array([0.94253323, 0.9691165 , 0.98827209, 0.97419859, 0.99335418,\n",
              "        0.99843628, 0.9910086 , 0.99804535, 1.        , 0.9870993 ,\n",
              "        0.99921814, 1.        , 0.99921814, 1.        , 1.        ,\n",
              "        1.        , 1.        , 1.        ]),\n",
              " 'split2_train_balanced_accuracy': array([0.929717  , 0.96171564, 0.98505186, 0.96749818, 0.99110873,\n",
              "        0.99798863, 0.98877752, 0.9974172 , 1.        , 0.98306324,\n",
              "        0.99885714, 1.        , 0.99885714, 1.        , 1.        ,\n",
              "        1.        , 1.        , 1.        ]),\n",
              " 'split2_train_f1': array([0.93530556, 0.96541347, 0.98692158, 0.9711132 , 0.99259511,\n",
              "        0.99826201, 0.98998719, 0.99782692, 1.        , 0.98559349,\n",
              "        0.99913101, 1.        , 0.99913101, 1.        , 1.        ,\n",
              "        1.        , 1.        , 1.        ]),\n",
              " 'split2_train_precision': array([0.94177208, 0.96945356, 0.98887221, 0.97504929, 0.99413166,\n",
              "        0.99853705, 0.9912303 , 0.99824037, 1.        , 0.98827314,\n",
              "        0.99940653, 1.        , 0.99940653, 1.        , 1.        ,\n",
              "        1.        , 1.        , 1.        ]),\n",
              " 'split2_train_recall': array([0.929717  , 0.96171564, 0.98505186, 0.96749818, 0.99110873,\n",
              "        0.99798863, 0.98877752, 0.9974172 , 1.        , 0.98306324,\n",
              "        0.99885714, 1.        , 0.99885714, 1.        , 1.        ,\n",
              "        1.        , 1.        , 1.        ]),\n",
              " 'split3_test_accuracy': array([0.80686695, 0.83690987, 0.83690987, 0.84120172, 0.84120172,\n",
              "        0.84978541, 0.84549356, 0.84120172, 0.84549356, 0.83690987,\n",
              "        0.84978541, 0.84120172, 0.82832618, 0.83690987, 0.82832618,\n",
              "        0.85407725, 0.84120172, 0.83261803]),\n",
              " 'split3_test_balanced_accuracy': array([0.76646242, 0.79530229, 0.80126634, 0.80155229, 0.80453431,\n",
              "        0.81703431, 0.80780229, 0.80155229, 0.80780229, 0.80126634,\n",
              "        0.81703431, 0.80751634, 0.78876634, 0.80126634, 0.78876634,\n",
              "        0.82030229, 0.80453431, 0.79799837]),\n",
              " 'split3_test_f1': array([0.77640586, 0.80904934, 0.81187319, 0.81478139, 0.81615593,\n",
              "        0.82734529, 0.82046233, 0.81478139, 0.82046233, 0.81187319,\n",
              "        0.82734529, 0.8174793 , 0.8005137 , 0.81187319, 0.8005137 ,\n",
              "        0.83167602, 0.81615593, 0.80761332]),\n",
              " 'split3_test_precision': array([0.79324762, 0.83418084, 0.82865419, 0.83800366, 0.83514656,\n",
              "        0.84292153, 0.84181637, 0.83800366, 0.84181637, 0.82865419,\n",
              "        0.84292153, 0.83262637, 0.82067683, 0.82865419, 0.82067683,\n",
              "        0.84942068, 0.83514656, 0.82233121]),\n",
              " 'split3_test_recall': array([0.76646242, 0.79530229, 0.80126634, 0.80155229, 0.80453431,\n",
              "        0.81703431, 0.80780229, 0.80155229, 0.80780229, 0.80126634,\n",
              "        0.81703431, 0.80751634, 0.78876634, 0.80126634, 0.78876634,\n",
              "        0.82030229, 0.80453431, 0.79799837]),\n",
              " 'split3_train_accuracy': array([0.9421423 , 0.9691165 , 0.98358092, 0.97458952, 0.98749023,\n",
              "        0.99726349, 0.98905395, 0.99765442, 0.99960907, 0.98670837,\n",
              "        0.99765442, 0.99960907, 0.99804535, 1.        , 1.        ,\n",
              "        0.99960907, 1.        , 1.        ]),\n",
              " 'split3_train_balanced_accuracy': array([0.92777387, 0.95924658, 0.97846906, 0.96724658, 0.98390901,\n",
              "        0.99654868, 0.98646906, 0.99684577, 0.99942857, 0.98221747,\n",
              "        0.99657143, 0.99942857, 0.9974172 , 1.        , 1.        ,\n",
              "        0.99942857, 1.        , 1.        ]),\n",
              " 'split3_train_f1': array([0.93465263, 0.96523224, 0.98163845, 0.97151   , 0.98604185,\n",
              "        0.99695768, 0.9878071 , 0.99739158, 0.99956562, 0.98514433,\n",
              "        0.99739014, 0.99956562, 0.99782692, 1.        , 1.        ,\n",
              "        0.99956562, 1.        , 1.        ]),\n",
              " 'split3_train_precision': array([0.94287952, 0.97213007, 0.98504602, 0.97622142, 0.98828044,\n",
              "        0.99737041, 0.98918635, 0.99794403, 0.99970309, 0.98827186,\n",
              "        0.9982238 , 0.99970309, 0.99824037, 1.        , 1.        ,\n",
              "        0.99970309, 1.        , 1.        ]),\n",
              " 'split3_train_recall': array([0.92777387, 0.95924658, 0.97846906, 0.96724658, 0.98390901,\n",
              "        0.99654868, 0.98646906, 0.99684577, 0.99942857, 0.98221747,\n",
              "        0.99657143, 0.99942857, 0.9974172 , 1.        , 1.        ,\n",
              "        0.99942857, 1.        , 1.        ]),\n",
              " 'split4_test_accuracy': array([0.8111588 , 0.81974249, 0.81974249, 0.82832618, 0.82403433,\n",
              "        0.82832618, 0.84120172, 0.84978541, 0.84549356, 0.82403433,\n",
              "        0.8111588 , 0.81545064, 0.83690987, 0.84120172, 0.84120172,\n",
              "        0.83261803, 0.84120172, 0.83690987]),\n",
              " 'split4_test_balanced_accuracy': array([0.78464052, 0.79117647, 0.79714052, 0.79771242, 0.8004085 ,\n",
              "        0.80069444, 0.81944444, 0.82896242, 0.82569444, 0.79742647,\n",
              "        0.77867647, 0.78492647, 0.81319444, 0.81944444, 0.81944444,\n",
              "        0.80694444, 0.81646242, 0.81021242]),\n",
              " 'split4_test_f1': array([0.78800662, 0.79632867, 0.79890679, 0.80472679, 0.80308371,\n",
              "        0.80602731, 0.82229505, 0.83190073, 0.82763439, 0.80182554,\n",
              "        0.78519946, 0.79078703, 0.81691481, 0.82229505, 0.82229505,\n",
              "        0.81149258, 0.82115963, 0.81572594]),\n",
              " 'split4_test_precision': array([0.79198793, 0.80290668, 0.80082713, 0.81435473, 0.80611056,\n",
              "        0.81280809, 0.82550783, 0.83520646, 0.82973532, 0.80721519,\n",
              "        0.79425466, 0.79858733, 0.82127891, 0.82550783, 0.82550783,\n",
              "        0.81704641, 0.82687764, 0.8227095 ]),\n",
              " 'split4_test_recall': array([0.78464052, 0.79117647, 0.79714052, 0.79771242, 0.8004085 ,\n",
              "        0.80069444, 0.81944444, 0.82896242, 0.82569444, 0.79742647,\n",
              "        0.77867647, 0.78492647, 0.81319444, 0.81944444, 0.81944444,\n",
              "        0.80694444, 0.81646242, 0.81021242]),\n",
              " 'split4_train_accuracy': array([0.95074277, 0.97498045, 0.98866302, 0.97849883, 0.99296325,\n",
              "        0.99921814, 0.99179046, 0.99921814, 0.99960907, 0.98631744,\n",
              "        0.99804535, 0.99960907, 0.99921814, 0.99960907, 1.        ,\n",
              "        0.99960907, 1.        , 1.        ]),\n",
              " 'split4_train_balanced_accuracy': array([0.9376019 , 0.96918971, 0.98589763, 0.97350955, 0.99136033,\n",
              "        0.99885714, 0.99046906, 0.99885714, 0.99942857, 0.98356642,\n",
              "        0.9974172 , 0.99942857, 0.99885714, 0.99942857, 1.        ,\n",
              "        0.99942857, 1.        , 1.        ]),\n",
              " 'split4_train_f1': array([0.94439963, 0.97203638, 0.98736812, 0.97598915, 0.99217041,\n",
              "        0.99913101, 0.99087304, 0.99913101, 0.99956562, 0.98476312,\n",
              "        0.99782692, 0.99956562, 0.99913101, 0.99956562, 1.        ,\n",
              "        0.99956562, 1.        , 1.        ]),\n",
              " 'split4_train_precision': array([0.95246802, 0.97508053, 0.98888848, 0.97861642, 0.99299536,\n",
              "        0.99940653, 0.99128073, 0.99940653, 0.99970309, 0.98599304,\n",
              "        0.99824037, 0.99970309, 0.99940653, 0.99970309, 1.        ,\n",
              "        0.99970309, 1.        , 1.        ]),\n",
              " 'split4_train_recall': array([0.9376019 , 0.96918971, 0.98589763, 0.97350955, 0.99136033,\n",
              "        0.99885714, 0.99046906, 0.99885714, 0.99942857, 0.98356642,\n",
              "        0.9974172 , 0.99942857, 0.99885714, 0.99942857, 1.        ,\n",
              "        0.99942857, 1.        , 1.        ]),\n",
              " 'split5_test_accuracy': array([0.81545064, 0.83690987, 0.85407725, 0.82832618, 0.85407725,\n",
              "        0.85407725, 0.84549356, 0.84120172, 0.84549356, 0.84978541,\n",
              "        0.84549356, 0.84978541, 0.81545064, 0.84120172, 0.84120172,\n",
              "        0.84978541, 0.85407725, 0.84549356]),\n",
              " 'split5_test_balanced_accuracy': array([0.77896242, 0.80126634, 0.82924837, 0.79174837, 0.83223039,\n",
              "        0.83223039, 0.81973039, 0.81646242, 0.82271242, 0.82001634,\n",
              "        0.81376634, 0.81703431, 0.77896242, 0.80751634, 0.81049837,\n",
              "        0.82299837, 0.82924837, 0.81973039]),\n",
              " 'split5_test_f1': array([0.78788135, 0.81187319, 0.83512321, 0.80197178, 0.83618693,\n",
              "        0.83618693, 0.82542458, 0.82115963, 0.82655087, 0.8285504 ,\n",
              "        0.82303797, 0.82734529, 0.78788135, 0.8174793 , 0.81875328,\n",
              "        0.82971037, 0.83512321, 0.82542458]),\n",
              " 'split5_test_precision': array([0.8017409 , 0.82865419, 0.84251232, 0.81827094, 0.84080624,\n",
              "        0.84080624, 0.83261091, 0.82687764, 0.83104257, 0.84054947,\n",
              "        0.83659071, 0.84292153, 0.8017409 , 0.83262637, 0.8304208 ,\n",
              "        0.83848459, 0.84251232, 0.83261091]),\n",
              " 'split5_test_recall': array([0.77896242, 0.80126634, 0.82924837, 0.79174837, 0.83223039,\n",
              "        0.83223039, 0.81973039, 0.81646242, 0.82271242, 0.82001634,\n",
              "        0.81376634, 0.81703431, 0.77896242, 0.80751634, 0.81049837,\n",
              "        0.82299837, 0.82924837, 0.81973039]),\n",
              " 'split5_train_accuracy': array([0.9421423 , 0.97028929, 0.98123534, 0.97615324, 0.99139953,\n",
              "        0.99765442, 0.99139953, 0.99687256, 0.99960907, 0.98631744,\n",
              "        0.99687256, 0.99960907, 0.99882721, 1.        , 1.        ,\n",
              "        0.99960907, 1.        , 1.        ]),\n",
              " 'split5_train_balanced_accuracy': array([0.92859689, 0.96342993, 0.97668653, 0.9695323 , 0.98825159,\n",
              "        0.99657143, 0.98907461, 0.99570291, 0.99942857, 0.98192038,\n",
              "        0.99625159, 0.99942857, 0.99828571, 1.        , 1.        ,\n",
              "        0.99942857, 1.        , 1.        ]),\n",
              " 'split5_train_f1': array([0.93476923, 0.96675541, 0.97905099, 0.97329395, 0.99040377,\n",
              "        0.99739014, 0.99041987, 0.99652018, 0.99956562, 0.98471173,\n",
              "        0.99652402, 0.99956562, 0.99869615, 1.        , 1.        ,\n",
              "        0.99956562, 1.        , 1.        ]),\n",
              " 'split5_train_precision': array([0.94201995, 0.97035579, 0.98154837, 0.97740121, 0.99266226,\n",
              "        0.9982238 , 0.99180645, 0.99735239, 0.99970309, 0.98768568,\n",
              "        0.99679811, 0.99970309, 0.99911032, 1.        , 1.        ,\n",
              "        0.99970309, 1.        , 1.        ]),\n",
              " 'split5_train_recall': array([0.92859689, 0.96342993, 0.97668653, 0.9695323 , 0.98825159,\n",
              "        0.99657143, 0.98907461, 0.99570291, 0.99942857, 0.98192038,\n",
              "        0.99625159, 0.99942857, 0.99828571, 1.        , 1.        ,\n",
              "        0.99942857, 1.        , 1.        ]),\n",
              " 'split6_test_accuracy': array([0.78540773, 0.79399142, 0.80257511, 0.77682403, 0.78969957,\n",
              "        0.79399142, 0.78969957, 0.79828326, 0.80257511, 0.77682403,\n",
              "        0.79399142, 0.78969957, 0.78540773, 0.79399142, 0.78540773,\n",
              "        0.78540773, 0.80257511, 0.78969957]),\n",
              " 'split6_test_balanced_accuracy': array([0.74117647, 0.75367647, 0.76319444, 0.73464052, 0.75339052,\n",
              "        0.76262255, 0.75637255, 0.7629085 , 0.7691585 , 0.73762255,\n",
              "        0.7566585 , 0.75935458, 0.74714052, 0.75367647, 0.74714052,\n",
              "        0.7441585 , 0.76021242, 0.7504085 ]),\n",
              " 'split6_test_f1': array([0.75064212, 0.76236614, 0.77226755, 0.74256332, 0.75997057,\n",
              "        0.76723277, 0.76159452, 0.76976769, 0.7754358 , 0.74438819,\n",
              "        0.76405063, 0.76315735, 0.75421941, 0.76236614, 0.75421941,\n",
              "        0.75246473, 0.77059075, 0.7582834 ]),\n",
              " 'split6_test_precision': array([0.76782798, 0.77673797, 0.78712121, 0.75597148, 0.76964876,\n",
              "        0.77320245, 0.76866438, 0.77977743, 0.78420462, 0.75490798,\n",
              "        0.77532866, 0.7678903 , 0.76511832, 0.77673797, 0.76511832,\n",
              "        0.76635472, 0.78896752, 0.77085543]),\n",
              " 'split6_test_recall': array([0.74117647, 0.75367647, 0.76319444, 0.73464052, 0.75339052,\n",
              "        0.76262255, 0.75637255, 0.7629085 , 0.7691585 , 0.73762255,\n",
              "        0.7566585 , 0.75935458, 0.74714052, 0.75367647, 0.74714052,\n",
              "        0.7441585 , 0.76021242, 0.7504085 ]),\n",
              " 'split6_train_accuracy': array([0.94800625, 0.9781079 , 0.98866302, 0.98553557, 0.99374511,\n",
              "        0.99765442, 0.99413604, 0.99921814, 0.99960907, 0.98944488,\n",
              "        0.99921814, 0.99960907, 0.99921814, 0.99960907, 1.        ,\n",
              "        1.        , 1.        , 1.        ]),\n",
              " 'split6_train_balanced_accuracy': array([0.9349736 , 0.97238944, 0.98480027, 0.9813262 , 0.99140582,\n",
              "        0.99657143, 0.99225159, 0.99885714, 0.99942857, 0.98676615,\n",
              "        0.99885714, 0.99942857, 0.99885714, 0.99942857, 1.        ,\n",
              "        1.        , 1.        , 1.        ]),\n",
              " 'split6_train_f1': array([0.94136307, 0.97551793, 0.98733973, 0.98384725, 0.99302874,\n",
              "        0.99739014, 0.99346991, 0.99913101, 0.99956562, 0.98823929,\n",
              "        0.99913101, 0.99956562, 0.99913101, 0.99956562, 1.        ,\n",
              "        1.        , 1.        , 1.        ]),\n",
              " 'split6_train_precision': array([0.94888339, 0.97888283, 0.99002891, 0.98651737, 0.99471146,\n",
              "        0.9982238 , 0.9947218 , 0.99940653, 0.99970309, 0.98976234,\n",
              "        0.99940653, 0.99970309, 0.99940653, 0.99970309, 1.        ,\n",
              "        1.        , 1.        , 1.        ]),\n",
              " 'split6_train_recall': array([0.9349736 , 0.97238944, 0.98480027, 0.9813262 , 0.99140582,\n",
              "        0.99657143, 0.99225159, 0.99885714, 0.99942857, 0.98676615,\n",
              "        0.99885714, 0.99942857, 0.99885714, 0.99942857, 1.        ,\n",
              "        1.        , 1.        , 1.        ]),\n",
              " 'split7_test_accuracy': array([0.76724138, 0.75862069, 0.77586207, 0.76724138, 0.77155172,\n",
              "        0.79741379, 0.80172414, 0.80172414, 0.80172414, 0.78448276,\n",
              "        0.7887931 , 0.7887931 , 0.77586207, 0.78448276, 0.7887931 ,\n",
              "        0.81034483, 0.80172414, 0.79741379]),\n",
              " 'split7_test_balanced_accuracy': array([0.71638951, 0.70373128, 0.72292546, 0.71945065, 0.71965748,\n",
              "        0.75150989, 0.757839  , 0.76396128, 0.76702242, 0.74170597,\n",
              "        0.74497394, 0.7419128 , 0.73210888, 0.74170597, 0.74497394,\n",
              "        0.76743609, 0.75477786, 0.74844875]),\n",
              " 'split7_test_f1': array([0.72600822, 0.71358025, 0.7340388 , 0.728125  , 0.73001339,\n",
              "        0.76245561, 0.76840278, 0.77180978, 0.77341826, 0.75015077,\n",
              "        0.75423197, 0.75234734, 0.7401568 , 0.75015077, 0.75423197,\n",
              "        0.77847222, 0.7665996 , 0.76057792]),\n",
              " 'split7_test_precision': array([0.74565605, 0.73607516, 0.75831656, 0.74435744, 0.75189753,\n",
              "        0.7827381 , 0.78710272, 0.78367565, 0.78234625, 0.76426956,\n",
              "        0.77026287, 0.7719494 , 0.75377657, 0.76426956, 0.77026287,\n",
              "        0.79778904, 0.7892364 , 0.78491461]),\n",
              " 'split7_test_recall': array([0.71638951, 0.70373128, 0.72292546, 0.71945065, 0.71965748,\n",
              "        0.75150989, 0.757839  , 0.76396128, 0.76702242, 0.74170597,\n",
              "        0.74497394, 0.7419128 , 0.73210888, 0.74170597, 0.74497394,\n",
              "        0.76743609, 0.75477786, 0.74844875]),\n",
              " 'split7_train_accuracy': array([0.9441188 , 0.97420868, 0.99023056, 0.97499023, 0.99023056,\n",
              "        0.99804611, 0.99218445, 0.99921844, 0.99960922, 0.99101211,\n",
              "        0.99921844, 1.        , 0.99882767, 1.        , 1.        ,\n",
              "        1.        , 1.        , 1.        ]),\n",
              " 'split7_train_balanced_accuracy': array([0.93178969, 0.96807621, 0.98764641, 0.96894407, 0.9879201 ,\n",
              "        0.99741981, 0.9902266 , 0.99885845, 0.99942922, 0.98824058,\n",
              "        0.99913214, 1.        , 0.99828767, 1.        , 1.        ,\n",
              "        1.        , 1.        , 1.        ]),\n",
              " 'split7_train_f1': array([0.93715205, 0.97116826, 0.98911871, 0.97204195, 0.98912475,\n",
              "        0.99782855, 0.99130221, 0.99913166, 0.99956595, 0.98998363,\n",
              "        0.99913214, 1.        , 0.99869713, 1.        , 1.        ,\n",
              "        1.        , 1.        , 1.        ]),\n",
              " 'split7_train_precision': array([0.94331837, 0.97449504, 0.99064089, 0.9753748 , 0.99036275,\n",
              "        0.99824102, 0.99240422, 0.99940653, 0.99970309, 0.99179652,\n",
              "        0.99913214, 1.        , 0.99911032, 1.        , 1.        ,\n",
              "        1.        , 1.        , 1.        ]),\n",
              " 'split7_train_recall': array([0.93178969, 0.96807621, 0.98764641, 0.96894407, 0.9879201 ,\n",
              "        0.99741981, 0.9902266 , 0.99885845, 0.99942922, 0.98824058,\n",
              "        0.99913214, 1.        , 0.99828767, 1.        , 1.        ,\n",
              "        1.        , 1.        , 1.        ]),\n",
              " 'split8_test_accuracy': array([0.77586207, 0.77155172, 0.79310345, 0.77586207, 0.78448276,\n",
              "        0.7887931 , 0.78448276, 0.78448276, 0.79310345, 0.76724138,\n",
              "        0.78448276, 0.77586207, 0.78017241, 0.78448276, 0.7887931 ,\n",
              "        0.78448276, 0.78448276, 0.78017241]),\n",
              " 'split8_test_balanced_accuracy': array([0.75353686, 0.7441466 , 0.76966989, 0.75353686, 0.76007281,\n",
              "        0.76640192, 0.76007281, 0.76007281, 0.76966989, 0.73781749,\n",
              "        0.76007281, 0.75047572, 0.75374369, 0.76007281, 0.76946306,\n",
              "        0.75701167, 0.76007281, 0.75374369]),\n",
              " 'split8_test_f1': array([0.75197368, 0.74489097, 0.76966989, 0.75197368, 0.76007281,\n",
              "        0.76558408, 0.76007281, 0.76007281, 0.76966989, 0.73926074,\n",
              "        0.76007281, 0.75047572, 0.75451773, 0.76007281, 0.766968  ,\n",
              "        0.75857476, 0.76007281, 0.75451773]),\n",
              " 'split8_test_precision': array([0.75055188, 0.745671  , 0.76966989, 0.75055188, 0.76007281,\n",
              "        0.76480263, 0.76007281, 0.76007281, 0.76966989, 0.74084625,\n",
              "        0.76007281, 0.75047572, 0.75532801, 0.76007281, 0.76479675,\n",
              "        0.76028488, 0.76007281, 0.75532801]),\n",
              " 'split8_test_recall': array([0.75353686, 0.7441466 , 0.76966989, 0.75353686, 0.76007281,\n",
              "        0.76640192, 0.76007281, 0.76007281, 0.76966989, 0.73781749,\n",
              "        0.76007281, 0.75047572, 0.75374369, 0.76007281, 0.76946306,\n",
              "        0.75701167, 0.76007281, 0.75374369]),\n",
              " 'split8_train_accuracy': array([0.95740524, 0.98163345, 0.99140289, 0.97616256, 0.99570145,\n",
              "        0.99921844, 0.99491989, 0.99882767, 1.        , 0.99531067,\n",
              "        0.99843689, 0.99960922, 0.99921844, 1.        , 1.        ,\n",
              "        1.        , 1.        , 1.        ]),\n",
              " 'split8_train_balanced_accuracy': array([0.94818552, 0.97700514, 0.98881136, 0.97093009, 0.99454252,\n",
              "        0.99885845, 0.99312728, 0.99828767, 1.        , 0.99397175,\n",
              "        0.99799058, 0.99942922, 0.99885845, 1.        , 1.        ,\n",
              "        1.        , 1.        , 1.        ]),\n",
              " 'split8_train_f1': array([0.95226139, 0.97949731, 0.9904218 , 0.97339012, 0.99522018,\n",
              "        0.99913166, 0.99434487, 0.99869713, 1.        , 0.99478421,\n",
              "        0.99826332, 0.99956595, 0.99913166, 1.        , 1.        ,\n",
              "        1.        , 1.        , 1.        ]),\n",
              " 'split8_train_precision': array([0.9567738 , 0.98213749, 0.99209173, 0.97599708, 0.99590817,\n",
              "        0.99940653, 0.99559601, 0.99911032, 1.        , 0.99561156,\n",
              "        0.99853771, 0.99970309, 0.99940653, 1.        , 1.        ,\n",
              "        1.        , 1.        , 1.        ]),\n",
              " 'split8_train_recall': array([0.94818552, 0.97700514, 0.98881136, 0.97093009, 0.99454252,\n",
              "        0.99885845, 0.99312728, 0.99828767, 1.        , 0.99397175,\n",
              "        0.99799058, 0.99942922, 0.99885845, 1.        , 1.        ,\n",
              "        1.        , 1.        , 1.        ]),\n",
              " 'split9_test_accuracy': array([0.80603448, 0.80603448, 0.7887931 , 0.80603448, 0.80172414,\n",
              "        0.79741379, 0.79310345, 0.79310345, 0.7887931 , 0.7887931 ,\n",
              "        0.79310345, 0.7887931 , 0.80603448, 0.80603448, 0.80172414,\n",
              "        0.80172414, 0.80603448, 0.81034483]),\n",
              " 'split9_test_balanced_accuracy': array([0.77335153, 0.77335153, 0.76027964, 0.77641267, 0.77620584,\n",
              "        0.77293787, 0.76660875, 0.76660875, 0.76334078, 0.76027964,\n",
              "        0.76354761, 0.76027964, 0.77641267, 0.78253495, 0.77926698,\n",
              "        0.77620584, 0.78559609, 0.79192521]),\n",
              " 'split9_test_f1': array([0.77910839, 0.77910839, 0.76264746, 0.78059391, 0.77788878,\n",
              "        0.77377124, 0.76823177, 0.76823177, 0.76414448, 0.76264746,\n",
              "        0.76673649, 0.76264746, 0.78059391, 0.783398  , 0.77926698,\n",
              "        0.77788878, 0.78472007, 0.79013158]),\n",
              " 'split9_test_precision': array([0.78680556, 0.78680556, 0.76535088, 0.78575094, 0.7797235 ,\n",
              "        0.77464202, 0.77000419, 0.77000419, 0.76498501, 0.76535088,\n",
              "        0.77053079, 0.76535088, 0.78575094, 0.78429903, 0.77926698,\n",
              "        0.7797235 , 0.78388158, 0.78848827]),\n",
              " 'split9_test_recall': array([0.77335153, 0.77335153, 0.76027964, 0.77641267, 0.77620584,\n",
              "        0.77293787, 0.76660875, 0.76660875, 0.76334078, 0.76027964,\n",
              "        0.76354761, 0.76027964, 0.77641267, 0.78253495, 0.77926698,\n",
              "        0.77620584, 0.78559609, 0.79192521]),\n",
              " 'split9_train_accuracy': array([0.94958968, 0.97186401, 0.98554123, 0.98202423, 0.99374756,\n",
              "        0.99804611, 0.99140289, 0.99804611, 0.99960922, 0.99218445,\n",
              "        0.99921844, 0.99960922, 0.99843689, 1.        , 1.        ,\n",
              "        0.99960922, 1.        , 1.        ]),\n",
              " 'split9_train_balanced_accuracy': array([0.93759106, 0.96492524, 0.98161816, 0.97730223, 0.99141496,\n",
              "        0.99741981, 0.98935874, 0.99714612, 0.99942922, 0.98967923,\n",
              "        0.99913214, 0.99942922, 0.99826427, 1.        , 1.        ,\n",
              "        0.99942922, 1.        , 1.        ]),\n",
              " 'split9_train_f1': array([0.94327176, 0.96851138, 0.98386866, 0.97992788, 0.99303403,\n",
              "        0.99782855, 0.99043244, 0.99782736, 0.99956595, 0.99129254,\n",
              "        0.99913214, 0.99956595, 0.99826427, 1.        , 1.        ,\n",
              "        0.99956595, 1.        , 1.        ]),\n",
              " 'split9_train_precision': array([0.94983789, 0.97241696, 0.9862381 , 0.98271776, 0.9947128 ,\n",
              "        0.99824102, 0.9915325 , 0.99851896, 0.99970309, 0.99296542,\n",
              "        0.99913214, 0.99970309, 0.99826427, 1.        , 1.        ,\n",
              "        0.99970309, 1.        , 1.        ]),\n",
              " 'split9_train_recall': array([0.93759106, 0.96492524, 0.98161816, 0.97730223, 0.99141496,\n",
              "        0.99741981, 0.98935874, 0.99714612, 0.99942922, 0.98967923,\n",
              "        0.99913214, 0.99942922, 0.99826427, 1.        , 1.        ,\n",
              "        0.99942922, 1.        , 1.        ]),\n",
              " 'std_fit_time': array([0.00637272, 0.00905675, 0.01045798, 0.00607623, 0.00601618,\n",
              "        0.00878128, 0.00895623, 0.00910053, 0.01290487, 0.00596436,\n",
              "        0.00584821, 0.01763444, 0.02025723, 0.01190468, 0.0239821 ,\n",
              "        0.00629573, 0.00911855, 0.0068808 ]),\n",
              " 'std_score_time': array([0.00011943, 0.00019654, 0.00013847, 0.00011564, 0.00014895,\n",
              "        0.00019842, 0.00018513, 0.00078394, 0.00029175, 0.000279  ,\n",
              "        0.0001196 , 0.00031029, 0.00061842, 0.0006057 , 0.00029387,\n",
              "        0.00023042, 0.00019369, 0.0004529 ]),\n",
              " 'std_test_accuracy': array([0.0223447 , 0.02501773, 0.02351014, 0.02641126, 0.02390403,\n",
              "        0.02319786, 0.02378678, 0.02261601, 0.02306686, 0.02523082,\n",
              "        0.02178807, 0.0225211 , 0.01865409, 0.02021457, 0.01970331,\n",
              "        0.02215094, 0.01991698, 0.02055549]),\n",
              " 'std_test_balanced_accuracy': array([0.02904047, 0.02849828, 0.0285136 , 0.02935274, 0.02932611,\n",
              "        0.02702871, 0.02633369, 0.02508594, 0.02546451, 0.02741888,\n",
              "        0.0236177 , 0.02300092, 0.02268672, 0.02391716, 0.02297956,\n",
              "        0.02466299, 0.02329672, 0.02483745]),\n",
              " 'std_test_f1': array([0.02744905, 0.02933891, 0.02805257, 0.03036131, 0.02876035,\n",
              "        0.0269877 , 0.02683891, 0.02544155, 0.02592072, 0.02840084,\n",
              "        0.02458852, 0.02471838, 0.02223484, 0.02369728, 0.02275044,\n",
              "        0.02524651, 0.02306908, 0.02428291]),\n",
              " 'std_test_precision': array([0.02476056, 0.03040296, 0.02728791, 0.03203497, 0.02753817,\n",
              "        0.02690482, 0.02837496, 0.02698255, 0.02767843, 0.03006365,\n",
              "        0.02645752, 0.02769731, 0.02195414, 0.02396379, 0.02353129,\n",
              "        0.02647435, 0.02327719, 0.02357784]),\n",
              " 'std_test_recall': array([0.02904047, 0.02849828, 0.0285136 , 0.02935274, 0.02932611,\n",
              "        0.02702871, 0.02633369, 0.02508594, 0.02546451, 0.02741888,\n",
              "        0.0236177 , 0.02300092, 0.02268672, 0.02391716, 0.02297956,\n",
              "        0.02466299, 0.02329672, 0.02483745]),\n",
              " 'std_train_accuracy': array([0.00435694, 0.00432059, 0.00279436, 0.00351338, 0.0020632 ,\n",
              "        0.00064093, 0.00153456, 0.00082095, 0.00014567, 0.00281312,\n",
              "        0.00081576, 0.00016926, 0.00041717, 0.00018429, 0.        ,\n",
              "        0.00019544, 0.        , 0.        ]),\n",
              " 'std_train_balanced_accuracy': array([0.0053081 , 0.00519504, 0.00360296, 0.00441628, 0.00259516,\n",
              "        0.00085581, 0.00173857, 0.00110495, 0.00021286, 0.0036331 ,\n",
              "        0.0010514 , 0.00024734, 0.00053011, 0.00026937, 0.        ,\n",
              "        0.00028561, 0.        , 0.        ]),\n",
              " 'std_train_f1': array([0.00495373, 0.00485397, 0.00313275, 0.00395302, 0.00230411,\n",
              "        0.00071318, 0.00170925, 0.00091346, 0.00016183, 0.0031476 ,\n",
              "        0.00090708, 0.00018804, 0.00046377, 0.00020477, 0.        ,\n",
              "        0.00021713, 0.        , 0.        ]),\n",
              " 'std_train_precision': array([0.00462944, 0.00467276, 0.00270922, 0.00349516, 0.00205453,\n",
              "        0.00059972, 0.00171943, 0.00072801, 0.00011065, 0.00268881,\n",
              "        0.00079636, 0.00012857, 0.00043377, 0.00013997, 0.        ,\n",
              "        0.00014846, 0.        , 0.        ]),\n",
              " 'std_train_recall': array([0.0053081 , 0.00519504, 0.00360296, 0.00441628, 0.00259516,\n",
              "        0.00085581, 0.00173857, 0.00110495, 0.00021286, 0.0036331 ,\n",
              "        0.0010514 , 0.00024734, 0.00053011, 0.00026937, 0.        ,\n",
              "        0.00028561, 0.        , 0.        ])}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dBPF3eRkLqXj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "d96eb9c2-4dc8-44b5-8702-f3e0adcdf423"
      },
      "source": [
        "print('Best score: {}'.format(grid_search.best_score_))\n",
        "print('Best parameters: {}'.format(grid_search.best_params_))\n",
        "\n",
        "best_xgb = grid_search.best_estimator_\n",
        "best_xgb"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best score: 0.7915275269714278\n",
            "Best parameters: {'learning_rate': 0.1, 'loss': 'ls', 'max_depth': 8, 'min_samples_split': 5, 'n_estimators': 150}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
              "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
              "              learning_rate=0.1, loss='ls', max_delta_step=0, max_depth=8,\n",
              "              min_child_weight=1, min_samples_split=5, missing=None,\n",
              "              n_estimators=150, n_jobs=1, nthread=None,\n",
              "              objective='binary:logistic', random_state=0, reg_alpha=0,\n",
              "              reg_lambda=1, scale_pos_weight=1, seed=None, silent=None,\n",
              "              subsample=1, verbosity=1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KLiGmYjLmluG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b01eca1e-3c54-41bb-c1df-548e327ee47c"
      },
      "source": [
        "my_model=best_xgb\n",
        "my_model.fit(xTrain, yTrain)\n",
        "my_model.score(xTest, yTest)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8065902578796562"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_IeZREYLqX6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "a5151636-0575-4bd7-a0d3-dca783ff5546"
      },
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "y_pred = my_model.predict(xTrain)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(yTrain, y_pred))\n",
        "\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(yTrain, y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Confusion Matrix:\n",
            "[[ 948    7]\n",
            " [   0 1836]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.99      1.00       955\n",
            "           1       1.00      1.00      1.00      1836\n",
            "\n",
            "    accuracy                           1.00      2791\n",
            "   macro avg       1.00      1.00      1.00      2791\n",
            "weighted avg       1.00      1.00      1.00      2791\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97UBgbYNLqYG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "outputId": "7ff66feb-acf3-4c09-da1d-aac02f623347"
      },
      "source": [
        "# The snippet below will retrieve the feature importances from the model and make them into a DataFrame.\n",
        "feature_importances = pd.DataFrame(my_model.feature_importances_,\n",
        "                                   index = train_X.columns,\n",
        "                                   columns=['importance']).sort_values('importance', ascending=False)\n",
        "feature_importances"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>importance</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>alcohol</th>\n",
              "      <td>0.261550</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>volatile.acidity</th>\n",
              "      <td>0.124965</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>free.sulfur.dioxide</th>\n",
              "      <td>0.088863</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>fixed.acidity</th>\n",
              "      <td>0.068570</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>residual.sugar</th>\n",
              "      <td>0.068069</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sulphates</th>\n",
              "      <td>0.067639</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>citric.acid</th>\n",
              "      <td>0.065586</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pH</th>\n",
              "      <td>0.065426</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>density</th>\n",
              "      <td>0.064982</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>total.sulfur.dioxide</th>\n",
              "      <td>0.062853</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>chlorides</th>\n",
              "      <td>0.061498</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                      importance\n",
              "alcohol                 0.261550\n",
              "volatile.acidity        0.124965\n",
              "free.sulfur.dioxide     0.088863\n",
              "fixed.acidity           0.068570\n",
              "residual.sugar          0.068069\n",
              "sulphates               0.067639\n",
              "citric.acid             0.065586\n",
              "pH                      0.065426\n",
              "density                 0.064982\n",
              "total.sulfur.dioxide    0.062853\n",
              "chlorides               0.061498"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AkKGdjHelq0e",
        "colab_type": "text"
      },
      "source": [
        "Public Score: **0.77650**\n",
        "\n",
        "Private Score: **0.82699**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vLE3dftaEIPT"
      },
      "source": [
        "XGBoost si è rivelato un buon metodo che ci ha permesso di ottenere risultati sopra la media, spesso con un valore superiore di private score rispetto che al public score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BsIamdomhl4T"
      },
      "source": [
        "#### AdaBoost"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UA_AYAsPhl4a"
      },
      "source": [
        "Uno degli algoritmi di Boosting che abbiamo osservato e utilizzato è AdaBoost (**Ada**ptive **Boost**ing). Esso è pensato per la classificazione binaria.\n",
        "Rappresenta una tecnica di boosting che permette di combinare più “classificatori deboli” in un unico “classificatore forte”. Un classificatore debole è semplicemente un classificatore che funziona male, ma funziona meglio di un’ipotesi casuale. Mettendo insieme tanti modelli di questo tipo, l’AdaBoost riesce a generare un modello che complessivamente è migliore dei singoli classificatori deboli presi singolarmente.\n",
        "\n",
        "Ad ogni iterazione, un nuovo classificatore debole viene introdotto in sequenza e mira a compensare le “carenze” dei modelli precedenti per creare un forte classificatore."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yvZDhMzPhl4b",
        "colab": {}
      },
      "source": [
        "# Codice standard\n",
        "\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "\n",
        "ada = AdaBoostClassifier()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Bm3V0VPGhl4g"
      },
      "source": [
        "##### Il nostro utilizzo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fK1CpPeWhl4h"
      },
      "source": [
        "Come in molti altri casi, abbiamo sfruttato AdaBoost insieme al metodo della Grid Search, modificando i suoi parametri più significativi, oltre che quelli del Decision Tree passatogli come stimatore, cercando di ottenere il miglior risultato possibile. I parametri più importanti su cui abbiamo agito sono:\n",
        "\n",
        "*   **base_estimator:** lo stimatore base dal quale costruire il metodo d'insieme. Se non è specificato, lo stimatore base è un Decision Tree con *max_depth*=1. Pertando abbiamo deciso di prendere il Decision Tree migliore ottenuto attraverso una Grid Search, al fine di avere una buona base di partenza.\n",
        "*   **n_estimators:** il numero massimo di stimatori utilizzati, raggiunti i quali il boosting termina. In caso di fit perfetto, la procedura si ferma prima. Di base sono 50, ma abbiamo notato che aumentandoli fino ad un valore di 300/400 le performance aumentavano, seppur con un aumento del tempo necessario all'esecuzione, il quale rimaneva non eccessivo..\n",
        "*   **learning rate:** il tasso di apprendimento riduce il contributo di ogni stimatore ad un fattore pari al valore che gli viene passato. Abbiamo visto che utilizzando un *learning rate* inferiore ad 1 (solitamente intorno a 0.1), riuscivamo ad ottenere classificazioni più precise e quindi punteggi migliori.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YWjF6XMthl4h"
      },
      "source": [
        "##### Best Result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VKfSSnay5mvQ",
        "colab": {}
      },
      "source": [
        "dtc = DecisionTreeClassifier()\n",
        "\n",
        "parameter_grid_tree = {\n",
        "    'criterion':['gini','entropy'], \n",
        "    'splitter': ['best']\n",
        "    } \n",
        "\n",
        "cross_validation_tree = StratifiedKFold(n_splits=3)\n",
        "cross_validation_tree.get_n_splits(xTrain, yTrain)\n",
        "#Create the scoring dictionary\n",
        "SCORING = {'accuracy': 'accuracy',\n",
        "'balanced_accuracy': 'balanced_accuracy',\n",
        "'precision': 'precision_macro',\n",
        "'recall': 'recall_macro',\n",
        "'f1': 'f1_macro'}\n",
        "\n",
        "grid_search_tree = GridSearchCV(dtc, param_grid=parameter_grid_tree, cv=cross_validation_tree, scoring=SCORING,return_train_score=True, refit='f1')\n",
        "grid_search_tree.fit(xTrain, yTrain)\n",
        "best_dtc = grid_search_tree.best_estimator_\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sH3sj_uP53AF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "2273ee25-78a8-4db0-fe12-a06dd5e59bb0"
      },
      "source": [
        "best_dtc = grid_search_tree.best_estimator_\n",
        "best_dtc"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='entropy',\n",
              "                       max_depth=None, max_features=None, max_leaf_nodes=None,\n",
              "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                       min_samples_leaf=1, min_samples_split=2,\n",
              "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
              "                       random_state=None, splitter='best')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5HNkQnC5hl4i",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "4f3d035a-b964-451b-fbcf-b8eba16dc05f"
      },
      "source": [
        "#User defined AdaBoost\n",
        "\n",
        "ada = AdaBoostClassifier()\n",
        "\n",
        "parameter_grid = {\n",
        "    'base_estimator': [best_dtc], \n",
        "    'n_estimators': [200,350,500], \n",
        "    'learning_rate': [0.1,0.01]\n",
        "    }\n",
        "\n",
        "grid_search = GridSearchCV(ada, param_grid=parameter_grid, cv=cross_validation, scoring=SCORING,return_train_score=True, refit='f1')\n",
        "grid_search.fit(xTrain, yTrain)\n",
        "\n",
        "cross_validation = StratifiedKFold(n_splits=10)\n",
        "cross_validation.get_n_splits(xTrain, yTrain)\n",
        "#Create the scoring dictionary\n",
        "SCORING = {'accuracy': 'accuracy',\n",
        "'balanced_accuracy': 'balanced_accuracy',\n",
        "'precision': 'precision_macro',\n",
        "'recall': 'recall_macro',\n",
        "'f1': 'f1_macro'}\n",
        "\n",
        "grid_search = GridSearchCV(ada, param_grid=parameter_grid, cv=cross_validation, scoring=SCORING,return_train_score=True, refit='f1')\n",
        "\n",
        "grid_search.fit(xTrain, yTrain)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=StratifiedKFold(n_splits=10, random_state=None, shuffle=False),\n",
              "             error_score=nan,\n",
              "             estimator=AdaBoostClassifier(algorithm='SAMME.R',\n",
              "                                          base_estimator=None,\n",
              "                                          learning_rate=1.0, n_estimators=50,\n",
              "                                          random_state=None),\n",
              "             iid='deprecated', n_jobs=None,\n",
              "             param_grid={'base_estimator': [DecisionTreeClassifier(ccp_alpha=0.0,\n",
              "                                                                   class_weight=None,\n",
              "                                                                   criterion='entropy',\n",
              "                                                                   m...\n",
              "                                                                   min_weight_fraction_leaf=0.0,\n",
              "                                                                   presort='deprecated',\n",
              "                                                                   random_state=None,\n",
              "                                                                   splitter='best')],\n",
              "                         'learning_rate': [0.1, 0.01],\n",
              "                         'n_estimators': [200, 350, 500]},\n",
              "             pre_dispatch='2*n_jobs', refit='f1', return_train_score=True,\n",
              "             scoring={'accuracy': 'accuracy',\n",
              "                      'balanced_accuracy': 'balanced_accuracy',\n",
              "                      'f1': 'f1_macro', 'precision': 'precision_macro',\n",
              "                      'recall': 'recall_macro'},\n",
              "             verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "K1gtZdt1bx4Y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1327a5af-de96-4a34-bff7-81d75507adbd"
      },
      "source": [
        "grid_search.cv_results_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'mean_fit_time': array([0.03158085, 0.03117623, 0.03131053, 0.03119669, 0.03131526,\n",
              "        0.03101509]),\n",
              " 'mean_score_time': array([0.00419478, 0.0041357 , 0.00418043, 0.00418499, 0.00412748,\n",
              "        0.00416887]),\n",
              " 'mean_test_accuracy': array([0.75135305, 0.75744624, 0.7560151 , 0.7578021 , 0.75708781,\n",
              "        0.75744624]),\n",
              " 'mean_test_balanced_accuracy': array([0.72039879, 0.72600525, 0.72445346, 0.7265206 , 0.72626003,\n",
              "        0.72851218]),\n",
              " 'mean_test_f1': array([0.72144997, 0.72742724, 0.72608215, 0.72822684, 0.727649  ,\n",
              "        0.72908091]),\n",
              " 'mean_test_precision': array([0.72422226, 0.73027058, 0.72906561, 0.73090054, 0.73061279,\n",
              "        0.73063618]),\n",
              " 'mean_test_recall': array([0.72039879, 0.72600525, 0.72445346, 0.7265206 , 0.72626003,\n",
              "        0.72851218]),\n",
              " 'mean_train_accuracy': array([1., 1., 1., 1., 1., 1.]),\n",
              " 'mean_train_balanced_accuracy': array([1., 1., 1., 1., 1., 1.]),\n",
              " 'mean_train_f1': array([1., 1., 1., 1., 1., 1.]),\n",
              " 'mean_train_precision': array([1., 1., 1., 1., 1., 1.]),\n",
              " 'mean_train_recall': array([1., 1., 1., 1., 1., 1.]),\n",
              " 'param_base_estimator': masked_array(data=[DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='entropy',\n",
              "                        max_depth=None, max_features=None, max_leaf_nodes=None,\n",
              "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                        min_samples_leaf=1, min_samples_split=2,\n",
              "                        min_weight_fraction_leaf=0.0, presort='deprecated',\n",
              "                        random_state=None, splitter='best'),\n",
              "                    DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='entropy',\n",
              "                        max_depth=None, max_features=None, max_leaf_nodes=None,\n",
              "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                        min_samples_leaf=1, min_samples_split=2,\n",
              "                        min_weight_fraction_leaf=0.0, presort='deprecated',\n",
              "                        random_state=None, splitter='best'),\n",
              "                    DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='entropy',\n",
              "                        max_depth=None, max_features=None, max_leaf_nodes=None,\n",
              "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                        min_samples_leaf=1, min_samples_split=2,\n",
              "                        min_weight_fraction_leaf=0.0, presort='deprecated',\n",
              "                        random_state=None, splitter='best'),\n",
              "                    DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='entropy',\n",
              "                        max_depth=None, max_features=None, max_leaf_nodes=None,\n",
              "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                        min_samples_leaf=1, min_samples_split=2,\n",
              "                        min_weight_fraction_leaf=0.0, presort='deprecated',\n",
              "                        random_state=None, splitter='best'),\n",
              "                    DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='entropy',\n",
              "                        max_depth=None, max_features=None, max_leaf_nodes=None,\n",
              "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                        min_samples_leaf=1, min_samples_split=2,\n",
              "                        min_weight_fraction_leaf=0.0, presort='deprecated',\n",
              "                        random_state=None, splitter='best'),\n",
              "                    DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='entropy',\n",
              "                        max_depth=None, max_features=None, max_leaf_nodes=None,\n",
              "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                        min_samples_leaf=1, min_samples_split=2,\n",
              "                        min_weight_fraction_leaf=0.0, presort='deprecated',\n",
              "                        random_state=None, splitter='best')],\n",
              "              mask=[False, False, False, False, False, False],\n",
              "        fill_value='?',\n",
              "             dtype=object),\n",
              " 'param_learning_rate': masked_array(data=[0.1, 0.1, 0.1, 0.01, 0.01, 0.01],\n",
              "              mask=[False, False, False, False, False, False],\n",
              "        fill_value='?',\n",
              "             dtype=object),\n",
              " 'param_n_estimators': masked_array(data=[200, 350, 500, 200, 350, 500],\n",
              "              mask=[False, False, False, False, False, False],\n",
              "        fill_value='?',\n",
              "             dtype=object),\n",
              " 'params': [{'base_estimator': DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='entropy',\n",
              "                          max_depth=None, max_features=None, max_leaf_nodes=None,\n",
              "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                          min_samples_leaf=1, min_samples_split=2,\n",
              "                          min_weight_fraction_leaf=0.0, presort='deprecated',\n",
              "                          random_state=None, splitter='best'),\n",
              "   'learning_rate': 0.1,\n",
              "   'n_estimators': 200},\n",
              "  {'base_estimator': DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='entropy',\n",
              "                          max_depth=None, max_features=None, max_leaf_nodes=None,\n",
              "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                          min_samples_leaf=1, min_samples_split=2,\n",
              "                          min_weight_fraction_leaf=0.0, presort='deprecated',\n",
              "                          random_state=None, splitter='best'),\n",
              "   'learning_rate': 0.1,\n",
              "   'n_estimators': 350},\n",
              "  {'base_estimator': DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='entropy',\n",
              "                          max_depth=None, max_features=None, max_leaf_nodes=None,\n",
              "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                          min_samples_leaf=1, min_samples_split=2,\n",
              "                          min_weight_fraction_leaf=0.0, presort='deprecated',\n",
              "                          random_state=None, splitter='best'),\n",
              "   'learning_rate': 0.1,\n",
              "   'n_estimators': 500},\n",
              "  {'base_estimator': DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='entropy',\n",
              "                          max_depth=None, max_features=None, max_leaf_nodes=None,\n",
              "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                          min_samples_leaf=1, min_samples_split=2,\n",
              "                          min_weight_fraction_leaf=0.0, presort='deprecated',\n",
              "                          random_state=None, splitter='best'),\n",
              "   'learning_rate': 0.01,\n",
              "   'n_estimators': 200},\n",
              "  {'base_estimator': DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='entropy',\n",
              "                          max_depth=None, max_features=None, max_leaf_nodes=None,\n",
              "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                          min_samples_leaf=1, min_samples_split=2,\n",
              "                          min_weight_fraction_leaf=0.0, presort='deprecated',\n",
              "                          random_state=None, splitter='best'),\n",
              "   'learning_rate': 0.01,\n",
              "   'n_estimators': 350},\n",
              "  {'base_estimator': DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='entropy',\n",
              "                          max_depth=None, max_features=None, max_leaf_nodes=None,\n",
              "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                          min_samples_leaf=1, min_samples_split=2,\n",
              "                          min_weight_fraction_leaf=0.0, presort='deprecated',\n",
              "                          random_state=None, splitter='best'),\n",
              "   'learning_rate': 0.01,\n",
              "   'n_estimators': 500}],\n",
              " 'rank_test_accuracy': array([6, 3, 5, 1, 4, 2], dtype=int32),\n",
              " 'rank_test_balanced_accuracy': array([6, 4, 5, 2, 3, 1], dtype=int32),\n",
              " 'rank_test_f1': array([6, 4, 5, 2, 3, 1], dtype=int32),\n",
              " 'rank_test_precision': array([6, 4, 5, 1, 3, 2], dtype=int32),\n",
              " 'rank_test_recall': array([6, 4, 5, 2, 3, 1], dtype=int32),\n",
              " 'split0_test_accuracy': array([0.725     , 0.725     , 0.71785714, 0.73214286, 0.725     ,\n",
              "        0.725     ]),\n",
              " 'split0_test_balanced_accuracy': array([0.67866848, 0.68115942, 0.67572464, 0.69157609, 0.68365036,\n",
              "        0.68863225]),\n",
              " 'split0_test_f1': array([0.68386633, 0.6857097 , 0.67937847, 0.69561247, 0.68749547,\n",
              "        0.69090049]),\n",
              " 'split0_test_precision': array([0.69301572, 0.69306184, 0.68486094, 0.70153654, 0.69319874,\n",
              "        0.69373219]),\n",
              " 'split0_test_recall': array([0.67866848, 0.68115942, 0.67572464, 0.69157609, 0.68365036,\n",
              "        0.68863225]),\n",
              " 'split0_train_accuracy': array([1., 1., 1., 1., 1., 1.]),\n",
              " 'split0_train_balanced_accuracy': array([1., 1., 1., 1., 1., 1.]),\n",
              " 'split0_train_f1': array([1., 1., 1., 1., 1., 1.]),\n",
              " 'split0_train_precision': array([1., 1., 1., 1., 1., 1.]),\n",
              " 'split0_train_recall': array([1., 1., 1., 1., 1., 1.]),\n",
              " 'split1_test_accuracy': array([0.74193548, 0.74193548, 0.73835125, 0.74551971, 0.73476703,\n",
              "        0.75268817]),\n",
              " 'split1_test_balanced_accuracy': array([0.7076087 , 0.70506293, 0.7125286 , 0.71287185, 0.70217391,\n",
              "        0.7208524 ]),\n",
              " 'split1_test_f1': array([0.70967742, 0.70809114, 0.71088083, 0.7144731 , 0.70319149,\n",
              "        0.72251611]),\n",
              " 'split1_test_precision': array([0.71212298, 0.71197516, 0.70943737, 0.71628691, 0.70430108,\n",
              "        0.72439549]),\n",
              " 'split1_test_recall': array([0.7076087 , 0.70506293, 0.7125286 , 0.71287185, 0.70217391,\n",
              "        0.7208524 ]),\n",
              " 'split1_train_accuracy': array([1., 1., 1., 1., 1., 1.]),\n",
              " 'split1_train_balanced_accuracy': array([1., 1., 1., 1., 1., 1.]),\n",
              " 'split1_train_f1': array([1., 1., 1., 1., 1., 1.]),\n",
              " 'split1_train_precision': array([1., 1., 1., 1., 1., 1.]),\n",
              " 'split1_train_recall': array([1., 1., 1., 1., 1., 1.]),\n",
              " 'split2_test_accuracy': array([0.72043011, 0.72401434, 0.75985663, 0.7311828 , 0.72759857,\n",
              "        0.72759857]),\n",
              " 'split2_test_balanced_accuracy': array([0.69894165, 0.69402174, 0.73137872, 0.70200229, 0.70183066,\n",
              "        0.70183066]),\n",
              " 'split2_test_f1': array([0.69469697, 0.69352469, 0.73196541, 0.70148508, 0.69972811,\n",
              "        0.69972811]),\n",
              " 'split2_test_precision': array([0.69183032, 0.69304986, 0.73257619, 0.70099044, 0.6979798 ,\n",
              "        0.6979798 ]),\n",
              " 'split2_test_recall': array([0.69894165, 0.69402174, 0.73137872, 0.70200229, 0.70183066,\n",
              "        0.70183066]),\n",
              " 'split2_train_accuracy': array([1., 1., 1., 1., 1., 1.]),\n",
              " 'split2_train_balanced_accuracy': array([1., 1., 1., 1., 1., 1.]),\n",
              " 'split2_train_f1': array([1., 1., 1., 1., 1., 1.]),\n",
              " 'split2_train_precision': array([1., 1., 1., 1., 1., 1.]),\n",
              " 'split2_train_recall': array([1., 1., 1., 1., 1., 1.]),\n",
              " 'split3_test_accuracy': array([0.80286738, 0.83512545, 0.80645161, 0.81003584, 0.81003584,\n",
              "        0.82078853]),\n",
              " 'split3_test_balanced_accuracy': array([0.79453661, 0.83172197, 0.797254  , 0.7999714 , 0.80506293,\n",
              "        0.81830664]),\n",
              " 'split3_test_f1': array([0.78615327, 0.82153187, 0.78958101, 0.79301801, 0.79481024,\n",
              "        0.80682951]),\n",
              " 'split3_test_precision': array([0.78075581, 0.81506738, 0.78440066, 0.7881044 , 0.78874269,\n",
              "        0.80026983]),\n",
              " 'split3_test_recall': array([0.79453661, 0.83172197, 0.797254  , 0.7999714 , 0.80506293,\n",
              "        0.81830664]),\n",
              " 'split3_train_accuracy': array([1., 1., 1., 1., 1., 1.]),\n",
              " 'split3_train_balanced_accuracy': array([1., 1., 1., 1., 1., 1.]),\n",
              " 'split3_train_f1': array([1., 1., 1., 1., 1., 1.]),\n",
              " 'split3_train_precision': array([1., 1., 1., 1., 1., 1.]),\n",
              " 'split3_train_recall': array([1., 1., 1., 1., 1., 1.]),\n",
              " 'split4_test_accuracy': array([0.76344086, 0.76702509, 0.77419355, 0.77060932, 0.77777778,\n",
              "        0.77060932]),\n",
              " 'split4_test_balanced_accuracy': array([0.73664188, 0.7368135 , 0.74479405, 0.73698513, 0.75005721,\n",
              "        0.73698513]),\n",
              " 'split4_test_f1': array([0.73664188, 0.73860213, 0.74664514, 0.74052546, 0.7513226 ,\n",
              "        0.74052546]),\n",
              " 'split4_test_precision': array([0.73664188, 0.74061265, 0.74872123, 0.74497339, 0.75268817,\n",
              "        0.74497339]),\n",
              " 'split4_test_recall': array([0.73664188, 0.7368135 , 0.74479405, 0.73698513, 0.75005721,\n",
              "        0.73698513]),\n",
              " 'split4_train_accuracy': array([1., 1., 1., 1., 1., 1.]),\n",
              " 'split4_train_balanced_accuracy': array([1., 1., 1., 1., 1., 1.]),\n",
              " 'split4_train_f1': array([1., 1., 1., 1., 1., 1.]),\n",
              " 'split4_train_precision': array([1., 1., 1., 1., 1., 1.]),\n",
              " 'split4_train_recall': array([1., 1., 1., 1., 1., 1.]),\n",
              " 'split5_test_accuracy': array([0.73476703, 0.73476703, 0.72759857, 0.74193548, 0.72759857,\n",
              "        0.73476703]),\n",
              " 'split5_test_balanced_accuracy': array([0.70217391, 0.69708238, 0.69419336, 0.7076087 , 0.69419336,\n",
              "        0.69962815]),\n",
              " 'split5_test_f1': array([0.70319149, 0.69998256, 0.69516964, 0.70967742, 0.69516964,\n",
              "        0.7016129 ]),\n",
              " 'split5_test_precision': array([0.70430108, 0.70372561, 0.69623656, 0.71212298, 0.69623656,\n",
              "        0.7039689 ]),\n",
              " 'split5_test_recall': array([0.70217391, 0.69708238, 0.69419336, 0.7076087 , 0.69419336,\n",
              "        0.69962815]),\n",
              " 'split5_train_accuracy': array([1., 1., 1., 1., 1., 1.]),\n",
              " 'split5_train_balanced_accuracy': array([1., 1., 1., 1., 1., 1.]),\n",
              " 'split5_train_f1': array([1., 1., 1., 1., 1., 1.]),\n",
              " 'split5_train_precision': array([1., 1., 1., 1., 1., 1.]),\n",
              " 'split5_train_recall': array([1., 1., 1., 1., 1., 1.]),\n",
              " 'split6_test_accuracy': array([0.77777778, 0.77419355, 0.77777778, 0.78136201, 0.78494624,\n",
              "        0.78494624]),\n",
              " 'split6_test_balanced_accuracy': array([0.74146175, 0.74615779, 0.74393784, 0.74914617, 0.74692623,\n",
              "        0.75435451]),\n",
              " 'split6_test_f1': array([0.74722352, 0.74796748, 0.74863404, 0.7533657 , 0.75396825,\n",
              "        0.75806452]),\n",
              " 'split6_test_precision': array([0.75557296, 0.75      , 0.75496787, 0.75884092, 0.76483516,\n",
              "        0.76269841]),\n",
              " 'split6_test_recall': array([0.74146175, 0.74615779, 0.74393784, 0.74914617, 0.74692623,\n",
              "        0.75435451]),\n",
              " 'split6_train_accuracy': array([1., 1., 1., 1., 1., 1.]),\n",
              " 'split6_train_balanced_accuracy': array([1., 1., 1., 1., 1., 1.]),\n",
              " 'split6_train_f1': array([1., 1., 1., 1., 1., 1.]),\n",
              " 'split6_train_precision': array([1., 1., 1., 1., 1., 1.]),\n",
              " 'split6_train_recall': array([1., 1., 1., 1., 1., 1.]),\n",
              " 'split7_test_accuracy': array([0.73835125, 0.74910394, 0.74910394, 0.75268817, 0.76702509,\n",
              "        0.74193548]),\n",
              " 'split7_test_balanced_accuracy': array([0.69902664, 0.70969945, 0.70722336, 0.71490779, 0.72831284,\n",
              "        0.70918716]),\n",
              " 'split7_test_f1': array([0.70321311, 0.7146072 , 0.71296296, 0.71947541, 0.73423509,\n",
              "        0.71121334]),\n",
              " 'split7_test_precision': array([0.70932112, 0.72195445, 0.72225275, 0.72602371, 0.74323833,\n",
              "        0.71361311]),\n",
              " 'split7_test_recall': array([0.69902664, 0.70969945, 0.70722336, 0.71490779, 0.72831284,\n",
              "        0.70918716]),\n",
              " 'split7_train_accuracy': array([1., 1., 1., 1., 1., 1.]),\n",
              " 'split7_train_balanced_accuracy': array([1., 1., 1., 1., 1., 1.]),\n",
              " 'split7_train_f1': array([1., 1., 1., 1., 1., 1.]),\n",
              " 'split7_train_precision': array([1., 1., 1., 1., 1., 1.]),\n",
              " 'split7_train_recall': array([1., 1., 1., 1., 1., 1.]),\n",
              " 'split8_test_accuracy': array([0.77060932, 0.76702509, 0.77419355, 0.77060932, 0.77419355,\n",
              "        0.77419355]),\n",
              " 'split8_test_balanced_accuracy': array([0.73104508, 0.72831284, 0.73625342, 0.73847336, 0.73872951,\n",
              "        0.7412056 ]),\n",
              " 'split8_test_f1': array([0.73756614, 0.73423509, 0.74241247, 0.74193548, 0.74386885,\n",
              "        0.74527933]),\n",
              " 'split8_test_precision': array([0.7478022 , 0.74323833, 0.751698  , 0.7462963 , 0.75107759,\n",
              "        0.75059137]),\n",
              " 'split8_test_recall': array([0.73104508, 0.72831284, 0.73625342, 0.73847336, 0.73872951,\n",
              "        0.7412056 ]),\n",
              " 'split8_train_accuracy': array([1., 1., 1., 1., 1., 1.]),\n",
              " 'split8_train_balanced_accuracy': array([1., 1., 1., 1., 1., 1.]),\n",
              " 'split8_train_f1': array([1., 1., 1., 1., 1., 1.]),\n",
              " 'split8_train_precision': array([1., 1., 1., 1., 1., 1.]),\n",
              " 'split8_train_recall': array([1., 1., 1., 1., 1., 1.]),\n",
              " 'split9_test_accuracy': array([0.73835125, 0.7562724 , 0.73476703, 0.74193548, 0.74193548,\n",
              "        0.74193548]),\n",
              " 'split9_test_balanced_accuracy': array([0.7138832 , 0.73002049, 0.70124658, 0.71166325, 0.71166325,\n",
              "        0.71413934]),\n",
              " 'split9_test_f1': array([0.71226955, 0.73002049, 0.70319149, 0.71270023, 0.71270023,\n",
              "        0.71413934]),\n",
              " 'split9_test_precision': array([0.71085859, 0.73002049, 0.70550453, 0.71382979, 0.71382979,\n",
              "        0.71413934]),\n",
              " 'split9_test_recall': array([0.7138832 , 0.73002049, 0.70124658, 0.71166325, 0.71166325,\n",
              "        0.71413934]),\n",
              " 'split9_train_accuracy': array([1., 1., 1., 1., 1., 1.]),\n",
              " 'split9_train_balanced_accuracy': array([1., 1., 1., 1., 1., 1.]),\n",
              " 'split9_train_f1': array([1., 1., 1., 1., 1., 1.]),\n",
              " 'split9_train_precision': array([1., 1., 1., 1., 1., 1.]),\n",
              " 'split9_train_recall': array([1., 1., 1., 1., 1., 1.]),\n",
              " 'std_fit_time': array([0.00148399, 0.00095967, 0.00121319, 0.00093676, 0.0010528 ,\n",
              "        0.00112919]),\n",
              " 'std_score_time': array([2.06882515e-04, 8.62953536e-05, 1.08797480e-04, 2.10482869e-04,\n",
              "        1.00420278e-04, 2.30548257e-04]),\n",
              " 'std_test_accuracy': array([0.02494294, 0.03081297, 0.02601521, 0.0237886 , 0.02809926,\n",
              "        0.02860774]),\n",
              " 'std_test_balanced_accuracy': array([0.03084482, 0.04036769, 0.03247816, 0.02980121, 0.03405415,\n",
              "        0.03569186]),\n",
              " 'std_test_f1': array([0.02891089, 0.03692793, 0.03054311, 0.02792628, 0.03215379,\n",
              "        0.03307216]),\n",
              " 'std_test_precision': array([0.02802879, 0.03425393, 0.02943195, 0.026695  , 0.03199409,\n",
              "        0.03197024]),\n",
              " 'std_test_recall': array([0.03084482, 0.04036769, 0.03247816, 0.02980121, 0.03405415,\n",
              "        0.03569186]),\n",
              " 'std_train_accuracy': array([0., 0., 0., 0., 0., 0.]),\n",
              " 'std_train_balanced_accuracy': array([0., 0., 0., 0., 0., 0.]),\n",
              " 'std_train_f1': array([0., 0., 0., 0., 0., 0.]),\n",
              " 'std_train_precision': array([0., 0., 0., 0., 0., 0.]),\n",
              " 'std_train_recall': array([0., 0., 0., 0., 0., 0.])}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YM0IUUx0bx4e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "07ccbea3-0ef4-4ffe-da7c-bd0c2885df21"
      },
      "source": [
        "print('Best score: {}'.format(grid_search.best_score_))\n",
        "print('Best parameters: {}'.format(grid_search.best_params_))\n",
        "\n",
        "best_ada = grid_search.best_estimator_\n",
        "best_ada"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best score: 0.7290809106334031\n",
            "Best parameters: {'base_estimator': DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='entropy',\n",
            "                       max_depth=None, max_features=None, max_leaf_nodes=None,\n",
            "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
            "                       min_samples_leaf=1, min_samples_split=2,\n",
            "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
            "                       random_state=None, splitter='best'), 'learning_rate': 0.01, 'n_estimators': 500}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AdaBoostClassifier(algorithm='SAMME.R',\n",
              "                   base_estimator=DecisionTreeClassifier(ccp_alpha=0.0,\n",
              "                                                         class_weight=None,\n",
              "                                                         criterion='entropy',\n",
              "                                                         max_depth=None,\n",
              "                                                         max_features=None,\n",
              "                                                         max_leaf_nodes=None,\n",
              "                                                         min_impurity_decrease=0.0,\n",
              "                                                         min_impurity_split=None,\n",
              "                                                         min_samples_leaf=1,\n",
              "                                                         min_samples_split=2,\n",
              "                                                         min_weight_fraction_leaf=0.0,\n",
              "                                                         presort='deprecated',\n",
              "                                                         random_state=None,\n",
              "                                                         splitter='best'),\n",
              "                   learning_rate=0.01, n_estimators=500, random_state=None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qvrrXE8Jbx4g",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8274ff8a-745e-476b-d45e-b95376b03c52"
      },
      "source": [
        "my_model=best_ada\n",
        "my_model.fit(xTrain, yTrain)\n",
        "my_model.score(xTest, yTest)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7750716332378224"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oSuTe1vTbx4i",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "e593ed49-e3e1-467c-8c3a-ea141e7e2cef"
      },
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "y_pred = my_model.predict(xTrain)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(yTrain, y_pred))\n",
        "\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(yTrain, y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Confusion Matrix:\n",
            "[[ 955    0]\n",
            " [   0 1836]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00       955\n",
            "           1       1.00      1.00      1.00      1836\n",
            "\n",
            "    accuracy                           1.00      2791\n",
            "   macro avg       1.00      1.00      1.00      2791\n",
            "weighted avg       1.00      1.00      1.00      2791\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FJHyTli2bx4m",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "outputId": "b690b9ef-1f55-4ae7-e833-1447da01a398"
      },
      "source": [
        "# The snippet below will retrieve the feature importances from the model and make them into a DataFrame.\n",
        "feature_importances = pd.DataFrame(my_model.feature_importances_,\n",
        "                                   index = train_X.columns,\n",
        "                                   columns=['importance']).sort_values('importance', ascending=False)\n",
        "feature_importances"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>importance</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>alcohol</th>\n",
              "      <td>0.191016</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>volatile.acidity</th>\n",
              "      <td>0.123458</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>free.sulfur.dioxide</th>\n",
              "      <td>0.110736</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>density</th>\n",
              "      <td>0.087466</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>fixed.acidity</th>\n",
              "      <td>0.080059</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pH</th>\n",
              "      <td>0.079870</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>total.sulfur.dioxide</th>\n",
              "      <td>0.075350</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>citric.acid</th>\n",
              "      <td>0.068358</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>residual.sugar</th>\n",
              "      <td>0.066408</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>chlorides</th>\n",
              "      <td>0.061719</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sulphates</th>\n",
              "      <td>0.055560</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                      importance\n",
              "alcohol                 0.191016\n",
              "volatile.acidity        0.123458\n",
              "free.sulfur.dioxide     0.110736\n",
              "density                 0.087466\n",
              "fixed.acidity           0.080059\n",
              "pH                      0.079870\n",
              "total.sulfur.dioxide    0.075350\n",
              "citric.acid             0.068358\n",
              "residual.sugar          0.066408\n",
              "chlorides               0.061719\n",
              "sulphates               0.055560"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMPnGQ5fnmB2",
        "colab_type": "text"
      },
      "source": [
        "Public Score: **0.77650**\n",
        "\n",
        "Private Score: **0.75950**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QeVQBxT2hl4k"
      },
      "source": [
        "AdaBoost si è rivelato un metodo che ci ha fatto di ottenere scarsi risultati, pertanto abbiamo effettuato poche prove con questo classificatore, concentrandoci più su quelli che potevano offrire risultati migliori."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "w-QWwa4Ybau3"
      },
      "source": [
        "## *Auto-sklearn*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9_DH-F2lbau8"
      },
      "source": [
        "Auto-sklearn è un toolkit di apprendimento automatizzato e un \"drop-in replacement for a scikit-learn estimator\". \\\\\n",
        "Impostando alcuni semplici parametri è in grado di costruire autonomamente un modello sfruttando varie tecniche di machine learning basate sulla tipologia di dati in ingresso.\n",
        "\n",
        "**NOTA:** La successiva porzione di codice è solamente a scopo illustrativo in quanto manca la parte di import e installazioni necessaria per il funzionamento di auto-sklearn, abbiamo deciso di non includerla nella presentazione in quanto molto dispendiosa a livello di tempo e risorse di calcolo, rimane comunque visionabile nel colab denominato \"Autosklearn\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iRXnxOuJngdl",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "import sklearn.model_selection\n",
        "import sklearn.datasets\n",
        "import sklearn.metrics\n",
        "\n",
        "import autosklearn.metrics\n",
        "import autosklearn.classification\n",
        "\n",
        "accuracy_scorer = autosklearn.metrics.make_scorer(name=\"f1\",\n",
        "                                                  score_func=autosklearn.metrics.f1,\n",
        "                                                  greater_is_better=True,\n",
        "                                                  needs_proba=False,\n",
        "                                                  needs_threshold=False,\n",
        "                                                  dummy=None)\n",
        "cls = autosklearn.classification.AutoSklearnClassifier()\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_Jtqwqo9bavJ"
      },
      "source": [
        "Uno dei principali vantaggi è la possibilità di scegliere la classification metric in base alle esigenze del problema, sia tra quelle di default offerte da autosklearn.metrics sia tra metriche personalizzate. \\\\\n",
        "Abbiamo sfruttato l'f1 measure come metrica, impostando \"greater_is_better=True\" in modo da ottenere il modello con il valore di f1 più elevato. \\\\\n",
        "Inoltre è possibile settare i parametri riguardanti il tempo di esecuzione dell'algoritmo, per evitare overfitting o periodi di calcolo eccessivamente elevati."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ENJwUytocnE9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "a3975931-2688-4bf2-d2f7-453036603b62"
      },
      "source": [
        "# Lista delle metriche di classificazione disponibili\n",
        "print(\"Available CLASSIFICATION metrics autosklearn.metrics.*:\")\n",
        "print(\"\\t*\" + \"\\n\\t*\".join(autosklearn.metrics.CLASSIFICATION_METRICS))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Available CLASSIFICATION metrics autosklearn.metrics.*:\n",
            "\t*accuracy\n",
            "\t*balanced_accuracy\n",
            "\t*roc_auc\n",
            "\t*average_precision\n",
            "\t*log_loss\n",
            "\t*precision\n",
            "\t*precision_macro\n",
            "\t*precision_micro\n",
            "\t*precision_samples\n",
            "\t*precision_weighted\n",
            "\t*recall\n",
            "\t*recall_macro\n",
            "\t*recall_micro\n",
            "\t*recall_samples\n",
            "\t*recall_weighted\n",
            "\t*f1\n",
            "\t*f1_macro\n",
            "\t*f1_micro\n",
            "\t*f1_samples\n",
            "\t*f1_weighted\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3hAeIxyRosWY",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "accuracy_scorer = autosklearn.metrics.make_scorer(name=\"f1\",\n",
        "                                                  score_func=autosklearn.metrics.f1_macro,\n",
        "                                                  greater_is_better=True)\n",
        "cls = autosklearn.classification.\\\n",
        "    AutoSklearnClassifier(time_left_for_this_task=500,\n",
        "                          per_run_time_limit=100, seed=1, metric=accuracy_scorer)\n",
        "cls.fit(xTrain, yTrain)\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jwIOxSjoxml",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "predictions = cls.predict(xTest)\n",
        "print(\"Accuracy score {:g} using {:s}\".\n",
        "      format(sklearn.metrics.accuracy_score(yTest, predictions),\n",
        "             \"f1\"))\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "S2XKy_t1bavX"
      },
      "source": [
        "**Public score**: 0.80802 \\\\\n",
        "**Private score**: 0.82331 \\\\\n",
        "Il risultato ottenuto è relativamente positivo, avendo la possibilità di concentrarsi unicamente su questo metodo sarebbe possibile definire una misura personalizzata più accurata della f1_macro e lasciar girare il programma per più tempo, cercando di ottenere punteggi migliori."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWyGnbFxlABi",
        "colab_type": "text"
      },
      "source": [
        "# **Considerazioni finali**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0ZOOa8slGfh",
        "colab_type": "text"
      },
      "source": [
        "In conclusione, per quanto riguarda la parte di preprocessing ci siamo limitati ad approcci superficiali di feature creation, che essendo sviluppati ulteriormente potrebbero migliorare il comportamento dei modelli. Sarebbe anche possibile pensare a strategie alternative per la normalizzazione dei dati e ricerca di outliers. Inoltre, la scarsa conoscenza di statistiche e soprattutto dell'argomento su cui si basa il dataset ci ha impedito di compiere analisi dei dati più accurate.\n",
        "\n",
        "Altro aspetto che può essere approfondito ulteriormente sono le metriche di valutazione dei modelli, in cui ci siamo limitati prevalentemente a ‘accuracy’ e ‘f1’, ciò ha penalizzato in parte il nostro lavoro portandoci a scegliere modelli per la valutazione finale che non erano effettivamente i migliori tra quelli sviluppati e a scartarne altri che invece lo erano.\n",
        "\n",
        "Ci riteniamo soddisfatti del lavoro in quanto abbiamo avuto modo di testare ed approfondire tutti i metodi di machine learning affrontati durante il corso. Inoltre i limiti imposti dalle restrizioni anti-covid ci hanno \"costretti\" a sfruttare al massimo i mezzi per il lavoro a distanza, acquisendo maggiori capacità per l'utilizzo ottimale di GitHub e Google Colab.\n"
      ]
    }
  ]
}